{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit"
  },
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "1 subject.\n",
    "No feature extraction."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             FP1        AF3        F7        F3       FC1        FC5  \\\n",
       "0       0.948232  10.260175  1.013050 -7.658428 -1.811108  11.011411   \n",
       "1       1.653335  12.795443 -1.067832 -3.267558 -4.783876   7.402976   \n",
       "2       3.013726  10.426192  3.908249  0.701542 -0.522649   1.120469   \n",
       "3       1.495061   8.229207  6.094405  2.959722  1.299854  -0.832024   \n",
       "4      -1.264836   3.751782  4.145906  3.459897 -0.916779  -0.784404   \n",
       "...          ...        ...       ...       ...       ...        ...   \n",
       "322555 -0.097608  -0.062015 -0.151092  0.013060 -0.046585  -0.060403   \n",
       "322556 -0.097608  -0.062015 -0.151092  0.013060 -0.046585  -0.060403   \n",
       "322557 -0.097608  -0.062015 -0.151092  0.013060 -0.046585  -0.060403   \n",
       "322558 -0.097608  -0.062015 -0.151092  0.013060 -0.046585  -0.060403   \n",
       "322559 -0.097608  -0.062015 -0.151092  0.013060 -0.046585  -0.060403   \n",
       "\n",
       "              T7        C3       CP1        CP5  ...        T8       FC6  \\\n",
       "0       3.026008 -2.380048  3.978952  -9.657708  ...  1.134884  1.435263   \n",
       "1       2.676232 -3.614201 -1.434440  -0.906298  ...  1.824904 -1.665933   \n",
       "2       2.046996 -4.286566 -6.174767   9.408599  ... -4.090714  1.008985   \n",
       "3       2.192056 -5.170547 -6.571542  14.101073  ... -2.502785  0.562838   \n",
       "4      -4.694002 -5.332026 -7.793136  16.284187  ...  6.397443 -3.709545   \n",
       "...          ...       ...       ...        ...  ...       ...       ...   \n",
       "322555  0.149920  0.000826  0.214036  -0.050429  ...  0.068903  0.140944   \n",
       "322556  0.149920  0.000826  0.215036  -0.050429  ...  0.068903  0.140944   \n",
       "322557  0.149920  0.000826  0.215036  -0.050429  ...  0.068903  0.140944   \n",
       "322558  0.149920  0.000826  0.214036  -0.050429  ...  0.068903  0.140944   \n",
       "322559  0.149920  0.000826  0.215036  -0.050429  ...  0.068903  0.140944   \n",
       "\n",
       "             FC2        F4        F8       AF4       Fp2        Fz         Cz  \\\n",
       "0       1.388486  4.239575  4.888393  0.596471  0.589618 -2.276449  -0.109300   \n",
       "1      -1.577337  4.557239  6.007259 -1.881391 -4.831903 -1.739787  -6.518661   \n",
       "2      -6.161669  0.636801  2.921478 -4.484906 -8.186728  0.555901 -11.727187   \n",
       "3      -4.305524 -2.965047 -0.679860 -4.483488 -7.905437 -1.168129  -9.051847   \n",
       "4      -0.162134  3.248115 -1.103246 -2.547386 -4.419073 -1.319219  -4.072682   \n",
       "...          ...       ...       ...       ...       ...       ...        ...   \n",
       "322555 -0.272806 -0.101962  0.036677 -0.007930  0.081379 -0.081538  -0.009341   \n",
       "322556 -0.272806 -0.101962  0.036677 -0.007930  0.081379 -0.081538  -0.009341   \n",
       "322557 -0.272806 -0.101962  0.036677 -0.007930  0.081379 -0.082538  -0.009341   \n",
       "322558 -0.272806 -0.101962  0.036677 -0.007930  0.081379 -0.081538  -0.009341   \n",
       "322559 -0.272806 -0.101962  0.036677 -0.007930  0.081379 -0.082538  -0.009341   \n",
       "\n",
       "        valence  \n",
       "0         happy  \n",
       "1         happy  \n",
       "2         happy  \n",
       "3         happy  \n",
       "4         happy  \n",
       "...         ...  \n",
       "322555    happy  \n",
       "322556    happy  \n",
       "322557    happy  \n",
       "322558    happy  \n",
       "322559    happy  \n",
       "\n",
       "[322560 rows x 33 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>FP1</th>\n      <th>AF3</th>\n      <th>F7</th>\n      <th>F3</th>\n      <th>FC1</th>\n      <th>FC5</th>\n      <th>T7</th>\n      <th>C3</th>\n      <th>CP1</th>\n      <th>CP5</th>\n      <th>...</th>\n      <th>T8</th>\n      <th>FC6</th>\n      <th>FC2</th>\n      <th>F4</th>\n      <th>F8</th>\n      <th>AF4</th>\n      <th>Fp2</th>\n      <th>Fz</th>\n      <th>Cz</th>\n      <th>valence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.948232</td>\n      <td>10.260175</td>\n      <td>1.013050</td>\n      <td>-7.658428</td>\n      <td>-1.811108</td>\n      <td>11.011411</td>\n      <td>3.026008</td>\n      <td>-2.380048</td>\n      <td>3.978952</td>\n      <td>-9.657708</td>\n      <td>...</td>\n      <td>1.134884</td>\n      <td>1.435263</td>\n      <td>1.388486</td>\n      <td>4.239575</td>\n      <td>4.888393</td>\n      <td>0.596471</td>\n      <td>0.589618</td>\n      <td>-2.276449</td>\n      <td>-0.109300</td>\n      <td>happy</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.653335</td>\n      <td>12.795443</td>\n      <td>-1.067832</td>\n      <td>-3.267558</td>\n      <td>-4.783876</td>\n      <td>7.402976</td>\n      <td>2.676232</td>\n      <td>-3.614201</td>\n      <td>-1.434440</td>\n      <td>-0.906298</td>\n      <td>...</td>\n      <td>1.824904</td>\n      <td>-1.665933</td>\n      <td>-1.577337</td>\n      <td>4.557239</td>\n      <td>6.007259</td>\n      <td>-1.881391</td>\n      <td>-4.831903</td>\n      <td>-1.739787</td>\n      <td>-6.518661</td>\n      <td>happy</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.013726</td>\n      <td>10.426192</td>\n      <td>3.908249</td>\n      <td>0.701542</td>\n      <td>-0.522649</td>\n      <td>1.120469</td>\n      <td>2.046996</td>\n      <td>-4.286566</td>\n      <td>-6.174767</td>\n      <td>9.408599</td>\n      <td>...</td>\n      <td>-4.090714</td>\n      <td>1.008985</td>\n      <td>-6.161669</td>\n      <td>0.636801</td>\n      <td>2.921478</td>\n      <td>-4.484906</td>\n      <td>-8.186728</td>\n      <td>0.555901</td>\n      <td>-11.727187</td>\n      <td>happy</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.495061</td>\n      <td>8.229207</td>\n      <td>6.094405</td>\n      <td>2.959722</td>\n      <td>1.299854</td>\n      <td>-0.832024</td>\n      <td>2.192056</td>\n      <td>-5.170547</td>\n      <td>-6.571542</td>\n      <td>14.101073</td>\n      <td>...</td>\n      <td>-2.502785</td>\n      <td>0.562838</td>\n      <td>-4.305524</td>\n      <td>-2.965047</td>\n      <td>-0.679860</td>\n      <td>-4.483488</td>\n      <td>-7.905437</td>\n      <td>-1.168129</td>\n      <td>-9.051847</td>\n      <td>happy</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-1.264836</td>\n      <td>3.751782</td>\n      <td>4.145906</td>\n      <td>3.459897</td>\n      <td>-0.916779</td>\n      <td>-0.784404</td>\n      <td>-4.694002</td>\n      <td>-5.332026</td>\n      <td>-7.793136</td>\n      <td>16.284187</td>\n      <td>...</td>\n      <td>6.397443</td>\n      <td>-3.709545</td>\n      <td>-0.162134</td>\n      <td>3.248115</td>\n      <td>-1.103246</td>\n      <td>-2.547386</td>\n      <td>-4.419073</td>\n      <td>-1.319219</td>\n      <td>-4.072682</td>\n      <td>happy</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>322555</th>\n      <td>-0.097608</td>\n      <td>-0.062015</td>\n      <td>-0.151092</td>\n      <td>0.013060</td>\n      <td>-0.046585</td>\n      <td>-0.060403</td>\n      <td>0.149920</td>\n      <td>0.000826</td>\n      <td>0.214036</td>\n      <td>-0.050429</td>\n      <td>...</td>\n      <td>0.068903</td>\n      <td>0.140944</td>\n      <td>-0.272806</td>\n      <td>-0.101962</td>\n      <td>0.036677</td>\n      <td>-0.007930</td>\n      <td>0.081379</td>\n      <td>-0.081538</td>\n      <td>-0.009341</td>\n      <td>happy</td>\n    </tr>\n    <tr>\n      <th>322556</th>\n      <td>-0.097608</td>\n      <td>-0.062015</td>\n      <td>-0.151092</td>\n      <td>0.013060</td>\n      <td>-0.046585</td>\n      <td>-0.060403</td>\n      <td>0.149920</td>\n      <td>0.000826</td>\n      <td>0.215036</td>\n      <td>-0.050429</td>\n      <td>...</td>\n      <td>0.068903</td>\n      <td>0.140944</td>\n      <td>-0.272806</td>\n      <td>-0.101962</td>\n      <td>0.036677</td>\n      <td>-0.007930</td>\n      <td>0.081379</td>\n      <td>-0.081538</td>\n      <td>-0.009341</td>\n      <td>happy</td>\n    </tr>\n    <tr>\n      <th>322557</th>\n      <td>-0.097608</td>\n      <td>-0.062015</td>\n      <td>-0.151092</td>\n      <td>0.013060</td>\n      <td>-0.046585</td>\n      <td>-0.060403</td>\n      <td>0.149920</td>\n      <td>0.000826</td>\n      <td>0.215036</td>\n      <td>-0.050429</td>\n      <td>...</td>\n      <td>0.068903</td>\n      <td>0.140944</td>\n      <td>-0.272806</td>\n      <td>-0.101962</td>\n      <td>0.036677</td>\n      <td>-0.007930</td>\n      <td>0.081379</td>\n      <td>-0.082538</td>\n      <td>-0.009341</td>\n      <td>happy</td>\n    </tr>\n    <tr>\n      <th>322558</th>\n      <td>-0.097608</td>\n      <td>-0.062015</td>\n      <td>-0.151092</td>\n      <td>0.013060</td>\n      <td>-0.046585</td>\n      <td>-0.060403</td>\n      <td>0.149920</td>\n      <td>0.000826</td>\n      <td>0.214036</td>\n      <td>-0.050429</td>\n      <td>...</td>\n      <td>0.068903</td>\n      <td>0.140944</td>\n      <td>-0.272806</td>\n      <td>-0.101962</td>\n      <td>0.036677</td>\n      <td>-0.007930</td>\n      <td>0.081379</td>\n      <td>-0.081538</td>\n      <td>-0.009341</td>\n      <td>happy</td>\n    </tr>\n    <tr>\n      <th>322559</th>\n      <td>-0.097608</td>\n      <td>-0.062015</td>\n      <td>-0.151092</td>\n      <td>0.013060</td>\n      <td>-0.046585</td>\n      <td>-0.060403</td>\n      <td>0.149920</td>\n      <td>0.000826</td>\n      <td>0.215036</td>\n      <td>-0.050429</td>\n      <td>...</td>\n      <td>0.068903</td>\n      <td>0.140944</td>\n      <td>-0.272806</td>\n      <td>-0.101962</td>\n      <td>0.036677</td>\n      <td>-0.007930</td>\n      <td>0.081379</td>\n      <td>-0.082538</td>\n      <td>-0.009341</td>\n      <td>happy</td>\n    </tr>\n  </tbody>\n</table>\n<p>322560 rows Ã— 33 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "files = os.listdir('d\\\\')\n",
    "\n",
    "list_of_dfs = []\n",
    "for file in files:\n",
    "    with open(os.path.join('d\\\\',file), 'rb') as pickle_file:\n",
    "        dictRaw = pd.read_pickle(pickle_file) \n",
    "        labels = dictRaw.get('labels')\n",
    "        data = dictRaw.get('data')\n",
    "        \n",
    "        dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\n",
    "\n",
    "        a,b,c = data.shape\n",
    "        E = data.reshape(40, 322560)\n",
    "        allData = pd.DataFrame(E).transpose()\n",
    "        rows = list()\n",
    "        for _,row in dfLabels.iterrows():\n",
    "            rows += [row]*8064\n",
    "        aux = pd.DataFrame(rows).reset_index(drop=True)\n",
    "        allData = allData.merge(aux, left_index=True, right_index=True)\n",
    "        \n",
    "        list_of_dfs.append(allData)\n",
    "big_df = pd.concat(list_of_dfs, ignore_index=True)#ignore_index to reset index of big_df\n",
    "\n",
    "data = big_df.drop(['arousal', 'dominance', 'liking'], axis=1)\n",
    "data['valence'] = np.where(data['valence'] <5, 'sad', 'happy')\n",
    "\n",
    "data.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature','valence']\n",
    "\n",
    "data.drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "copia = data.copy()\n",
    "\n",
    "y = copia[['valence']]\n",
    "\n",
    "x = copia.drop(columns=['valence'])\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "source": [
    "# Random Forest"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-3-353746cc4ea1>:4: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  clf.fit(xTrain, yTrain)\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:   11.6s finished\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=2, n_jobs=5, random_state=0, verbose=1)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=0, verbose=1, n_jobs=5)\n",
    "clf.fit(xTrain, yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6491505456349206"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "predict = clf.predict(xTest)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(yTest, predict)"
   ]
  },
  {
   "source": [
    "# SVM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Python39\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm_clf = svm.LinearSVC()\n",
    "svm_clf.fit(xTrain, np.ravel(yTrain))\n",
    "svm_predict = svm_clf.predict(xTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.5456039186507936"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "accuracy_score(yTest, svm_predict)"
   ]
  },
  {
   "source": [
    "# kNN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of K-NN classifier on training set: 0.88\n",
      "Accuracy of K-NN classifier on test set: 0.81\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "n_neighbors = 7\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors)\n",
    "knn.fit(xTrain, np.ravel(yTrain))\n",
    "\n",
    "print('Accuracy of K-NN classifier on training set: {:.2f}'\n",
    "     .format(knn.score(xTrain, yTrain)))\n",
    "print('Accuracy of K-NN classifier on test set: {:.2f}'\n",
    "     .format(knn.score(xTest, yTest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8100818452380952"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "knn_predict = knn.predict(xTest)\n",
    "accuracy_score(yTest, knn_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}