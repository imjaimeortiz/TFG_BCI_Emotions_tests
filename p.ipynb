{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python390jvsc74a57bd063fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d",
   "display_name": "Python 3.9.0 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             FP1        AF3        F7        F3       FC1        FC5  \\\n",
       "0       0.948232  10.260175  1.013050 -7.658428 -1.811108  11.011411   \n",
       "1       1.653335  12.795443 -1.067832 -3.267558 -4.783876   7.402976   \n",
       "2       3.013726  10.426192  3.908249  0.701542 -0.522649   1.120469   \n",
       "3       1.495061   8.229207  6.094405  2.959722  1.299854  -0.832024   \n",
       "4      -1.264836   3.751782  4.145906  3.459897 -0.916779  -0.784404   \n",
       "...          ...        ...       ...       ...       ...        ...   \n",
       "322555 -0.097608  -0.062015 -0.151092  0.013060 -0.046585  -0.060403   \n",
       "322556 -0.097608  -0.062015 -0.151092  0.013060 -0.046585  -0.060403   \n",
       "322557 -0.097608  -0.062015 -0.151092  0.013060 -0.046585  -0.060403   \n",
       "322558 -0.097608  -0.062015 -0.151092  0.013060 -0.046585  -0.060403   \n",
       "322559 -0.097608  -0.062015 -0.151092  0.013060 -0.046585  -0.060403   \n",
       "\n",
       "              T7        C3       CP1        CP5  ...       zEMG      tEMG  \\\n",
       "0       3.026008 -2.380048  3.978952  -9.657708  ... -16.301457 -0.706001   \n",
       "1       2.676232 -3.614201 -1.434440  -0.906298  ...  -4.812761 -5.921890   \n",
       "2       2.046996 -4.286566 -6.174767   9.408599  ...   6.188724 -5.339078   \n",
       "3       2.192056 -5.170547 -6.571542  14.101073  ...  10.774304 -2.384648   \n",
       "4      -4.694002 -5.332026 -7.793136  16.284187  ...  14.252287 -4.426890   \n",
       "...          ...       ...       ...        ...  ...        ...       ...   \n",
       "322555  0.149920  0.000826  0.214036  -0.050429  ...   0.027177  0.028145   \n",
       "322556  0.149920  0.000826  0.215036  -0.050429  ...   0.028177  0.028145   \n",
       "322557  0.149920  0.000826  0.215036  -0.050429  ...   0.028177  0.028145   \n",
       "322558  0.149920  0.000826  0.214036  -0.050429  ...   0.028177  0.028145   \n",
       "322559  0.149920  0.000826  0.215036  -0.050429  ...   0.028177  0.028145   \n",
       "\n",
       "             GSR  Respiration     PLethy  Temperature  valence  arousal  \\\n",
       "0      -1.398098   -11.394413   2.531878     3.083006     7.71     7.60   \n",
       "1       2.184835   -13.450227   0.158111     0.627215     7.71     7.60   \n",
       "2       0.680208    -9.662999  -4.674497    -3.402560     7.71     7.60   \n",
       "3      -3.005392    -1.804665 -11.282898    -5.091942     7.71     7.60   \n",
       "4      -7.057571     6.404798 -11.679159    -5.858427     7.71     7.60   \n",
       "...          ...          ...        ...          ...      ...      ...   \n",
       "322555  0.059504     0.046749  -0.010698     0.186422     5.10     7.12   \n",
       "322556  0.059504     0.046749  -0.010698     0.186422     5.10     7.12   \n",
       "322557  0.059504     0.046749  -0.010698     0.186422     5.10     7.12   \n",
       "322558  0.059504     0.046749  -0.010698     0.186422     5.10     7.12   \n",
       "322559  0.059504     0.046749  -0.010698     0.186422     5.10     7.12   \n",
       "\n",
       "        dominance  liking  \n",
       "0            6.90    7.83  \n",
       "1            6.90    7.83  \n",
       "2            6.90    7.83  \n",
       "3            6.90    7.83  \n",
       "4            6.90    7.83  \n",
       "...           ...     ...  \n",
       "322555       6.17    5.97  \n",
       "322556       6.17    5.97  \n",
       "322557       6.17    5.97  \n",
       "322558       6.17    5.97  \n",
       "322559       6.17    5.97  \n",
       "\n",
       "[322560 rows x 44 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>FP1</th>\n      <th>AF3</th>\n      <th>F7</th>\n      <th>F3</th>\n      <th>FC1</th>\n      <th>FC5</th>\n      <th>T7</th>\n      <th>C3</th>\n      <th>CP1</th>\n      <th>CP5</th>\n      <th>...</th>\n      <th>zEMG</th>\n      <th>tEMG</th>\n      <th>GSR</th>\n      <th>Respiration</th>\n      <th>PLethy</th>\n      <th>Temperature</th>\n      <th>valence</th>\n      <th>arousal</th>\n      <th>dominance</th>\n      <th>liking</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.948232</td>\n      <td>10.260175</td>\n      <td>1.013050</td>\n      <td>-7.658428</td>\n      <td>-1.811108</td>\n      <td>11.011411</td>\n      <td>3.026008</td>\n      <td>-2.380048</td>\n      <td>3.978952</td>\n      <td>-9.657708</td>\n      <td>...</td>\n      <td>-16.301457</td>\n      <td>-0.706001</td>\n      <td>-1.398098</td>\n      <td>-11.394413</td>\n      <td>2.531878</td>\n      <td>3.083006</td>\n      <td>7.71</td>\n      <td>7.60</td>\n      <td>6.90</td>\n      <td>7.83</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.653335</td>\n      <td>12.795443</td>\n      <td>-1.067832</td>\n      <td>-3.267558</td>\n      <td>-4.783876</td>\n      <td>7.402976</td>\n      <td>2.676232</td>\n      <td>-3.614201</td>\n      <td>-1.434440</td>\n      <td>-0.906298</td>\n      <td>...</td>\n      <td>-4.812761</td>\n      <td>-5.921890</td>\n      <td>2.184835</td>\n      <td>-13.450227</td>\n      <td>0.158111</td>\n      <td>0.627215</td>\n      <td>7.71</td>\n      <td>7.60</td>\n      <td>6.90</td>\n      <td>7.83</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.013726</td>\n      <td>10.426192</td>\n      <td>3.908249</td>\n      <td>0.701542</td>\n      <td>-0.522649</td>\n      <td>1.120469</td>\n      <td>2.046996</td>\n      <td>-4.286566</td>\n      <td>-6.174767</td>\n      <td>9.408599</td>\n      <td>...</td>\n      <td>6.188724</td>\n      <td>-5.339078</td>\n      <td>0.680208</td>\n      <td>-9.662999</td>\n      <td>-4.674497</td>\n      <td>-3.402560</td>\n      <td>7.71</td>\n      <td>7.60</td>\n      <td>6.90</td>\n      <td>7.83</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.495061</td>\n      <td>8.229207</td>\n      <td>6.094405</td>\n      <td>2.959722</td>\n      <td>1.299854</td>\n      <td>-0.832024</td>\n      <td>2.192056</td>\n      <td>-5.170547</td>\n      <td>-6.571542</td>\n      <td>14.101073</td>\n      <td>...</td>\n      <td>10.774304</td>\n      <td>-2.384648</td>\n      <td>-3.005392</td>\n      <td>-1.804665</td>\n      <td>-11.282898</td>\n      <td>-5.091942</td>\n      <td>7.71</td>\n      <td>7.60</td>\n      <td>6.90</td>\n      <td>7.83</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-1.264836</td>\n      <td>3.751782</td>\n      <td>4.145906</td>\n      <td>3.459897</td>\n      <td>-0.916779</td>\n      <td>-0.784404</td>\n      <td>-4.694002</td>\n      <td>-5.332026</td>\n      <td>-7.793136</td>\n      <td>16.284187</td>\n      <td>...</td>\n      <td>14.252287</td>\n      <td>-4.426890</td>\n      <td>-7.057571</td>\n      <td>6.404798</td>\n      <td>-11.679159</td>\n      <td>-5.858427</td>\n      <td>7.71</td>\n      <td>7.60</td>\n      <td>6.90</td>\n      <td>7.83</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>322555</th>\n      <td>-0.097608</td>\n      <td>-0.062015</td>\n      <td>-0.151092</td>\n      <td>0.013060</td>\n      <td>-0.046585</td>\n      <td>-0.060403</td>\n      <td>0.149920</td>\n      <td>0.000826</td>\n      <td>0.214036</td>\n      <td>-0.050429</td>\n      <td>...</td>\n      <td>0.027177</td>\n      <td>0.028145</td>\n      <td>0.059504</td>\n      <td>0.046749</td>\n      <td>-0.010698</td>\n      <td>0.186422</td>\n      <td>5.10</td>\n      <td>7.12</td>\n      <td>6.17</td>\n      <td>5.97</td>\n    </tr>\n    <tr>\n      <th>322556</th>\n      <td>-0.097608</td>\n      <td>-0.062015</td>\n      <td>-0.151092</td>\n      <td>0.013060</td>\n      <td>-0.046585</td>\n      <td>-0.060403</td>\n      <td>0.149920</td>\n      <td>0.000826</td>\n      <td>0.215036</td>\n      <td>-0.050429</td>\n      <td>...</td>\n      <td>0.028177</td>\n      <td>0.028145</td>\n      <td>0.059504</td>\n      <td>0.046749</td>\n      <td>-0.010698</td>\n      <td>0.186422</td>\n      <td>5.10</td>\n      <td>7.12</td>\n      <td>6.17</td>\n      <td>5.97</td>\n    </tr>\n    <tr>\n      <th>322557</th>\n      <td>-0.097608</td>\n      <td>-0.062015</td>\n      <td>-0.151092</td>\n      <td>0.013060</td>\n      <td>-0.046585</td>\n      <td>-0.060403</td>\n      <td>0.149920</td>\n      <td>0.000826</td>\n      <td>0.215036</td>\n      <td>-0.050429</td>\n      <td>...</td>\n      <td>0.028177</td>\n      <td>0.028145</td>\n      <td>0.059504</td>\n      <td>0.046749</td>\n      <td>-0.010698</td>\n      <td>0.186422</td>\n      <td>5.10</td>\n      <td>7.12</td>\n      <td>6.17</td>\n      <td>5.97</td>\n    </tr>\n    <tr>\n      <th>322558</th>\n      <td>-0.097608</td>\n      <td>-0.062015</td>\n      <td>-0.151092</td>\n      <td>0.013060</td>\n      <td>-0.046585</td>\n      <td>-0.060403</td>\n      <td>0.149920</td>\n      <td>0.000826</td>\n      <td>0.214036</td>\n      <td>-0.050429</td>\n      <td>...</td>\n      <td>0.028177</td>\n      <td>0.028145</td>\n      <td>0.059504</td>\n      <td>0.046749</td>\n      <td>-0.010698</td>\n      <td>0.186422</td>\n      <td>5.10</td>\n      <td>7.12</td>\n      <td>6.17</td>\n      <td>5.97</td>\n    </tr>\n    <tr>\n      <th>322559</th>\n      <td>-0.097608</td>\n      <td>-0.062015</td>\n      <td>-0.151092</td>\n      <td>0.013060</td>\n      <td>-0.046585</td>\n      <td>-0.060403</td>\n      <td>0.149920</td>\n      <td>0.000826</td>\n      <td>0.215036</td>\n      <td>-0.050429</td>\n      <td>...</td>\n      <td>0.028177</td>\n      <td>0.028145</td>\n      <td>0.059504</td>\n      <td>0.046749</td>\n      <td>-0.010698</td>\n      <td>0.186422</td>\n      <td>5.10</td>\n      <td>7.12</td>\n      <td>6.17</td>\n      <td>5.97</td>\n    </tr>\n  </tbody>\n</table>\n<p>322560 rows × 44 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "files = os.listdir('d\\\\')\n",
    "\n",
    "list_of_dfs = []\n",
    "for file in files:\n",
    "    with open(os.path.join('d\\\\',file), 'rb') as pickle_file:\n",
    "        dictRaw = pd.read_pickle(pickle_file) \n",
    "        labels = dictRaw.get('labels')\n",
    "        data = dictRaw.get('data')\n",
    "        \n",
    "        dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\n",
    "\n",
    "        a,b,c = data.shape\n",
    "        E = data.reshape(40, 322560)\n",
    "        allData = pd.DataFrame(E).transpose()\n",
    "        rows = list()\n",
    "        for _,row in dfLabels.iterrows():\n",
    "            rows += [row]*8064\n",
    "        aux = pd.DataFrame(rows).reset_index(drop=True)\n",
    "        allData = allData.merge(aux, left_index=True, right_index=True)\n",
    "        \n",
    "        list_of_dfs.append(allData)\n",
    "big_df = pd.concat(list_of_dfs, ignore_index=True)#ignore_index to reset index of big_df\n",
    "\n",
    "\n",
    "big_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature','valence', 'arousal', 'dominance', 'liking']\n",
    "\n",
    "big_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             FP1        AF3        F7        F3       FC1        FC5  \\\n",
       "0       0.948232  10.260175  1.013050 -7.658428 -1.811108  11.011411   \n",
       "1       1.653335  12.795443 -1.067832 -3.267558 -4.783876   7.402976   \n",
       "2       3.013726  10.426192  3.908249  0.701542 -0.522649   1.120469   \n",
       "3       1.495061   8.229207  6.094405  2.959722  1.299854  -0.832024   \n",
       "4      -1.264836   3.751782  4.145906  3.459897 -0.916779  -0.784404   \n",
       "...          ...        ...       ...       ...       ...        ...   \n",
       "322555 -0.097608  -0.062015 -0.151092  0.013060 -0.046585  -0.060403   \n",
       "322556 -0.097608  -0.062015 -0.151092  0.013060 -0.046585  -0.060403   \n",
       "322557 -0.097608  -0.062015 -0.151092  0.013060 -0.046585  -0.060403   \n",
       "322558 -0.097608  -0.062015 -0.151092  0.013060 -0.046585  -0.060403   \n",
       "322559 -0.097608  -0.062015 -0.151092  0.013060 -0.046585  -0.060403   \n",
       "\n",
       "              T7        C3       CP1        CP5  ...        F4        F8  \\\n",
       "0       3.026008 -2.380048  3.978952  -9.657708  ...  4.239575  4.888393   \n",
       "1       2.676232 -3.614201 -1.434440  -0.906298  ...  4.557239  6.007259   \n",
       "2       2.046996 -4.286566 -6.174767   9.408599  ...  0.636801  2.921478   \n",
       "3       2.192056 -5.170547 -6.571542  14.101073  ... -2.965047 -0.679860   \n",
       "4      -4.694002 -5.332026 -7.793136  16.284187  ...  3.248115 -1.103246   \n",
       "...          ...       ...       ...        ...  ...       ...       ...   \n",
       "322555  0.149920  0.000826  0.214036  -0.050429  ... -0.101962  0.036677   \n",
       "322556  0.149920  0.000826  0.215036  -0.050429  ... -0.101962  0.036677   \n",
       "322557  0.149920  0.000826  0.215036  -0.050429  ... -0.101962  0.036677   \n",
       "322558  0.149920  0.000826  0.214036  -0.050429  ... -0.101962  0.036677   \n",
       "322559  0.149920  0.000826  0.215036  -0.050429  ... -0.101962  0.036677   \n",
       "\n",
       "             AF4       Fp2        Fz         Cz  valence  arousal  dominance  \\\n",
       "0       0.596471  0.589618 -2.276449  -0.109300     7.71     7.60       6.90   \n",
       "1      -1.881391 -4.831903 -1.739787  -6.518661     7.71     7.60       6.90   \n",
       "2      -4.484906 -8.186728  0.555901 -11.727187     7.71     7.60       6.90   \n",
       "3      -4.483488 -7.905437 -1.168129  -9.051847     7.71     7.60       6.90   \n",
       "4      -2.547386 -4.419073 -1.319219  -4.072682     7.71     7.60       6.90   \n",
       "...          ...       ...       ...        ...      ...      ...        ...   \n",
       "322555 -0.007930  0.081379 -0.081538  -0.009341     5.10     7.12       6.17   \n",
       "322556 -0.007930  0.081379 -0.081538  -0.009341     5.10     7.12       6.17   \n",
       "322557 -0.007930  0.081379 -0.082538  -0.009341     5.10     7.12       6.17   \n",
       "322558 -0.007930  0.081379 -0.081538  -0.009341     5.10     7.12       6.17   \n",
       "322559 -0.007930  0.081379 -0.082538  -0.009341     5.10     7.12       6.17   \n",
       "\n",
       "        liking  \n",
       "0         7.83  \n",
       "1         7.83  \n",
       "2         7.83  \n",
       "3         7.83  \n",
       "4         7.83  \n",
       "...        ...  \n",
       "322555    5.97  \n",
       "322556    5.97  \n",
       "322557    5.97  \n",
       "322558    5.97  \n",
       "322559    5.97  \n",
       "\n",
       "[322560 rows x 36 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>FP1</th>\n      <th>AF3</th>\n      <th>F7</th>\n      <th>F3</th>\n      <th>FC1</th>\n      <th>FC5</th>\n      <th>T7</th>\n      <th>C3</th>\n      <th>CP1</th>\n      <th>CP5</th>\n      <th>...</th>\n      <th>F4</th>\n      <th>F8</th>\n      <th>AF4</th>\n      <th>Fp2</th>\n      <th>Fz</th>\n      <th>Cz</th>\n      <th>valence</th>\n      <th>arousal</th>\n      <th>dominance</th>\n      <th>liking</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.948232</td>\n      <td>10.260175</td>\n      <td>1.013050</td>\n      <td>-7.658428</td>\n      <td>-1.811108</td>\n      <td>11.011411</td>\n      <td>3.026008</td>\n      <td>-2.380048</td>\n      <td>3.978952</td>\n      <td>-9.657708</td>\n      <td>...</td>\n      <td>4.239575</td>\n      <td>4.888393</td>\n      <td>0.596471</td>\n      <td>0.589618</td>\n      <td>-2.276449</td>\n      <td>-0.109300</td>\n      <td>7.71</td>\n      <td>7.60</td>\n      <td>6.90</td>\n      <td>7.83</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.653335</td>\n      <td>12.795443</td>\n      <td>-1.067832</td>\n      <td>-3.267558</td>\n      <td>-4.783876</td>\n      <td>7.402976</td>\n      <td>2.676232</td>\n      <td>-3.614201</td>\n      <td>-1.434440</td>\n      <td>-0.906298</td>\n      <td>...</td>\n      <td>4.557239</td>\n      <td>6.007259</td>\n      <td>-1.881391</td>\n      <td>-4.831903</td>\n      <td>-1.739787</td>\n      <td>-6.518661</td>\n      <td>7.71</td>\n      <td>7.60</td>\n      <td>6.90</td>\n      <td>7.83</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.013726</td>\n      <td>10.426192</td>\n      <td>3.908249</td>\n      <td>0.701542</td>\n      <td>-0.522649</td>\n      <td>1.120469</td>\n      <td>2.046996</td>\n      <td>-4.286566</td>\n      <td>-6.174767</td>\n      <td>9.408599</td>\n      <td>...</td>\n      <td>0.636801</td>\n      <td>2.921478</td>\n      <td>-4.484906</td>\n      <td>-8.186728</td>\n      <td>0.555901</td>\n      <td>-11.727187</td>\n      <td>7.71</td>\n      <td>7.60</td>\n      <td>6.90</td>\n      <td>7.83</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.495061</td>\n      <td>8.229207</td>\n      <td>6.094405</td>\n      <td>2.959722</td>\n      <td>1.299854</td>\n      <td>-0.832024</td>\n      <td>2.192056</td>\n      <td>-5.170547</td>\n      <td>-6.571542</td>\n      <td>14.101073</td>\n      <td>...</td>\n      <td>-2.965047</td>\n      <td>-0.679860</td>\n      <td>-4.483488</td>\n      <td>-7.905437</td>\n      <td>-1.168129</td>\n      <td>-9.051847</td>\n      <td>7.71</td>\n      <td>7.60</td>\n      <td>6.90</td>\n      <td>7.83</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-1.264836</td>\n      <td>3.751782</td>\n      <td>4.145906</td>\n      <td>3.459897</td>\n      <td>-0.916779</td>\n      <td>-0.784404</td>\n      <td>-4.694002</td>\n      <td>-5.332026</td>\n      <td>-7.793136</td>\n      <td>16.284187</td>\n      <td>...</td>\n      <td>3.248115</td>\n      <td>-1.103246</td>\n      <td>-2.547386</td>\n      <td>-4.419073</td>\n      <td>-1.319219</td>\n      <td>-4.072682</td>\n      <td>7.71</td>\n      <td>7.60</td>\n      <td>6.90</td>\n      <td>7.83</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>322555</th>\n      <td>-0.097608</td>\n      <td>-0.062015</td>\n      <td>-0.151092</td>\n      <td>0.013060</td>\n      <td>-0.046585</td>\n      <td>-0.060403</td>\n      <td>0.149920</td>\n      <td>0.000826</td>\n      <td>0.214036</td>\n      <td>-0.050429</td>\n      <td>...</td>\n      <td>-0.101962</td>\n      <td>0.036677</td>\n      <td>-0.007930</td>\n      <td>0.081379</td>\n      <td>-0.081538</td>\n      <td>-0.009341</td>\n      <td>5.10</td>\n      <td>7.12</td>\n      <td>6.17</td>\n      <td>5.97</td>\n    </tr>\n    <tr>\n      <th>322556</th>\n      <td>-0.097608</td>\n      <td>-0.062015</td>\n      <td>-0.151092</td>\n      <td>0.013060</td>\n      <td>-0.046585</td>\n      <td>-0.060403</td>\n      <td>0.149920</td>\n      <td>0.000826</td>\n      <td>0.215036</td>\n      <td>-0.050429</td>\n      <td>...</td>\n      <td>-0.101962</td>\n      <td>0.036677</td>\n      <td>-0.007930</td>\n      <td>0.081379</td>\n      <td>-0.081538</td>\n      <td>-0.009341</td>\n      <td>5.10</td>\n      <td>7.12</td>\n      <td>6.17</td>\n      <td>5.97</td>\n    </tr>\n    <tr>\n      <th>322557</th>\n      <td>-0.097608</td>\n      <td>-0.062015</td>\n      <td>-0.151092</td>\n      <td>0.013060</td>\n      <td>-0.046585</td>\n      <td>-0.060403</td>\n      <td>0.149920</td>\n      <td>0.000826</td>\n      <td>0.215036</td>\n      <td>-0.050429</td>\n      <td>...</td>\n      <td>-0.101962</td>\n      <td>0.036677</td>\n      <td>-0.007930</td>\n      <td>0.081379</td>\n      <td>-0.082538</td>\n      <td>-0.009341</td>\n      <td>5.10</td>\n      <td>7.12</td>\n      <td>6.17</td>\n      <td>5.97</td>\n    </tr>\n    <tr>\n      <th>322558</th>\n      <td>-0.097608</td>\n      <td>-0.062015</td>\n      <td>-0.151092</td>\n      <td>0.013060</td>\n      <td>-0.046585</td>\n      <td>-0.060403</td>\n      <td>0.149920</td>\n      <td>0.000826</td>\n      <td>0.214036</td>\n      <td>-0.050429</td>\n      <td>...</td>\n      <td>-0.101962</td>\n      <td>0.036677</td>\n      <td>-0.007930</td>\n      <td>0.081379</td>\n      <td>-0.081538</td>\n      <td>-0.009341</td>\n      <td>5.10</td>\n      <td>7.12</td>\n      <td>6.17</td>\n      <td>5.97</td>\n    </tr>\n    <tr>\n      <th>322559</th>\n      <td>-0.097608</td>\n      <td>-0.062015</td>\n      <td>-0.151092</td>\n      <td>0.013060</td>\n      <td>-0.046585</td>\n      <td>-0.060403</td>\n      <td>0.149920</td>\n      <td>0.000826</td>\n      <td>0.215036</td>\n      <td>-0.050429</td>\n      <td>...</td>\n      <td>-0.101962</td>\n      <td>0.036677</td>\n      <td>-0.007930</td>\n      <td>0.081379</td>\n      <td>-0.082538</td>\n      <td>-0.009341</td>\n      <td>5.10</td>\n      <td>7.12</td>\n      <td>6.17</td>\n      <td>5.97</td>\n    </tr>\n  </tbody>\n</table>\n<p>322560 rows × 36 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "big_df.drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\n",
    "big_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[5.25838171, 5.59610861, 5.90911369, 6.87436983],\n",
       "       [5.25699007, 5.59707262, 5.90931626, 6.87377288],\n",
       "       [5.25506965, 5.59834133, 5.90920977, 6.8727828 ],\n",
       "       ...,\n",
       "       [5.2566968 , 5.5970094 , 5.90935979, 6.87292762],\n",
       "       [5.25669681, 5.59700942, 5.90935981, 6.87292764],\n",
       "       [5.2566968 , 5.5970094 , 5.90935979, 6.87292762]])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# Linear regressor\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "data = big_df.iloc[:,0:32]\n",
    "labels = big_df.iloc[:,32:36]\n",
    "\n",
    "lin_reg.fit(data, labels)\n",
    "\n",
    "linear_predictor = lin_reg.predict(data)\n",
    "linear_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        valence  arousal  dominance  liking\n",
       "0          7.71     7.60       6.90    7.83\n",
       "1          7.71     7.60       6.90    7.83\n",
       "2          7.71     7.60       6.90    7.83\n",
       "3          7.71     7.60       6.90    7.83\n",
       "4          7.71     7.60       6.90    7.83\n",
       "...         ...      ...        ...     ...\n",
       "322555     5.10     7.12       6.17    5.97\n",
       "322556     5.10     7.12       6.17    5.97\n",
       "322557     5.10     7.12       6.17    5.97\n",
       "322558     5.10     7.12       6.17    5.97\n",
       "322559     5.10     7.12       6.17    5.97\n",
       "\n",
       "[322560 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>valence</th>\n      <th>arousal</th>\n      <th>dominance</th>\n      <th>liking</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7.71</td>\n      <td>7.60</td>\n      <td>6.90</td>\n      <td>7.83</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7.71</td>\n      <td>7.60</td>\n      <td>6.90</td>\n      <td>7.83</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7.71</td>\n      <td>7.60</td>\n      <td>6.90</td>\n      <td>7.83</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7.71</td>\n      <td>7.60</td>\n      <td>6.90</td>\n      <td>7.83</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7.71</td>\n      <td>7.60</td>\n      <td>6.90</td>\n      <td>7.83</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>322555</th>\n      <td>5.10</td>\n      <td>7.12</td>\n      <td>6.17</td>\n      <td>5.97</td>\n    </tr>\n    <tr>\n      <th>322556</th>\n      <td>5.10</td>\n      <td>7.12</td>\n      <td>6.17</td>\n      <td>5.97</td>\n    </tr>\n    <tr>\n      <th>322557</th>\n      <td>5.10</td>\n      <td>7.12</td>\n      <td>6.17</td>\n      <td>5.97</td>\n    </tr>\n    <tr>\n      <th>322558</th>\n      <td>5.10</td>\n      <td>7.12</td>\n      <td>6.17</td>\n      <td>5.97</td>\n    </tr>\n    <tr>\n      <th>322559</th>\n      <td>5.10</td>\n      <td>7.12</td>\n      <td>6.17</td>\n      <td>5.97</td>\n    </tr>\n  </tbody>\n</table>\n<p>322560 rows × 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.982190887189638"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lin_mse = mean_squared_error(labels, linear_predictor)\n",
    "lin_mrse = np.sqrt(lin_mse)\n",
    "lin_mrse\n",
    "\n",
    "# Error medio del predictor : 1.982190887189638"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.878413644294006e-13"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "# Decission Tree Regressor\n",
    "# This is a powerful model, capable of finding complex nonlinear relationships in the data\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(data, labels)\n",
    "tree_predictions = tree_reg.predict(data)\n",
    "tree_mse = mean_squared_error(labels, tree_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "tree_rmse\n",
    "\n",
    "# Error medio del predictor : 1.878413644294006e-13\n",
    "# Error bajísimo, \"the model has badly overfit the data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold cross-validation feature\n",
    "\n",
    "# With tree decisor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(tree_reg, data, labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "tree_rmse_scores = np.sqrt(-scores)\n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Scores: [2.88992712 2.69859699 3.25463431 3.09370128 2.91488416 2.9223349\n 2.76326424 3.47796673 2.00488759 2.03389831]\nMean: 2.805409564249376\nStandard deviation: 0.4493659703703122\n"
     ]
    }
   ],
   "source": [
    "display_scores(tree_rmse_scores)\n",
    "\n",
    "##############################################\n",
    "# NO ENTIENDO MUY BIEN ESTOS VALORES DE MEDIA Y DESVIACIÓN TÍPICA, DISTAN MUCHO DE LO QUE DEBERÍAN\n",
    "##############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Scores: [  1.98704937   1.89037368   2.4939853    2.37306445   1.89885236\n   2.30325901   1.89853493   2.85537378   2.13526026 149.24104334]\nMean: 16.907679647809513\nStandard deviation: 44.11212254550436\n"
     ]
    }
   ],
   "source": [
    "# With linear regression\n",
    "\n",
    "lin_scores = cross_val_score(lin_reg, data, labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "lin_rmse_scores = np.sqrt(-lin_scores)\n",
    "display_scores(lin_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-43c3453e1482>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mforest_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mforest_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mforest_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mforest_reg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"neg_mean_squared_error\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mforest_rmse_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mforest_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# RandomForest Regressor\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "forest_reg.fit(data, labels)\n",
    "forest_scores = cross_val_score(forest_reg, data, labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "forest_rmse_scores = np.sqrt(-forest_scores)\n",
    "display_scores(forest_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clasificar por \"liking\" sin tener nada en cuenta\n",
    "# OJO! Sólo estamos teniendo en cuenta el experimento sobre el primer sujeto (primer fichero)\n",
    "\n",
    "data_copy = data\n",
    "labels_copy = labels\n",
    "\n",
    "data2 = data_copy.drop(['valence', 'arousal', 'dominance'], axis=1)\n",
    "data2['liking'] = np.where(data_copy['liking'] <4, 'unlike', 'like')\n",
    "\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "copia = data2.copy()\n",
    "\n",
    "y = copia[['liking']]\n",
    "\n",
    "x = copia2.drop(columns=['liking'])\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=0, verbose=1, n_jobs=5)\n",
    "clf.fit(xTrain, yTrain)\n",
    "\n",
    "predict = clf.predict(xTest)\n",
    "\n",
    "accuracy_score(yTest, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# No me cuadra lo que dice en el libro de combinaciones con lo que hay en el código\n",
    "# Pone que primero prueba 12 combinaciones (3x4), pero yo entiendo que son 3x2 (s_stimators x max_features)\n",
    "param_grid = [\n",
    "    {'n_stimators': [3,10,30], 'max_features': [2,4,6,8]},\n",
    "    {'bootstrap': [False], 'n_stimators': [3,10], 'max_features': [2,3,4]}\n",
    "]\n",
    "\n",
    "# Cross-Validation de 5, el total de combinaciones será cv * la suma de las combinaciones de param_grid\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "grid_search.fit(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_\n",
    "\n",
    "# No entiendo el siguiente paso de obtener los resultados\n",
    "# Se tiene que mirar el valor de los parámetros de todos los resultados del RMSE y comprobar cuáles se corresponden con los del mejor estimador???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip (cvres['mean_test_score'], cvres['params']):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomizedSearchCV : No entiendo los parámetros que hay que seleccionar\n",
    "\n",
    "#########################################\n",
    "# Buscar documentación para ver cómo se seleccionan\n",
    "#########################################\n",
    "\n",
    "##########################################\n",
    "# De la parte de analizar los modelos y sus errores, no entiendo cómo se construye, pero creo que puede ser útil para eliminar features que no sean relevantes, como dice en el texto\n",
    "##########################################"
   ]
  }
 ]
}