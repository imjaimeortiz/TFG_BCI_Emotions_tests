{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit"
  },
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "1 subject. Feature extraction. Classification joining all frequency bands."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import os\r\n",
    "import pickle\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "from random import randint\r\n",
    "\r\n",
    "#def select_file(file):\r\n",
    "    #Lectura mediante pandas\r\n",
    "f = \"data_preprocessed_python\\\\s01.dat\"\r\n",
    "subject_id = 1\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "    # Frecuencia de muestreo\r\n",
    "    fs = 128\r\n",
    "    # Window\r\n",
    "    window = \"hann\"\r\n",
    "    # Length of each segment\r\n",
    "    # nperseg = 256 por defecto\r\n",
    "    # noverlap\r\n",
    "    # Por defecto a None, if None : noverlap = nperseg / 2\r\n",
    "    # [...]\r\n",
    "\r\n",
    "    # Definicion de bandas\r\n",
    "    eeg_bands = {'Delta': (1, 4),\r\n",
    "                'Theta': (4, 8),\r\n",
    "                'Alpha': (8, 14),\r\n",
    "                'Beta': (14, 30),\r\n",
    "                'Gamma': (30, 50)}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# FOURIER\r\n",
    "import scipy\r\n",
    "from scipy import signal\r\n",
    "\r\n",
    "# SE COGE EL PRIMER VÍDEOS, Df_VIDEOS[0]\r\n",
    "def do_stft(video, channel):\r\n",
    "    # Array \r\n",
    "    x = df_videos[video][channel]\r\n",
    "\r\n",
    "    # Array of the sample frequency, Array of the segment times, STFT of x\r\n",
    "    f, t , Zxx = scipy.signal.stft(x, fs, window)\r\n",
    "    return Zxx\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "#SEPARACION EN LAS DISTINTAS BANDAS DE FRECUENCIA\r\n",
    "\r\n",
    "def make_bands(Zxx):\r\n",
    "    #Obtener valores reales de STFT, solo positivos.\r\n",
    "    values = np.absolute(Zxx)\r\n",
    "\r\n",
    "    # Get frequencies for amplitudes in Hz\r\n",
    "    fft_freq = np.fft.rfftfreq(len(data), 1.0/fs)\r\n",
    "    #print(fft_freq)\r\n",
    "\r\n",
    "    # Obtención de cada una de las bandas de frecuencia\r\n",
    "    eeg_band_fft = dict()\r\n",
    "    for band in eeg_bands:  \r\n",
    "        freq_ix = np.where((fft_freq >= eeg_bands[band][0]) & (fft_freq <= eeg_bands[band][1]))[0]\r\n",
    "        eeg_band_fft[band] = values[freq_ix]\r\n",
    "\r\n",
    "    return eeg_band_fft\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# Ahora hay que hacer las medias sobre cada banda de frecuencia. La media entre cada una de las posiciones de los bloques. Hacer la media entre todas las posiciones 0, 1, 2... 63 de cada uno de los bloques\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "def channel_freq(eeg_band_fft):\r\n",
    "        \r\n",
    "        freq = np.zeros((5, 64))        # 5 bandas, 64 posiciones de los datos\r\n",
    "\r\n",
    "        # Se va a recorrer {Delta, Theta, Alpha, Beta, Gamma}\r\n",
    "        for band in eeg_bands:\r\n",
    "\r\n",
    "                # Para cada una de las 64 posiciones del array\r\n",
    "                for j in range (0, 64):\r\n",
    "                        \r\n",
    "                        val = []\r\n",
    "                        # Se va a recorrer cada uno de los arrays que hay en cada una de las bandas\r\n",
    "                        for i in range (0, len(eeg_band_fft[band])):\r\n",
    "                                val.append(eeg_band_fft[band][i][j])\r\n",
    "                                # Ejemplo : eeg_band_fft['Delta'][0][0*0 + 0]\r\n",
    "                        if (band == \"Delta\"):\r\n",
    "                                freq[0][j] = np.mean(val)\r\n",
    "                        elif (band == \"Theta\"):\r\n",
    "                                freq[1][j] = np.mean(val)\r\n",
    "                        elif (band == \"Alpha\"):\r\n",
    "                                freq[2][j] = np.mean(val)\r\n",
    "                        elif (band == \"Beta\"):\r\n",
    "                                freq[3][j] = np.mean(val)\r\n",
    "                        elif (band == \"Gamma\"):\r\n",
    "                                freq[4][j] = np.mean(val)\r\n",
    "        return(freq)     # En freq tenemos las frecuencias medias de cada una de las bandas\r\n",
    "\r\n",
    "# Recordamos : Estamos tratando los datos relativos al video 0, un canal concreto de un sujeto concreto"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "#df_file_videos = select_file('d\\\\s01.dat')\r\n",
    "# Obtener la división por bandas de frecuencia de cada uno de los canales del video 0\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "        \r\n",
    "#        print(freq[0])\r\n",
    " #       print(\"\\n\\n\")\r\n",
    "    # En freq tenemos la media de todas las bandas de frecuencia de cada uno de los canales del vídeo 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_alpha = pd.DataFrame(rows).reset_index(drop=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Todas las bandas una al lado de otra"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "data = all_bands_df.drop(['arousal', 'dominance', 'liking'], axis=1)\r\n",
    "data['valence'] = np.where(data['valence'] >5, 'happy', 'sad')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['valence']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['valence'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "param_distribs_rf = {\r\n",
    "    'max_depth': [3, 5, 10],\r\n",
    "    'min_samples_split': [2, 5, 10]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_svm = {\r\n",
    "    'kernel': ('linear', 'rbf'),\r\n",
    "    'C': [0.1, 1.0, 10.0, 100.0]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_knn = {\r\n",
    "    'n_neighbors': [3,5,6,7,8,9,10],\r\n",
    "    'weights': ['uniform', 'distance']\r\n",
    "}\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "#print(rnd_svm_search.best_params_)\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_happy = precision_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_prec_sad = precision_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_prec_happy = precision_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_prec_sad = precision_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_prec_happy = precision_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_prec_sad = precision_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_happy = recall_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_rec_sad = recall_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_rec_happy = recall_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_rec_sad = recall_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_rec_happy = recall_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_rec_sad = recall_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_happy = f1_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_f1_sad = f1_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_f1_happy = f1_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_f1_sad  = f1_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_f1_happy = f1_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_f1_sad = f1_score(yTest, knn_predict, pos_label=\"sad\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-32-941bfba26e67>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['valence'] = np.where(data['valence'] >5, 'happy', 'sad')\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "#print(classification_report(svm_predict, yTest))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "col_names = ['Subject',\r\n",
    "             'Experiment ID',\r\n",
    "             'Feature extraction',\r\n",
    "             'Band selection',\r\n",
    "             'Channel selection',\r\n",
    "             'Classification algorithm',\r\n",
    "             'Accuracy',\r\n",
    "             'Happy precision',\r\n",
    "             'Sad precision',\r\n",
    "             'Happy recall',\r\n",
    "             'Sad recall',\r\n",
    "             'Happy f1-score',\r\n",
    "             'Sad f1-score']\r\n",
    "\r\n",
    "data_CSV = [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_happy, rf_prec_sad, rf_rec_happy, rf_rec_sad, rf_f1_happy, rf_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_happy, svm_prec_sad, svm_rec_happy, svm_rec_sad, svm_f1_happy, svm_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_happy, knn_prec_sad, knn_rec_happy, knn_rec_sad, knn_f1_happy, knn_f1_sad]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment3_1.csv\", mode=\"a\", header=col_names, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "#def select_file(file):\r\n",
    "    #Lectura mediante pandas\r\n",
    "f = \"data_preprocessed_python\\\\s02.dat\"\r\n",
    "subject_id = 2\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['arousal', 'dominance', 'liking'], axis=1)\r\n",
    "data['valence'] = np.where(data['valence'] >5, 'happy', 'sad')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['valence']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['valence'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "param_distribs_rf = {\r\n",
    "    'max_depth': [3, 5, 10],\r\n",
    "    'min_samples_split': [2, 5, 10]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_svm = {\r\n",
    "    'kernel': ('linear','rbf'),\r\n",
    "    'C': [0.1, 1, 10, 100]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_knn = {\r\n",
    "    'n_neighbors': [3,5,7,9],\r\n",
    "    'weights': ['uniform', 'distance']\r\n",
    "}\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_happy = precision_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_prec_sad = precision_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_prec_happy = precision_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_prec_sad = precision_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_prec_happy = precision_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_prec_sad = precision_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_happy = recall_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_rec_sad = recall_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_rec_happy = recall_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_rec_sad = recall_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_rec_happy = recall_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_rec_sad = recall_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_happy = f1_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_f1_sad = f1_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_f1_happy = f1_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_f1_sad  = f1_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_f1_happy = f1_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_f1_sad = f1_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "data_CSV = [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_happy, rf_prec_sad, rf_rec_happy, rf_rec_sad, rf_f1_happy, rf_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_happy, svm_prec_sad, svm_rec_happy, svm_rec_sad, svm_f1_happy, svm_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_happy, knn_prec_sad, knn_rec_happy, knn_rec_sad, knn_f1_happy, knn_f1_sad]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment3_1.csv\", mode=\"a\", header=False, index=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-35-0a9ebf286f61>:115: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['valence'] = np.where(data['valence'] >5, 'happy', 'sad')\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  79 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done  91 out of 100 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Parameter grid for parameter (weights) needs to be a list or numpy array, but got (<class 'set'>). Single values need to be wrapped in a list with one element.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-0a9ebf286f61>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[0mknn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m \u001b[0mrnd_knn_search\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mknn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparam_distribs_knn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m \u001b[0mrnd_knn_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxTrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myTrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnd_knn_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, estimator, param_grid, scoring, n_jobs, refit, cv, verbose, pre_dispatch, error_score, return_train_score)\u001b[0m\n\u001b[0;32m   1290\u001b[0m             return_train_score=return_train_score)\n\u001b[0;32m   1291\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1292\u001b[1;33m         \u001b[0m_check_param_grid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1294\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_check_param_grid\u001b[1;34m(param_grid)\u001b[0m\n\u001b[0;32m    398\u001b[0m             if (isinstance(v, str) or\n\u001b[0;32m    399\u001b[0m                     not isinstance(v, (np.ndarray, Sequence))):\n\u001b[1;32m--> 400\u001b[1;33m                 raise ValueError(\"Parameter grid for parameter ({0}) needs to\"\n\u001b[0m\u001b[0;32m    401\u001b[0m                                  \u001b[1;34m\" be a list or numpy array, but got ({1}).\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m                                  \u001b[1;34m\" Single values need to be wrapped in a list\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Parameter grid for parameter (weights) needs to be a list or numpy array, but got (<class 'set'>). Single values need to be wrapped in a list with one element."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#def select_file(file):\r\n",
    "    #Lectura mediante pandas\r\n",
    "f = \"data_preprocessed_python\\\\s03.dat\"\r\n",
    "subject_id = 3\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['arousal', 'dominance', 'liking'], axis=1)\r\n",
    "data['valence'] = np.where(data['valence'] >5, 'happy', 'sad')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['valence']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['valence'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "param_distribs_rf = {\r\n",
    "    'max_depth': [3, 5, 10],\r\n",
    "    'min_samples_split': [2, 5, 10]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_svm = {\r\n",
    "    'kernel': ('linear','rbf'),\r\n",
    "    'C': [0.1, 1, 10, 100]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_knn = {\r\n",
    "    'n_neighbors': [3,5,7,9],\r\n",
    "    'weights': ['uniform', 'distance']\r\n",
    "}\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_happy = precision_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_prec_sad = precision_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_prec_happy = precision_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_prec_sad = precision_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_prec_happy = precision_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_prec_sad = precision_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_happy = recall_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_rec_sad = recall_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_rec_happy = recall_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_rec_sad = recall_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_rec_happy = recall_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_rec_sad = recall_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_happy = f1_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_f1_sad = f1_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_f1_happy = f1_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_f1_sad  = f1_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_f1_happy = f1_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_f1_sad = f1_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "data_CSV = [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_happy, rf_prec_sad, rf_rec_happy, rf_rec_sad, rf_f1_happy, rf_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_happy, svm_prec_sad, svm_rec_happy, svm_rec_sad, svm_f1_happy, svm_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_happy, knn_prec_sad, knn_rec_happy, knn_rec_sad, knn_f1_happy, knn_f1_sad]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment3_1.csv\", mode=\"a\", header=False, index=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-20-e370139dc912>:115: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['valence'] = np.where(data['valence'] >5, 'happy', 'sad')\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#def select_file(file):\r\n",
    "    #Lectura mediante pandas\r\n",
    "f = \"data_preprocessed_python\\\\s04.dat\"\r\n",
    "subject_id = 4\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['arousal', 'dominance', 'liking'], axis=1)\r\n",
    "data['valence'] = np.where(data['valence'] >5, 'happy', 'sad')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['valence']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['valence'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "param_distribs_rf = {\r\n",
    "    'max_depth': [3, 5, 10],\r\n",
    "    'min_samples_split': [2, 5, 10]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_svm = {\r\n",
    "    'kernel': ('linear','rbf'),\r\n",
    "    'C': [0.1, 1, 10, 100]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_knn = {\r\n",
    "    'n_neighbors': [3,5,7,9],\r\n",
    "    'weights': ['uniform', 'distance']\r\n",
    "}\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_happy = precision_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_prec_sad = precision_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_prec_happy = precision_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_prec_sad = precision_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_prec_happy = precision_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_prec_sad = precision_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_happy = recall_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_rec_sad = recall_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_rec_happy = recall_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_rec_sad = recall_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_rec_happy = recall_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_rec_sad = recall_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_happy = f1_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_f1_sad = f1_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_f1_happy = f1_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_f1_sad  = f1_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_f1_happy = f1_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_f1_sad = f1_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "data_CSV = [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_happy, rf_prec_sad, rf_rec_happy, rf_rec_sad, rf_f1_happy, rf_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_happy, svm_prec_sad, svm_rec_happy, svm_rec_sad, svm_f1_happy, svm_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_happy, knn_prec_sad, knn_rec_happy, knn_rec_sad, knn_f1_happy, knn_f1_sad]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment3_1.csv\", mode=\"a\", header=False, index=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-21-df22b788ee50>:115: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['valence'] = np.where(data['valence'] >5, 'happy', 'sad')\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  77 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 242, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1071, in f1_score\n",
      "    return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1195, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1464, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
      "  File \"C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1285, in _check_set_wise_labels\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label. It should be one of ['happy', 'sad']\n",
      "\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done  81 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done  82 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done  83 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done  84 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done  85 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done  86 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done  87 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done  88 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done  89 tasks      | elapsed:    0.2s\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-df22b788ee50>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[0mrnd_rf_search\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparam_distribs_rf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'f1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m \u001b[0mrnd_rf_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxTrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myTrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnd_rf_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxTest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    839\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 841\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    842\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1294\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1295\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1296\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    793\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[0;32m    794\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 795\u001b[1;33m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[0;32m    796\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1044\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1045\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    857\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    778\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    596\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    599\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    385\u001b[0m             \u001b[1;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[1;31m# since correctness does not rely on using threads.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n\u001b[0m\u001b[0;32m    388\u001b[0m                              \u001b[1;33m**\u001b[0m\u001b[0m_joblib_parallel_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'threads'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m                 delayed(_parallel_build_trees)(\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1054\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1055\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    931\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    433\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#def select_file(file):\r\n",
    "    #Lectura mediante pandas\r\n",
    "f = \"data_preprocessed_python\\\\s05.dat\"\r\n",
    "subject_id = 5\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['arousal', 'dominance', 'liking'], axis=1)\r\n",
    "data['valence'] = np.where(data['valence'] >5, 'happy', 'sad')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['valence']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['valence'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "param_distribs_rf = {\r\n",
    "    'max_depth': [3, 5, 10],\r\n",
    "    'min_samples_split': [2, 5, 10]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_svm = {\r\n",
    "    'kernel': ('linear','rbf'),\r\n",
    "    'C': [0.1, 1, 10, 100]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_knn = {\r\n",
    "    'n_neighbors': [3,5,7,9],\r\n",
    "    'weights': ['uniform', 'distance']\r\n",
    "}\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_happy = precision_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_prec_sad = precision_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_prec_happy = precision_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_prec_sad = precision_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_prec_happy = precision_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_prec_sad = precision_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_happy = recall_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_rec_sad = recall_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_rec_happy = recall_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_rec_sad = recall_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_rec_happy = recall_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_rec_sad = recall_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_happy = f1_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_f1_sad = f1_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_f1_happy = f1_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_f1_sad  = f1_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_f1_happy = f1_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_f1_sad = f1_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "data_CSV = [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_happy, rf_prec_sad, rf_rec_happy, rf_rec_sad, rf_f1_happy, rf_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_happy, svm_prec_sad, svm_rec_happy, svm_rec_sad, svm_f1_happy, svm_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_happy, knn_prec_sad, knn_rec_happy, knn_rec_sad, knn_f1_happy, knn_f1_sad]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment3_1.csv\", mode=\"a\", header=False, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#def select_file(file):\r\n",
    "    #Lectura mediante pandas\r\n",
    "f = \"data_preprocessed_python\\\\s06.dat\"\r\n",
    "subject_id = 6\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['arousal', 'dominance', 'liking'], axis=1)\r\n",
    "data['valence'] = np.where(data['valence'] >5, 'happy', 'sad')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['valence']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['valence'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "param_distribs_rf = {\r\n",
    "    'max_depth': [3, 5, 10],\r\n",
    "    'min_samples_split': [2, 5, 10]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_svm = {\r\n",
    "    'kernel': ('linear','rbf'),\r\n",
    "    'C': [0.1, 1, 10, 100]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_knn = {\r\n",
    "    'n_neighbors': [3,5,7,9],\r\n",
    "    'weights': ['uniform', 'distance']\r\n",
    "}\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_happy = precision_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_prec_sad = precision_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_prec_happy = precision_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_prec_sad = precision_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_prec_happy = precision_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_prec_sad = precision_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_happy = recall_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_rec_sad = recall_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_rec_happy = recall_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_rec_sad = recall_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_rec_happy = recall_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_rec_sad = recall_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_happy = f1_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_f1_sad = f1_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_f1_happy = f1_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_f1_sad  = f1_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_f1_happy = f1_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_f1_sad = f1_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "data_CSV = [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_happy, rf_prec_sad, rf_rec_happy, rf_rec_sad, rf_f1_happy, rf_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_happy, svm_prec_sad, svm_rec_happy, svm_rec_sad, svm_f1_happy, svm_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_happy, knn_prec_sad, knn_rec_happy, knn_rec_sad, knn_f1_happy, knn_f1_sad]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment3_1.csv\", mode=\"a\", header=False, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#def select_file(file):\r\n",
    "    #Lectura mediante pandas\r\n",
    "f = \"data_preprocessed_python\\\\s07.dat\"\r\n",
    "subject_id = 7\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['arousal', 'dominance', 'liking'], axis=1)\r\n",
    "data['valence'] = np.where(data['valence'] >5, 'happy', 'sad')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['valence']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['valence'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "param_distribs_rf = {\r\n",
    "    'max_depth': [3, 5, 10],\r\n",
    "    'min_samples_split': [2, 5, 10]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_svm = {\r\n",
    "    'kernel': ('linear','rbf'),\r\n",
    "    'C': [0.1, 1, 10, 100]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_knn = {\r\n",
    "    'n_neighbors': [3,5,7,9],\r\n",
    "    'weights': ['uniform', 'distance']\r\n",
    "}\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_happy = precision_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_prec_sad = precision_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_prec_happy = precision_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_prec_sad = precision_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_prec_happy = precision_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_prec_sad = precision_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_happy = recall_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_rec_sad = recall_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_rec_happy = recall_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_rec_sad = recall_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_rec_happy = recall_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_rec_sad = recall_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_happy = f1_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_f1_sad = f1_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_f1_happy = f1_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_f1_sad  = f1_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_f1_happy = f1_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_f1_sad = f1_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "data_CSV = [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_happy, rf_prec_sad, rf_rec_happy, rf_rec_sad, rf_f1_happy, rf_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_happy, svm_prec_sad, svm_rec_happy, svm_rec_sad, svm_f1_happy, svm_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_happy, knn_prec_sad, knn_rec_happy, knn_rec_sad, knn_f1_happy, knn_f1_sad]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment3_1.csv\", mode=\"a\", header=False, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#def select_file(file):\r\n",
    "    #Lectura mediante pandas\r\n",
    "f = \"data_preprocessed_python\\\\s08.dat\"\r\n",
    "subject_id = 8\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['arousal', 'dominance', 'liking'], axis=1)\r\n",
    "data['valence'] = np.where(data['valence'] >5, 'happy', 'sad')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['valence']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['valence'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "param_distribs_rf = {\r\n",
    "    'max_depth': [3, 5, 10],\r\n",
    "    'min_samples_split': [2, 5, 10]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_svm = {\r\n",
    "    'kernel': ('linear','rbf'),\r\n",
    "    'C': [0.1, 1, 10, 100]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_knn = {\r\n",
    "    'n_neighbors': [3,5,7,9],\r\n",
    "    'weights': ['uniform', 'distance']\r\n",
    "}\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_happy = precision_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_prec_sad = precision_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_prec_happy = precision_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_prec_sad = precision_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_prec_happy = precision_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_prec_sad = precision_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_happy = recall_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_rec_sad = recall_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_rec_happy = recall_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_rec_sad = recall_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_rec_happy = recall_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_rec_sad = recall_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_happy = f1_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_f1_sad = f1_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_f1_happy = f1_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_f1_sad  = f1_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_f1_happy = f1_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_f1_sad = f1_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "data_CSV = [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_happy, rf_prec_sad, rf_rec_happy, rf_rec_sad, rf_f1_happy, rf_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_happy, svm_prec_sad, svm_rec_happy, svm_rec_sad, svm_f1_happy, svm_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_happy, knn_prec_sad, knn_rec_happy, knn_rec_sad, knn_f1_happy, knn_f1_sad]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment3_1.csv\", mode=\"a\", header=False, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#def select_file(file):\r\n",
    "    #Lectura mediante pandas\r\n",
    "f = \"data_preprocessed_python\\\\s09.dat\"\r\n",
    "subject_id = 9\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['arousal', 'dominance', 'liking'], axis=1)\r\n",
    "data['valence'] = np.where(data['valence'] >5, 'happy', 'sad')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['valence']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['valence'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "param_distribs_rf = {\r\n",
    "    'max_depth': [3, 5, 10],\r\n",
    "    'min_samples_split': [2, 5, 10]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_svm = {\r\n",
    "    'kernel': ('linear','rbf'),\r\n",
    "    'C': [0.1, 1, 10, 100]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_knn = {\r\n",
    "    'n_neighbors': [3,5,7,9],\r\n",
    "    'weights': ['uniform', 'distance']\r\n",
    "}\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_happy = precision_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_prec_sad = precision_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_prec_happy = precision_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_prec_sad = precision_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_prec_happy = precision_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_prec_sad = precision_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_happy = recall_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_rec_sad = recall_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_rec_happy = recall_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_rec_sad = recall_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_rec_happy = recall_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_rec_sad = recall_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_happy = f1_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_f1_sad = f1_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_f1_happy = f1_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_f1_sad  = f1_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_f1_happy = f1_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_f1_sad = f1_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "data_CSV = [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_happy, rf_prec_sad, rf_rec_happy, rf_rec_sad, rf_f1_happy, rf_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_happy, svm_prec_sad, svm_rec_happy, svm_rec_sad, svm_f1_happy, svm_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_happy, knn_prec_sad, knn_rec_happy, knn_rec_sad, knn_f1_happy, knn_f1_sad]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment3_1.csv\", mode=\"a\", header=False, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#def select_file(file):\r\n",
    "    #Lectura mediante pandas\r\n",
    "f = \"data_preprocessed_python\\\\s10.dat\"\r\n",
    "subject_id = 10\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['arousal', 'dominance', 'liking'], axis=1)\r\n",
    "data['valence'] = np.where(data['valence'] >5, 'happy', 'sad')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['valence']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['valence'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "param_distribs_rf = {\r\n",
    "    'max_depth': [3, 5, 10],\r\n",
    "    'min_samples_split': [2, 5, 10]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_svm = {\r\n",
    "    'kernel': ('linear','rbf'),\r\n",
    "    'C': [0.1, 1, 10, 100]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_knn = {\r\n",
    "    'n_neighbors': [3,5,7,9],\r\n",
    "    'weights': ['uniform', 'distance']\r\n",
    "}\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_happy = precision_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_prec_sad = precision_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_prec_happy = precision_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_prec_sad = precision_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_prec_happy = precision_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_prec_sad = precision_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_happy = recall_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_rec_sad = recall_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_rec_happy = recall_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_rec_sad = recall_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_rec_happy = recall_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_rec_sad = recall_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_happy = f1_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_f1_sad = f1_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_f1_happy = f1_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_f1_sad  = f1_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_f1_happy = f1_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_f1_sad = f1_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "data_CSV = [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_happy, rf_prec_sad, rf_rec_happy, rf_rec_sad, rf_f1_happy, rf_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_happy, svm_prec_sad, svm_rec_happy, svm_rec_sad, svm_f1_happy, svm_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_happy, knn_prec_sad, knn_rec_happy, knn_rec_sad, knn_f1_happy, knn_f1_sad]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment3_1.csv\", mode=\"a\", header=False, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#def select_file(file):\r\n",
    "    #Lectura mediante pandas\r\n",
    "f = \"data_preprocessed_python\\\\s11.dat\"\r\n",
    "subject_id = 11\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['arousal', 'dominance', 'liking'], axis=1)\r\n",
    "data['valence'] = np.where(data['valence'] >5, 'happy', 'sad')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['valence']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['valence'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "param_distribs_rf = {\r\n",
    "    'max_depth': [3, 5, 10],\r\n",
    "    'min_samples_split': [2, 5, 10]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_svm = {\r\n",
    "    'kernel': ('linear','rbf'),\r\n",
    "    'C': [0.1, 1, 10, 100]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_knn = {\r\n",
    "    'n_neighbors': [3,5,7,9],\r\n",
    "    'weights': ['uniform', 'distance']\r\n",
    "}\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_happy = precision_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_prec_sad = precision_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_prec_happy = precision_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_prec_sad = precision_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_prec_happy = precision_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_prec_sad = precision_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_happy = recall_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_rec_sad = recall_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_rec_happy = recall_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_rec_sad = recall_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_rec_happy = recall_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_rec_sad = recall_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_happy = f1_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_f1_sad = f1_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_f1_happy = f1_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_f1_sad  = f1_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_f1_happy = f1_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_f1_sad = f1_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "data_CSV = [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_happy, rf_prec_sad, rf_rec_happy, rf_rec_sad, rf_f1_happy, rf_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_happy, svm_prec_sad, svm_rec_happy, svm_rec_sad, svm_f1_happy, svm_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_happy, knn_prec_sad, knn_rec_happy, knn_rec_sad, knn_f1_happy, knn_f1_sad]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment3_1.csv\", mode=\"a\", header=False, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#def select_file(file):\r\n",
    "    #Lectura mediante pandas\r\n",
    "f = \"data_preprocessed_python\\\\s12.dat\"\r\n",
    "subject_id = 12\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['arousal', 'dominance', 'liking'], axis=1)\r\n",
    "data['valence'] = np.where(data['valence'] >5, 'happy', 'sad')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['valence']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['valence'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "param_distribs_rf = {\r\n",
    "    'max_depth': [3, 5, 10],\r\n",
    "    'min_samples_split': [2, 5, 10]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_svm = {\r\n",
    "    'kernel': ('linear','rbf'),\r\n",
    "    'C': [0.1, 1, 10, 100]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_knn = {\r\n",
    "    'n_neighbors': [3,5,7,9],\r\n",
    "    'weights': ['uniform', 'distance']\r\n",
    "}\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_happy = precision_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_prec_sad = precision_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_prec_happy = precision_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_prec_sad = precision_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_prec_happy = precision_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_prec_sad = precision_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_happy = recall_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_rec_sad = recall_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_rec_happy = recall_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_rec_sad = recall_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_rec_happy = recall_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_rec_sad = recall_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_happy = f1_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_f1_sad = f1_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_f1_happy = f1_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_f1_sad  = f1_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_f1_happy = f1_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_f1_sad = f1_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "data_CSV = [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_happy, rf_prec_sad, rf_rec_happy, rf_rec_sad, rf_f1_happy, rf_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_happy, svm_prec_sad, svm_rec_happy, svm_rec_sad, svm_f1_happy, svm_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_happy, knn_prec_sad, knn_rec_happy, knn_rec_sad, knn_f1_happy, knn_f1_sad]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment3_1.csv\", mode=\"a\", header=False, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#def select_file(file):\r\n",
    "    #Lectura mediante pandas\r\n",
    "f = \"data_preprocessed_python\\\\s13.dat\"\r\n",
    "subject_id = 13\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['arousal', 'dominance', 'liking'], axis=1)\r\n",
    "data['valence'] = np.where(data['valence'] >5, 'happy', 'sad')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['valence']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['valence'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "param_distribs_rf = {\r\n",
    "    'max_depth': [3, 5, 10],\r\n",
    "    'min_samples_split': [2, 5, 10]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_svm = {\r\n",
    "    'kernel': ('linear','rbf'),\r\n",
    "    'C': [0.1, 1, 10, 100]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_knn = {\r\n",
    "    'n_neighbors': [3,5,7,9],\r\n",
    "    'weights': ['uniform', 'distance']\r\n",
    "}\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_happy = precision_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_prec_sad = precision_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_prec_happy = precision_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_prec_sad = precision_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_prec_happy = precision_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_prec_sad = precision_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_happy = recall_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_rec_sad = recall_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_rec_happy = recall_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_rec_sad = recall_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_rec_happy = recall_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_rec_sad = recall_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_happy = f1_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_f1_sad = f1_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_f1_happy = f1_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_f1_sad  = f1_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_f1_happy = f1_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_f1_sad = f1_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "data_CSV = [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_happy, rf_prec_sad, rf_rec_happy, rf_rec_sad, rf_f1_happy, rf_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_happy, svm_prec_sad, svm_rec_happy, svm_rec_sad, svm_f1_happy, svm_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_happy, knn_prec_sad, knn_rec_happy, knn_rec_sad, knn_f1_happy, knn_f1_sad]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment3_1.csv\", mode=\"a\", header=False, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#def select_file(file):\r\n",
    "    #Lectura mediante pandas\r\n",
    "f = \"data_preprocessed_python\\\\s14.dat\"\r\n",
    "subject_id = 14\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['arousal', 'dominance', 'liking'], axis=1)\r\n",
    "data['valence'] = np.where(data['valence'] >5, 'happy', 'sad')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['valence']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['valence'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "param_distribs_rf = {\r\n",
    "    'max_depth': [3, 5, 10],\r\n",
    "    'min_samples_split': [2, 5, 10]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_svm = {\r\n",
    "    'kernel': ('linear','rbf'),\r\n",
    "    'C': [0.1, 1, 10, 100]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_knn = {\r\n",
    "    'n_neighbors': [3,5,7,9],\r\n",
    "    'weights': ['uniform', 'distance']\r\n",
    "}\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_happy = precision_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_prec_sad = precision_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_prec_happy = precision_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_prec_sad = precision_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_prec_happy = precision_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_prec_sad = precision_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_happy = recall_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_rec_sad = recall_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_rec_happy = recall_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_rec_sad = recall_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_rec_happy = recall_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_rec_sad = recall_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_happy = f1_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_f1_sad = f1_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_f1_happy = f1_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_f1_sad  = f1_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_f1_happy = f1_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_f1_sad = f1_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "data_CSV = [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_happy, rf_prec_sad, rf_rec_happy, rf_rec_sad, rf_f1_happy, rf_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_happy, svm_prec_sad, svm_rec_happy, svm_rec_sad, svm_f1_happy, svm_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_happy, knn_prec_sad, knn_rec_happy, knn_rec_sad, knn_f1_happy, knn_f1_sad]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment3_1.csv\", mode=\"a\", header=False, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#def select_file(file):\r\n",
    "    #Lectura mediante pandas\r\n",
    "f = \"data_preprocessed_python\\\\s15.dat\"\r\n",
    "subject_id = 15\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['arousal', 'dominance', 'liking'], axis=1)\r\n",
    "data['valence'] = np.where(data['valence'] >5, 'happy', 'sad')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['valence']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['valence'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "param_distribs_rf = {\r\n",
    "    'max_depth': [3, 5, 10],\r\n",
    "    'min_samples_split': [2, 5, 10]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_svm = {\r\n",
    "    'kernel': ('linear','rbf'),\r\n",
    "    'C': [0.1, 1, 10, 100]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_knn = {\r\n",
    "    'n_neighbors': [3,5,7,9],\r\n",
    "    'weights': ['uniform', 'distance']\r\n",
    "}\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_happy = precision_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_prec_sad = precision_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_prec_happy = precision_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_prec_sad = precision_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_prec_happy = precision_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_prec_sad = precision_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_happy = recall_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_rec_sad = recall_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_rec_happy = recall_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_rec_sad = recall_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_rec_happy = recall_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_rec_sad = recall_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_happy = f1_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_f1_sad = f1_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_f1_happy = f1_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_f1_sad  = f1_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_f1_happy = f1_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_f1_sad = f1_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "data_CSV = [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_happy, rf_prec_sad, rf_rec_happy, rf_rec_sad, rf_f1_happy, rf_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_happy, svm_prec_sad, svm_rec_happy, svm_rec_sad, svm_f1_happy, svm_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_happy, knn_prec_sad, knn_rec_happy, knn_rec_sad, knn_f1_happy, knn_f1_sad]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment3_1.csv\", mode=\"a\", header=False, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#def select_file(file):\r\n",
    "    #Lectura mediante pandas\r\n",
    "f = \"data_preprocessed_python\\\\s16.dat\"\r\n",
    "subject_id = 16\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['arousal', 'dominance', 'liking'], axis=1)\r\n",
    "data['valence'] = np.where(data['valence'] >5, 'happy', 'sad')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['valence']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['valence'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "param_distribs_rf = {\r\n",
    "    'max_depth': [3, 5, 10],\r\n",
    "    'min_samples_split': [2, 5, 10]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_svm = {\r\n",
    "    'kernel': ('linear','rbf'),\r\n",
    "    'C': [0.1, 1, 10, 100]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_knn = {\r\n",
    "    'n_neighbors': [3,5,7,9],\r\n",
    "    'weights': ['uniform', 'distance']\r\n",
    "}\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_happy = precision_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_prec_sad = precision_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_prec_happy = precision_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_prec_sad = precision_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_prec_happy = precision_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_prec_sad = precision_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_happy = recall_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_rec_sad = recall_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_rec_happy = recall_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_rec_sad = recall_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_rec_happy = recall_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_rec_sad = recall_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_happy = f1_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_f1_sad = f1_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_f1_happy = f1_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_f1_sad  = f1_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_f1_happy = f1_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_f1_sad = f1_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "data_CSV = [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_happy, rf_prec_sad, rf_rec_happy, rf_rec_sad, rf_f1_happy, rf_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_happy, svm_prec_sad, svm_rec_happy, svm_rec_sad, svm_f1_happy, svm_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_happy, knn_prec_sad, knn_rec_happy, knn_rec_sad, knn_f1_happy, knn_f1_sad]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment3_1.csv\", mode=\"a\", header=False, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#def select_file(file):\r\n",
    "    #Lectura mediante pandas\r\n",
    "f = \"data_preprocessed_python\\\\s17.dat\"\r\n",
    "subject_id = 17\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['arousal', 'dominance', 'liking'], axis=1)\r\n",
    "data['valence'] = np.where(data['valence'] >5, 'happy', 'sad')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['valence']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['valence'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "param_distribs_rf = {\r\n",
    "    'max_depth': [3, 5, 10],\r\n",
    "    'min_samples_split': [2, 5, 10]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_svm = {\r\n",
    "    'kernel': ('linear','rbf'),\r\n",
    "    'C': [0.1, 1, 10, 100]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_knn = {\r\n",
    "    'n_neighbors': [3,5,7,9],\r\n",
    "    'weights': ['uniform', 'distance']\r\n",
    "}\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_happy = precision_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_prec_sad = precision_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_prec_happy = precision_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_prec_sad = precision_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_prec_happy = precision_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_prec_sad = precision_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_happy = recall_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_rec_sad = recall_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_rec_happy = recall_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_rec_sad = recall_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_rec_happy = recall_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_rec_sad = recall_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_happy = f1_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_f1_sad = f1_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_f1_happy = f1_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_f1_sad  = f1_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_f1_happy = f1_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_f1_sad = f1_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "data_CSV = [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_happy, rf_prec_sad, rf_rec_happy, rf_rec_sad, rf_f1_happy, rf_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_happy, svm_prec_sad, svm_rec_happy, svm_rec_sad, svm_f1_happy, svm_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_happy, knn_prec_sad, knn_rec_happy, knn_rec_sad, knn_f1_happy, knn_f1_sad]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment3_1.csv\", mode=\"a\", header=False, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#def select_file(file):\r\n",
    "    #Lectura mediante pandas\r\n",
    "f = \"data_preprocessed_python\\\\s18.dat\"\r\n",
    "subject_id = 18\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['arousal', 'dominance', 'liking'], axis=1)\r\n",
    "data['valence'] = np.where(data['valence'] >5, 'happy', 'sad')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['valence']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['valence'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "param_distribs_rf = {\r\n",
    "    'max_depth': [3, 5, 10],\r\n",
    "    'min_samples_split': [2, 5, 10]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_svm = {\r\n",
    "    'kernel': ('linear','rbf'),\r\n",
    "    'C': [0.1, 1, 10, 100]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_knn = {\r\n",
    "    'n_neighbors': [3,5,7,9],\r\n",
    "    'weights': ['uniform', 'distance']\r\n",
    "}\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_happy = precision_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_prec_sad = precision_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_prec_happy = precision_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_prec_sad = precision_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_prec_happy = precision_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_prec_sad = precision_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_happy = recall_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_rec_sad = recall_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_rec_happy = recall_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_rec_sad = recall_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_rec_happy = recall_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_rec_sad = recall_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_happy = f1_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_f1_sad = f1_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_f1_happy = f1_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_f1_sad  = f1_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_f1_happy = f1_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_f1_sad = f1_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "data_CSV = [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_happy, rf_prec_sad, rf_rec_happy, rf_rec_sad, rf_f1_happy, rf_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_happy, svm_prec_sad, svm_rec_happy, svm_rec_sad, svm_f1_happy, svm_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_happy, knn_prec_sad, knn_rec_happy, knn_rec_sad, knn_f1_happy, knn_f1_sad]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment3_1.csv\", mode=\"a\", header=False, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#def select_file(file):\r\n",
    "    #Lectura mediante pandas\r\n",
    "f = \"data_preprocessed_python\\\\s19.dat\"\r\n",
    "subject_id = 19\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['arousal', 'dominance', 'liking'], axis=1)\r\n",
    "data['valence'] = np.where(data['valence'] >5, 'happy', 'sad')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['valence']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['valence'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "param_distribs_rf = {\r\n",
    "    'max_depth': [3, 5, 10],\r\n",
    "    'min_samples_split': [2, 5, 10]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_svm = {\r\n",
    "    'kernel': ('linear','rbf'),\r\n",
    "    'C': [0.1, 1, 10, 100]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_knn = {\r\n",
    "    'n_neighbors': [3,5,7,9],\r\n",
    "    'weights': ['uniform', 'distance']\r\n",
    "}\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_happy = precision_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_prec_sad = precision_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_prec_happy = precision_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_prec_sad = precision_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_prec_happy = precision_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_prec_sad = precision_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_happy = recall_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_rec_sad = recall_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_rec_happy = recall_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_rec_sad = recall_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_rec_happy = recall_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_rec_sad = recall_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_happy = f1_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_f1_sad = f1_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_f1_happy = f1_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_f1_sad  = f1_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_f1_happy = f1_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_f1_sad = f1_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "data_CSV = [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_happy, rf_prec_sad, rf_rec_happy, rf_rec_sad, rf_f1_happy, rf_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_happy, svm_prec_sad, svm_rec_happy, svm_rec_sad, svm_f1_happy, svm_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_happy, knn_prec_sad, knn_rec_happy, knn_rec_sad, knn_f1_happy, knn_f1_sad]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment3_1.csv\", mode=\"a\", header=False, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#def select_file(file):\r\n",
    "    #Lectura mediante pandas\r\n",
    "f = \"data_preprocessed_python\\\\s20.dat\"\r\n",
    "subject_id = 20\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['arousal', 'dominance', 'liking'], axis=1)\r\n",
    "data['valence'] = np.where(data['valence'] >5, 'happy', 'sad')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['valence']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['valence'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "param_distribs_rf = {\r\n",
    "    'max_depth': [3, 5, 10],\r\n",
    "    'min_samples_split': [2, 5, 10]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_svm = {\r\n",
    "    'kernel': ('linear','rbf'),\r\n",
    "    'C': [0.1, 1, 10, 100]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_knn = {\r\n",
    "    'n_neighbors': [3,5,7,9],\r\n",
    "    'weights': ['uniform', 'distance']\r\n",
    "}\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_happy = precision_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_prec_sad = precision_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_prec_happy = precision_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_prec_sad = precision_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_prec_happy = precision_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_prec_sad = precision_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_happy = recall_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_rec_sad = recall_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_rec_happy = recall_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_rec_sad = recall_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_rec_happy = recall_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_rec_sad = recall_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_happy = f1_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_f1_sad = f1_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_f1_happy = f1_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_f1_sad  = f1_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_f1_happy = f1_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_f1_sad = f1_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "data_CSV = [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_happy, rf_prec_sad, rf_rec_happy, rf_rec_sad, rf_f1_happy, rf_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_happy, svm_prec_sad, svm_rec_happy, svm_rec_sad, svm_f1_happy, svm_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_happy, knn_prec_sad, knn_rec_happy, knn_rec_sad, knn_f1_happy, knn_f1_sad]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment3_1.csv\", mode=\"a\", header=False, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#def select_file(file):\r\n",
    "    #Lectura mediante pandas\r\n",
    "f = \"data_preprocessed_python\\\\s21.dat\"\r\n",
    "subject_id = 21\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['arousal', 'dominance', 'liking'], axis=1)\r\n",
    "data['valence'] = np.where(data['valence'] >5, 'happy', 'sad')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['valence']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['valence'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "param_distribs_rf = {\r\n",
    "    'max_depth': [3, 5, 10],\r\n",
    "    'min_samples_split': [2, 5, 10]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_svm = {\r\n",
    "    'kernel': ('linear','rbf'),\r\n",
    "    'C': [0.1, 1, 10, 100]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_knn = {\r\n",
    "    'n_neighbors': [3,5,7,9],\r\n",
    "    'weights': ['uniform', 'distance']\r\n",
    "}\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_happy = precision_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_prec_sad = precision_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_prec_happy = precision_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_prec_sad = precision_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_prec_happy = precision_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_prec_sad = precision_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_happy = recall_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_rec_sad = recall_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_rec_happy = recall_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_rec_sad = recall_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_rec_happy = recall_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_rec_sad = recall_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_happy = f1_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_f1_sad = f1_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_f1_happy = f1_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_f1_sad  = f1_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_f1_happy = f1_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_f1_sad = f1_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "data_CSV = [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_happy, rf_prec_sad, rf_rec_happy, rf_rec_sad, rf_f1_happy, rf_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_happy, svm_prec_sad, svm_rec_happy, svm_rec_sad, svm_f1_happy, svm_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_happy, knn_prec_sad, knn_rec_happy, knn_rec_sad, knn_f1_happy, knn_f1_sad]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment3_1.csv\", mode=\"a\", header=False, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#def select_file(file):\r\n",
    "    #Lectura mediante pandas\r\n",
    "f = \"data_preprocessed_python\\\\s22.dat\"\r\n",
    "subject_id = 22\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['arousal', 'dominance', 'liking'], axis=1)\r\n",
    "data['valence'] = np.where(data['valence'] >5, 'happy', 'sad')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['valence']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['valence'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "param_distribs_rf = {\r\n",
    "    'max_depth': [3, 5, 10],\r\n",
    "    'min_samples_split': [2, 5, 10]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_svm = {\r\n",
    "    'kernel': ('linear','rbf'),\r\n",
    "    'C': [0.1, 1, 10, 100]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_knn = {\r\n",
    "    'n_neighbors': [3,5,7,9],\r\n",
    "    'weights': ['uniform', 'distance']\r\n",
    "}\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_happy = precision_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_prec_sad = precision_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_prec_happy = precision_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_prec_sad = precision_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_prec_happy = precision_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_prec_sad = precision_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_happy = recall_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_rec_sad = recall_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_rec_happy = recall_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_rec_sad = recall_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_rec_happy = recall_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_rec_sad = recall_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_happy = f1_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_f1_sad = f1_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_f1_happy = f1_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_f1_sad  = f1_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_f1_happy = f1_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_f1_sad = f1_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "data_CSV = [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_happy, rf_prec_sad, rf_rec_happy, rf_rec_sad, rf_f1_happy, rf_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_happy, svm_prec_sad, svm_rec_happy, svm_rec_sad, svm_f1_happy, svm_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_happy, knn_prec_sad, knn_rec_happy, knn_rec_sad, knn_f1_happy, knn_f1_sad]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment3_1.csv\", mode=\"a\", header=False, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#def select_file(file):\r\n",
    "    #Lectura mediante pandas\r\n",
    "f = \"data_preprocessed_python\\\\s23.dat\"\r\n",
    "subject_id = 23\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['arousal', 'dominance', 'liking'], axis=1)\r\n",
    "data['valence'] = np.where(data['valence'] >5, 'happy', 'sad')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['valence']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['valence'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "param_distribs_rf = {\r\n",
    "    'max_depth': [3, 5, 10],\r\n",
    "    'min_samples_split': [2, 5, 10]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_svm = {\r\n",
    "    'kernel': ('linear','rbf'),\r\n",
    "    'C': [0.1, 1, 10, 100]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_knn = {\r\n",
    "    'n_neighbors': [3,5,7,9],\r\n",
    "    'weights': ['uniform', 'distance']\r\n",
    "}\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_happy = precision_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_prec_sad = precision_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_prec_happy = precision_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_prec_sad = precision_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_prec_happy = precision_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_prec_sad = precision_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_happy = recall_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_rec_sad = recall_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_rec_happy = recall_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_rec_sad = recall_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_rec_happy = recall_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_rec_sad = recall_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_happy = f1_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_f1_sad = f1_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_f1_happy = f1_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_f1_sad  = f1_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_f1_happy = f1_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_f1_sad = f1_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "data_CSV = [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_happy, rf_prec_sad, rf_rec_happy, rf_rec_sad, rf_f1_happy, rf_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_happy, svm_prec_sad, svm_rec_happy, svm_rec_sad, svm_f1_happy, svm_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_happy, knn_prec_sad, knn_rec_happy, knn_rec_sad, knn_f1_happy, knn_f1_sad]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment3_1.csv\", mode=\"a\", header=False, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#def select_file(file):\r\n",
    "    #Lectura mediante pandas\r\n",
    "f = \"data_preprocessed_python\\\\s24.dat\"\r\n",
    "subject_id = 24\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['arousal', 'dominance', 'liking'], axis=1)\r\n",
    "data['valence'] = np.where(data['valence'] >5, 'happy', 'sad')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['valence']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['valence'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "param_distribs_rf = {\r\n",
    "    'max_depth': [3, 5, 10],\r\n",
    "    'min_samples_split': [2, 5, 10]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_svm = {\r\n",
    "    'kernel': ('linear','rbf'),\r\n",
    "    'C': [0.1, 1, 10, 100]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_knn = {\r\n",
    "    'n_neighbors': [3,5,7,9],\r\n",
    "    'weights': ['uniform', 'distance']\r\n",
    "}\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_happy = precision_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_prec_sad = precision_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_prec_happy = precision_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_prec_sad = precision_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_prec_happy = precision_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_prec_sad = precision_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_happy = recall_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_rec_sad = recall_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_rec_happy = recall_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_rec_sad = recall_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_rec_happy = recall_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_rec_sad = recall_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_happy = f1_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_f1_sad = f1_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_f1_happy = f1_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_f1_sad  = f1_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_f1_happy = f1_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_f1_sad = f1_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "data_CSV = [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_happy, rf_prec_sad, rf_rec_happy, rf_rec_sad, rf_f1_happy, rf_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_happy, svm_prec_sad, svm_rec_happy, svm_rec_sad, svm_f1_happy, svm_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_happy, knn_prec_sad, knn_rec_happy, knn_rec_sad, knn_f1_happy, knn_f1_sad]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment3_1.csv\", mode=\"a\", header=False, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#def select_file(file):\r\n",
    "    #Lectura mediante pandas\r\n",
    "f = \"data_preprocessed_python\\\\s25.dat\"\r\n",
    "subject_id = 25\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['arousal', 'dominance', 'liking'], axis=1)\r\n",
    "data['valence'] = np.where(data['valence'] >5, 'happy', 'sad')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['valence']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['valence'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "param_distribs_rf = {\r\n",
    "    'max_depth': [3, 5, 10],\r\n",
    "    'min_samples_split': [2, 5, 10]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_svm = {\r\n",
    "    'kernel': ('linear','rbf'),\r\n",
    "    'C': [0.1, 1, 10, 100]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_knn = {\r\n",
    "    'n_neighbors': [3,5,7,9],\r\n",
    "    'weights': ['uniform', 'distance']\r\n",
    "}\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_happy = precision_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_prec_sad = precision_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_prec_happy = precision_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_prec_sad = precision_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_prec_happy = precision_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_prec_sad = precision_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_happy = recall_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_rec_sad = recall_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_rec_happy = recall_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_rec_sad = recall_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_rec_happy = recall_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_rec_sad = recall_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_happy = f1_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_f1_sad = f1_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_f1_happy = f1_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_f1_sad  = f1_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_f1_happy = f1_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_f1_sad = f1_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "data_CSV = [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_happy, rf_prec_sad, rf_rec_happy, rf_rec_sad, rf_f1_happy, rf_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_happy, svm_prec_sad, svm_rec_happy, svm_rec_sad, svm_f1_happy, svm_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_happy, knn_prec_sad, knn_rec_happy, knn_rec_sad, knn_f1_happy, knn_f1_sad]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment3_1.csv\", mode=\"a\", header=False, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#def select_file(file):\r\n",
    "    #Lectura mediante pandas\r\n",
    "f = \"data_preprocessed_python\\\\s26.dat\"\r\n",
    "subject_id = 26\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['arousal', 'dominance', 'liking'], axis=1)\r\n",
    "data['valence'] = np.where(data['valence'] >5, 'happy', 'sad')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['valence']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['valence'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "param_distribs_rf = {\r\n",
    "    'max_depth': [3, 5, 10],\r\n",
    "    'min_samples_split': [2, 5, 10]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_svm = {\r\n",
    "    'kernel': ('linear','rbf'),\r\n",
    "    'C': [0.1, 1, 10, 100]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_knn = {\r\n",
    "    'n_neighbors': [3,5,7,9],\r\n",
    "    'weights': ['uniform', 'distance']\r\n",
    "}\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_happy = precision_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_prec_sad = precision_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_prec_happy = precision_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_prec_sad = precision_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_prec_happy = precision_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_prec_sad = precision_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_happy = recall_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_rec_sad = recall_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_rec_happy = recall_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_rec_sad = recall_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_rec_happy = recall_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_rec_sad = recall_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_happy = f1_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_f1_sad = f1_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_f1_happy = f1_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_f1_sad  = f1_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_f1_happy = f1_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_f1_sad = f1_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "data_CSV = [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_happy, rf_prec_sad, rf_rec_happy, rf_rec_sad, rf_f1_happy, rf_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_happy, svm_prec_sad, svm_rec_happy, svm_rec_sad, svm_f1_happy, svm_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_happy, knn_prec_sad, knn_rec_happy, knn_rec_sad, knn_f1_happy, knn_f1_sad]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment3_1.csv\", mode=\"a\", header=False, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#def select_file(file):\r\n",
    "    #Lectura mediante pandas\r\n",
    "f = \"data_preprocessed_python\\\\s27.dat\"\r\n",
    "subject_id = 27\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['arousal', 'dominance', 'liking'], axis=1)\r\n",
    "data['valence'] = np.where(data['valence'] >5, 'happy', 'sad')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['valence']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['valence'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "param_distribs_rf = {\r\n",
    "    'max_depth': [3, 5, 10],\r\n",
    "    'min_samples_split': [2, 5, 10]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_svm = {\r\n",
    "    'kernel': ('linear','rbf'),\r\n",
    "    'C': [0.1, 1, 10, 100]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_knn = {\r\n",
    "    'n_neighbors': [3,5,7,9],\r\n",
    "    'weights': ['uniform', 'distance']\r\n",
    "}\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_happy = precision_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_prec_sad = precision_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_prec_happy = precision_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_prec_sad = precision_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_prec_happy = precision_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_prec_sad = precision_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_happy = recall_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_rec_sad = recall_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_rec_happy = recall_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_rec_sad = recall_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_rec_happy = recall_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_rec_sad = recall_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_happy = f1_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_f1_sad = f1_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_f1_happy = f1_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_f1_sad  = f1_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_f1_happy = f1_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_f1_sad = f1_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "data_CSV = [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_happy, rf_prec_sad, rf_rec_happy, rf_rec_sad, rf_f1_happy, rf_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_happy, svm_prec_sad, svm_rec_happy, svm_rec_sad, svm_f1_happy, svm_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_happy, knn_prec_sad, knn_rec_happy, knn_rec_sad, knn_f1_happy, knn_f1_sad]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment3_1.csv\", mode=\"a\", header=False, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#def select_file(file):\r\n",
    "    #Lectura mediante pandas\r\n",
    "f = \"data_preprocessed_python\\\\s28.dat\"\r\n",
    "subject_id = 28\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['arousal', 'dominance', 'liking'], axis=1)\r\n",
    "data['valence'] = np.where(data['valence'] >5, 'happy', 'sad')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['valence']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['valence'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "param_distribs_rf = {\r\n",
    "    'max_depth': [3, 5, 10],\r\n",
    "    'min_samples_split': [2, 5, 10]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_svm = {\r\n",
    "    'kernel': ('linear','rbf'),\r\n",
    "    'C': [0.1, 1, 10, 100]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_knn = {\r\n",
    "    'n_neighbors': [3,5,7,9],\r\n",
    "    'weights': ['uniform', 'distance']\r\n",
    "}\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_happy = precision_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_prec_sad = precision_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_prec_happy = precision_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_prec_sad = precision_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_prec_happy = precision_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_prec_sad = precision_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_happy = recall_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_rec_sad = recall_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_rec_happy = recall_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_rec_sad = recall_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_rec_happy = recall_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_rec_sad = recall_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_happy = f1_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_f1_sad = f1_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_f1_happy = f1_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_f1_sad  = f1_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_f1_happy = f1_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_f1_sad = f1_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "data_CSV = [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_happy, rf_prec_sad, rf_rec_happy, rf_rec_sad, rf_f1_happy, rf_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_happy, svm_prec_sad, svm_rec_happy, svm_rec_sad, svm_f1_happy, svm_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_happy, knn_prec_sad, knn_rec_happy, knn_rec_sad, knn_f1_happy, knn_f1_sad]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment3_1.csv\", mode=\"a\", header=False, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#def select_file(file):\r\n",
    "    #Lectura mediante pandas\r\n",
    "f = \"data_preprocessed_python\\\\s29.dat\"\r\n",
    "subject_id = 29\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['arousal', 'dominance', 'liking'], axis=1)\r\n",
    "data['valence'] = np.where(data['valence'] >5, 'happy', 'sad')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['valence']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['valence'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "param_distribs_rf = {\r\n",
    "    'max_depth': [3, 5, 10],\r\n",
    "    'min_samples_split': [2, 5, 10]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_svm = {\r\n",
    "    'kernel': ('linear','rbf'),\r\n",
    "    'C': [0.1, 1, 10, 100]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_knn = {\r\n",
    "    'n_neighbors': [3,5,7,9],\r\n",
    "    'weights': ['uniform', 'distance']\r\n",
    "}\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_happy = precision_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_prec_sad = precision_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_prec_happy = precision_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_prec_sad = precision_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_prec_happy = precision_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_prec_sad = precision_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_happy = recall_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_rec_sad = recall_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_rec_happy = recall_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_rec_sad = recall_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_rec_happy = recall_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_rec_sad = recall_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_happy = f1_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_f1_sad = f1_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_f1_happy = f1_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_f1_sad  = f1_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_f1_happy = f1_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_f1_sad = f1_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "data_CSV = [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_happy, rf_prec_sad, rf_rec_happy, rf_rec_sad, rf_f1_happy, rf_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_happy, svm_prec_sad, svm_rec_happy, svm_rec_sad, svm_f1_happy, svm_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_happy, knn_prec_sad, knn_rec_happy, knn_rec_sad, knn_f1_happy, knn_f1_sad]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment3_1.csv\", mode=\"a\", header=False, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#def select_file(file):\r\n",
    "    #Lectura mediante pandas\r\n",
    "f = \"data_preprocessed_python\\\\s30.dat\"\r\n",
    "subject_id = 30\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['arousal', 'dominance', 'liking'], axis=1)\r\n",
    "data['valence'] = np.where(data['valence'] >5, 'happy', 'sad')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['valence']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['valence'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "param_distribs_rf = {\r\n",
    "    'max_depth': [3, 5, 10],\r\n",
    "    'min_samples_split': [2, 5, 10]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_svm = {\r\n",
    "    'kernel': ('linear','rbf'),\r\n",
    "    'C': [0.1, 1, 10, 100]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_knn = {\r\n",
    "    'n_neighbors': [3,5,7,9],\r\n",
    "    'weights': ['uniform', 'distance']\r\n",
    "}\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_happy = precision_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_prec_sad = precision_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_prec_happy = precision_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_prec_sad = precision_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_prec_happy = precision_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_prec_sad = precision_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_happy = recall_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_rec_sad = recall_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_rec_happy = recall_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_rec_sad = recall_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_rec_happy = recall_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_rec_sad = recall_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_happy = f1_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_f1_sad = f1_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_f1_happy = f1_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_f1_sad  = f1_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_f1_happy = f1_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_f1_sad = f1_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "data_CSV = [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_happy, rf_prec_sad, rf_rec_happy, rf_rec_sad, rf_f1_happy, rf_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_happy, svm_prec_sad, svm_rec_happy, svm_rec_sad, svm_f1_happy, svm_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_happy, knn_prec_sad, knn_rec_happy, knn_rec_sad, knn_f1_happy, knn_f1_sad]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment3_1.csv\", mode=\"a\", header=False, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#def select_file(file):\r\n",
    "    #Lectura mediante pandas\r\n",
    "f = \"data_preprocessed_python\\\\s31.dat\"\r\n",
    "subject_id = 31\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['arousal', 'dominance', 'liking'], axis=1)\r\n",
    "data['valence'] = np.where(data['valence'] >5, 'happy', 'sad')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['valence']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['valence'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "param_distribs_rf = {\r\n",
    "    'max_depth': [3, 5, 10],\r\n",
    "    'min_samples_split': [2, 5, 10]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_svm = {\r\n",
    "    'kernel': ('linear','rbf'),\r\n",
    "    'C': [0.1, 1, 10, 100]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_knn = {\r\n",
    "    'n_neighbors': [3,5,7,9],\r\n",
    "    'weights': ['uniform', 'distance']\r\n",
    "}\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_happy = precision_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_prec_sad = precision_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_prec_happy = precision_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_prec_sad = precision_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_prec_happy = precision_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_prec_sad = precision_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_happy = recall_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_rec_sad = recall_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_rec_happy = recall_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_rec_sad = recall_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_rec_happy = recall_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_rec_sad = recall_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_happy = f1_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_f1_sad = f1_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_f1_happy = f1_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_f1_sad  = f1_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_f1_happy = f1_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_f1_sad = f1_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "data_CSV = [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_happy, rf_prec_sad, rf_rec_happy, rf_rec_sad, rf_f1_happy, rf_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_happy, svm_prec_sad, svm_rec_happy, svm_rec_sad, svm_f1_happy, svm_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_happy, knn_prec_sad, knn_rec_happy, knn_rec_sad, knn_f1_happy, knn_f1_sad]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment3_1.csv\", mode=\"a\", header=False, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#def select_file(file):\r\n",
    "    #Lectura mediante pandas\r\n",
    "f = \"data_preprocessed_python\\\\s32.dat\"\r\n",
    "subject_id = 32\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['arousal', 'dominance', 'liking'], axis=1)\r\n",
    "data['valence'] = np.where(data['valence'] >5, 'happy', 'sad')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['valence']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['valence'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "param_distribs_rf = {\r\n",
    "    'max_depth': [3, 5, 10],\r\n",
    "    'min_samples_split': [2, 5, 10]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_svm = {\r\n",
    "    'kernel': ('linear','rbf'),\r\n",
    "    'C': [0.1, 1, 10, 100]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_knn = {\r\n",
    "    'n_neighbors': [3,5,7,9],\r\n",
    "    'weights': ['uniform', 'distance']\r\n",
    "}\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_happy = precision_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_prec_sad = precision_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_prec_happy = precision_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_prec_sad = precision_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_prec_happy = precision_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_prec_sad = precision_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_happy = recall_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_rec_sad = recall_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_rec_happy = recall_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_rec_sad = recall_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_rec_happy = recall_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_rec_sad = recall_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_happy = f1_score(yTest, predict, pos_label=\"happy\")\r\n",
    "rf_f1_sad = f1_score(yTest, predict, pos_label=\"sad\")\r\n",
    "svm_f1_happy = f1_score(yTest, svm_predict, pos_label=\"happy\")\r\n",
    "svm_f1_sad  = f1_score(yTest, svm_predict, pos_label=\"sad\")\r\n",
    "knn_f1_happy = f1_score(yTest, knn_predict, pos_label=\"happy\")\r\n",
    "knn_f1_sad = f1_score(yTest, knn_predict, pos_label=\"sad\")\r\n",
    "\r\n",
    "data_CSV = [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_happy, rf_prec_sad, rf_rec_happy, rf_rec_sad, rf_f1_happy, rf_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_happy, svm_prec_sad, svm_rec_happy, svm_rec_sad, svm_f1_happy, svm_f1_sad], [subject_id, 3, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_happy, knn_prec_sad, knn_rec_happy, knn_rec_sad, knn_f1_happy, knn_f1_sad]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment3_1.csv\", mode=\"a\", header=False, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}