{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit"
  },
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "1 subject. Feature extraction. Classification joining all frequency bands."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import csv\r\n",
    "\r\n",
    "#def select_file(file):\r\n",
    "    #Lectura mediante pandas\r\n",
    "f = \"data_preprocessed_python\\\\s01.dat\"\r\n",
    "subject_id = 1\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "    # Frecuencia de muestreo\r\n",
    "    fs = 128\r\n",
    "    # Window\r\n",
    "    window = \"hann\"\r\n",
    "    # Length of each segment\r\n",
    "    # nperseg = 256 por defecto\r\n",
    "    # noverlap\r\n",
    "    # Por defecto a None, if None : noverlap = nperseg / 2\r\n",
    "    # [...]\r\n",
    "\r\n",
    "    # Definicion de bandas\r\n",
    "    eeg_bands = {'Delta': (1, 4),\r\n",
    "                'Theta': (4, 8),\r\n",
    "                'Alpha': (8, 14),\r\n",
    "                'Beta': (14, 30),\r\n",
    "                'Gamma': (30, 50)}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# FOURIER\r\n",
    "import scipy\r\n",
    "from scipy import signal\r\n",
    "\r\n",
    "# SE COGE EL PRIMER VÍDEOS, Df_VIDEOS[0]\r\n",
    "def do_stft(video, channel):\r\n",
    "    # Array \r\n",
    "    x = df_videos[video][channel]\r\n",
    "\r\n",
    "    # Array of the sample frequency, Array of the segment times, STFT of x\r\n",
    "    f, t , Zxx = scipy.signal.stft(x, fs, window)\r\n",
    "    return Zxx\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#SEPARACION EN LAS DISTINTAS BANDAS DE FRECUENCIA\r\n",
    "\r\n",
    "def make_bands(Zxx):\r\n",
    "    #Obtener valores reales de STFT, solo positivos.\r\n",
    "    values = np.absolute(Zxx)\r\n",
    "\r\n",
    "    # Get frequencies for amplitudes in Hz\r\n",
    "    fft_freq = np.fft.rfftfreq(len(data), 1.0/fs)\r\n",
    "    #print(fft_freq)\r\n",
    "\r\n",
    "    # Obtención de cada una de las bandas de frecuencia\r\n",
    "    eeg_band_fft = dict()\r\n",
    "    for band in eeg_bands:  \r\n",
    "        freq_ix = np.where((fft_freq >= eeg_bands[band][0]) & (fft_freq <= eeg_bands[band][1]))[0]\r\n",
    "        eeg_band_fft[band] = values[freq_ix]\r\n",
    "    return eeg_band_fft\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Ahora hay que hacer las medias sobre cada banda de frecuencia. La media entre cada una de las posiciones de los bloques. Hacer la media entre todas las posiciones 0, 1, 2... 63 de cada uno de los bloques\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "def channel_freq(eeg_band_fft):\r\n",
    "        \r\n",
    "        freq = np.zeros((5, 64))        # 5 bandas, 64 posiciones de los datos\r\n",
    "\r\n",
    "        # Se va a recorrer {Delta, Theta, Alpha, Beta, Gamma}\r\n",
    "        for band in eeg_bands:\r\n",
    "\r\n",
    "                # Para cada una de las 64 posiciones del array\r\n",
    "                for j in range (0, 64):\r\n",
    "                        \r\n",
    "                        val = []\r\n",
    "                        # Se va a recorrer cada uno de los arrays que hay en cada una de las bandas\r\n",
    "                        for i in range (0, len(eeg_band_fft[band])):\r\n",
    "                                val.append(eeg_band_fft[band][i][j])\r\n",
    "                                # Ejemplo : eeg_band_fft['Delta'][0][0*0 + 0]\r\n",
    "                        if (band == \"Delta\"):\r\n",
    "                                freq[0][j] = np.mean(val)\r\n",
    "                        elif (band == \"Theta\"):\r\n",
    "                                freq[1][j] = np.mean(val)\r\n",
    "                        elif (band == \"Alpha\"):\r\n",
    "                                freq[2][j] = np.mean(val)\r\n",
    "                        elif (band == \"Beta\"):\r\n",
    "                                freq[3][j] = np.mean(val)\r\n",
    "                        elif (band == \"Gamma\"):\r\n",
    "                                freq[4][j] = np.mean(val)\r\n",
    "        return(freq)     # En freq tenemos las frecuencias medias de cada una de las bandas\r\n",
    "\r\n",
    "# Recordamos : Estamos tratando los datos relativos al video 0, un canal concreto de un sujeto concreto"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "#df_file_videos = select_file('d\\\\s01.dat')\r\n",
    "# Obtener la división por bandas de frecuencia de cada uno de los canales del video 0\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "        \r\n",
    "#        print(freq[0])\r\n",
    " #       print(\"\\n\\n\")\r\n",
    "    # En freq tenemos la media de todas las bandas de frecuencia de cada uno de los canales del vídeo 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_alpha = pd.DataFrame(rows).reset_index(drop=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Todas las bandas una al lado de otra"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "data = all_bands_df.drop(['dominance', 'liking'], axis=1)\r\n",
    "\r\n",
    "conditions = [\r\n",
    "    ((data['valence'] <4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] <4.5) & (data['arousal'] > 4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal'] > 4.5))]\r\n",
    "choices = ['sadness', 'pleasure', 'anger', 'joy']\r\n",
    "\r\n",
    "data['valence'] = np.select(conditions, choices, default='black')\r\n",
    "data = data.rename(columns={'valence' : 'label'})\r\n",
    "data = data.drop(columns='arousal')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['label']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['label'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "param_distribs_rf = {\r\n",
    "    'max_depth': [3, 5, 10],\r\n",
    "    'min_samples_split': [2, 5, 10]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_svm = {\r\n",
    "    'kernel': ('linear', 'rbf'),\r\n",
    "    'C': [0.1, 1.0, 10.0, 100.0]\r\n",
    "}\r\n",
    "\r\n",
    "param_distribs_knn = {\r\n",
    "    'n_neighbors': [3,5,6,7,8,9,10],\r\n",
    "    'weights': ['uniform', 'distance']\r\n",
    "}\r\n",
    "\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "#print(rnd_svm_search.best_params_)\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "params_dict = rnd_rf_search.best_params_\r\n",
    "params_dict.update(rnd_svm_search.best_params_)\r\n",
    "params_dict.update(rnd_knn_search.best_params_)\r\n",
    "with open('experiment6_1_params.csv', 'a') as f: \r\n",
    "    w = csv.DictWriter(f, params_dict.keys())\r\n",
    "    w.writeheader()\r\n",
    "    w.writerow(params_dict)\r\n",
    "\r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-15-b42ddd358f6d>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['valence'] = np.select(conditions, choices, default='black')\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  56 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_sadness = precision_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_prec_pleasure = precision_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_prec_anger = precision_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_prec_joy = precision_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_prec_sadness = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_prec_pleasure = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_prec_anger = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_prec_joy = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_prec_sadness = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_prec_pleasure = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_prec_anger = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_prec_joy = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_sadness = recall_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_rec_pleasure = recall_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_rec_anger = recall_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_rec_joy = recall_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_rec_sadness = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_rec_pleasure = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_rec_anger = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_rec_joy = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_rec_sadness = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_rec_pleasure = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_rec_anger = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_rec_joy = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_sadness = f1_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_f1_pleasure = f1_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_f1_anger = f1_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_f1_joy = f1_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_f1_sadness = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_f1_pleasure = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_f1_anger = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_f1_joy = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_f1_sadness = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_f1_pleasure = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_f1_anger = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_f1_joy = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "col_names = ['Subject',\r\n",
    "             'Experiment ID',\r\n",
    "             'Feature extraction',\r\n",
    "             'Band selection',\r\n",
    "             'Channel selection',\r\n",
    "             'Classification algorithm',\r\n",
    "             'Accuracy',\r\n",
    "             'Sadness precision',\r\n",
    "             'Pleasure precision',\r\n",
    "             'Anger precision',\r\n",
    "             'Joy precision',\r\n",
    "             'Sadness recall',\r\n",
    "             'Pleasure recall',\r\n",
    "             'Anger recall',\r\n",
    "             'Joy recall',\r\n",
    "             'Sadness f1-score',\r\n",
    "             'Pleasure f1-score',\r\n",
    "             'Anger f1-score',\r\n",
    "             'Joy f1-score']\r\n",
    "\r\n",
    "data_CSV = [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_sadness, rf_prec_pleasure, rf_prec_anger, rf_prec_joy, rf_rec_sadness, rf_rec_pleasure, rf_rec_anger, rf_rec_joy, rf_f1_sadness, rf_f1_pleasure, rf_f1_anger, rf_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_sadness, svm_prec_pleasure, svm_prec_anger, svm_prec_joy, svm_rec_sadness, svm_rec_pleasure, svm_rec_anger, svm_rec_joy, svm_f1_sadness, svm_f1_pleasure, svm_f1_anger, svm_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_sadness, knn_prec_pleasure, knn_prec_anger, knn_prec_joy, knn_rec_sadness, knn_rec_pleasure, knn_rec_anger, knn_rec_joy, knn_f1_sadness, knn_f1_pleasure, knn_f1_anger, knn_f1_joy]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment6_1.csv\", mode=\"a\", header=col_names, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "f = \"data_preprocessed_python\\\\s02.dat\"\r\n",
    "subject_id = 2\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_alpha = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['dominance', 'liking'], axis=1)\r\n",
    "\r\n",
    "conditions = [\r\n",
    "    ((data['valence'] <4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] <4.5) & (data['arousal'] > 4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal'] > 4.5))]\r\n",
    "choices = ['sadness', 'pleasure', 'anger', 'joy']\r\n",
    "\r\n",
    "data['valence'] = np.select(conditions, choices, default='black')\r\n",
    "data = data.rename(columns={'valence' : 'label'})\r\n",
    "data = data.drop(columns='arousal')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['label']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['label'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "#print(rnd_svm_search.best_params_)\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "params_dict = rnd_rf_search.best_params_\r\n",
    "params_dict.update(rnd_svm_search.best_params_)\r\n",
    "params_dict.update(rnd_knn_search.best_params_)\r\n",
    "with open('experiment6_1_params.csv', 'a') as f: \r\n",
    "    w = csv.DictWriter(f, params_dict.keys())\r\n",
    "    w.writerow(params_dict)\r\n",
    "\r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_sadness = precision_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_prec_pleasure = precision_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_prec_anger = precision_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_prec_joy = precision_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_prec_sadness = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_prec_pleasure = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_prec_anger = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_prec_joy = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_prec_sadness = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_prec_pleasure = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_prec_anger = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_prec_joy = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_sadness = recall_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_rec_pleasure = recall_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_rec_anger = recall_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_rec_joy = recall_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_rec_sadness = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_rec_pleasure = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_rec_anger = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_rec_joy = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_rec_sadness = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_rec_pleasure = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_rec_anger = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_rec_joy = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_sadness = f1_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_f1_pleasure = f1_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_f1_anger = f1_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_f1_joy = f1_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_f1_sadness = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_f1_pleasure = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_f1_anger = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_f1_joy = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_f1_sadness = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_f1_pleasure = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_f1_anger = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_f1_joy = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "data_CSV = [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_sadness, rf_prec_pleasure, rf_prec_anger, rf_prec_joy, rf_rec_sadness, rf_rec_pleasure, rf_rec_anger, rf_rec_joy, rf_f1_sadness, rf_f1_pleasure, rf_f1_anger, rf_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_sadness, svm_prec_pleasure, svm_prec_anger, svm_prec_joy, svm_rec_sadness, svm_rec_pleasure, svm_rec_anger, svm_rec_joy, svm_f1_sadness, svm_f1_pleasure, svm_f1_anger, svm_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_sadness, knn_prec_pleasure, knn_prec_anger, knn_prec_joy, knn_rec_sadness, knn_rec_pleasure, knn_rec_anger, knn_rec_joy, knn_f1_sadness, knn_f1_pleasure, knn_f1_anger, knn_f1_joy]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment6_1.csv\", mode=\"a\", header=False, index=False)\r\n",
    "\r\n",
    "f = \"data_preprocessed_python\\\\s03.dat\"\r\n",
    "subject_id = 3\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_alpha = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['dominance', 'liking'], axis=1)\r\n",
    "\r\n",
    "conditions = [\r\n",
    "    ((data['valence'] <4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] <4.5) & (data['arousal'] > 4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal'] > 4.5))]\r\n",
    "choices = ['sadness', 'pleasure', 'anger', 'joy']\r\n",
    "\r\n",
    "data['valence'] = np.select(conditions, choices, default='black')\r\n",
    "data = data.rename(columns={'valence' : 'label'})\r\n",
    "data = data.drop(columns='arousal')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['label']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['label'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "#print(rnd_svm_search.best_params_)\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "params_dict = rnd_rf_search.best_params_\r\n",
    "params_dict.update(rnd_svm_search.best_params_)\r\n",
    "params_dict.update(rnd_knn_search.best_params_)\r\n",
    "with open('experiment6_1_params.csv', 'a') as f: \r\n",
    "    w = csv.DictWriter(f, params_dict.keys())\r\n",
    "    w.writerow(params_dict)\r\n",
    "    \r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_sadness = precision_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_prec_pleasure = precision_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_prec_anger = precision_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_prec_joy = precision_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_prec_sadness = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_prec_pleasure = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_prec_anger = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_prec_joy = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_prec_sadness = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_prec_pleasure = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_prec_anger = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_prec_joy = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_sadness = recall_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_rec_pleasure = recall_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_rec_anger = recall_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_rec_joy = recall_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_rec_sadness = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_rec_pleasure = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_rec_anger = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_rec_joy = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_rec_sadness = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_rec_pleasure = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_rec_anger = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_rec_joy = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_sadness = f1_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_f1_pleasure = f1_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_f1_anger = f1_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_f1_joy = f1_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_f1_sadness = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_f1_pleasure = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_f1_anger = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_f1_joy = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_f1_sadness = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_f1_pleasure = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_f1_anger = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_f1_joy = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "data_CSV = [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_sadness, rf_prec_pleasure, rf_prec_anger, rf_prec_joy, rf_rec_sadness, rf_rec_pleasure, rf_rec_anger, rf_rec_joy, rf_f1_sadness, rf_f1_pleasure, rf_f1_anger, rf_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_sadness, svm_prec_pleasure, svm_prec_anger, svm_prec_joy, svm_rec_sadness, svm_rec_pleasure, svm_rec_anger, svm_rec_joy, svm_f1_sadness, svm_f1_pleasure, svm_f1_anger, svm_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_sadness, knn_prec_pleasure, knn_prec_anger, knn_prec_joy, knn_rec_sadness, knn_rec_pleasure, knn_rec_anger, knn_rec_joy, knn_f1_sadness, knn_f1_pleasure, knn_f1_anger, knn_f1_joy]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment6_1.csv\", mode=\"a\", header=False, index=False)\r\n",
    "\r\n",
    "f = \"data_preprocessed_python\\\\s04.dat\"\r\n",
    "subject_id = 4\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_alpha = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['dominance', 'liking'], axis=1)\r\n",
    "\r\n",
    "conditions = [\r\n",
    "    ((data['valence'] <4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] <4.5) & (data['arousal'] > 4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal'] > 4.5))]\r\n",
    "choices = ['sadness', 'pleasure', 'anger', 'joy']\r\n",
    "\r\n",
    "data['valence'] = np.select(conditions, choices, default='black')\r\n",
    "data = data.rename(columns={'valence' : 'label'})\r\n",
    "data = data.drop(columns='arousal')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['label']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['label'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "#print(rnd_svm_search.best_params_)\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "params_dict = rnd_rf_search.best_params_\r\n",
    "params_dict.update(rnd_svm_search.best_params_)\r\n",
    "params_dict.update(rnd_knn_search.best_params_)\r\n",
    "with open('experiment6_1_params.csv', 'a') as f: \r\n",
    "    w = csv.DictWriter(f, params_dict.keys())\r\n",
    "    w.writerow(params_dict)\r\n",
    "\r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_sadness = precision_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_prec_pleasure = precision_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_prec_anger = precision_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_prec_joy = precision_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_prec_sadness = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_prec_pleasure = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_prec_anger = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_prec_joy = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_prec_sadness = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_prec_pleasure = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_prec_anger = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_prec_joy = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_sadness = recall_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_rec_pleasure = recall_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_rec_anger = recall_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_rec_joy = recall_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_rec_sadness = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_rec_pleasure = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_rec_anger = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_rec_joy = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_rec_sadness = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_rec_pleasure = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_rec_anger = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_rec_joy = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_sadness = f1_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_f1_pleasure = f1_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_f1_anger = f1_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_f1_joy = f1_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_f1_sadness = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_f1_pleasure = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_f1_anger = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_f1_joy = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_f1_sadness = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_f1_pleasure = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_f1_anger = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_f1_joy = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "data_CSV = [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_sadness, rf_prec_pleasure, rf_prec_anger, rf_prec_joy, rf_rec_sadness, rf_rec_pleasure, rf_rec_anger, rf_rec_joy, rf_f1_sadness, rf_f1_pleasure, rf_f1_anger, rf_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_sadness, svm_prec_pleasure, svm_prec_anger, svm_prec_joy, svm_rec_sadness, svm_rec_pleasure, svm_rec_anger, svm_rec_joy, svm_f1_sadness, svm_f1_pleasure, svm_f1_anger, svm_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_sadness, knn_prec_pleasure, knn_prec_anger, knn_prec_joy, knn_rec_sadness, knn_rec_pleasure, knn_rec_anger, knn_rec_joy, knn_f1_sadness, knn_f1_pleasure, knn_f1_anger, knn_f1_joy]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment6_1.csv\", mode=\"a\", header=False, index=False)\r\n",
    "\r\n",
    "f = \"data_preprocessed_python\\\\s05.dat\"\r\n",
    "subject_id = 5\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_alpha = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['dominance', 'liking'], axis=1)\r\n",
    "\r\n",
    "conditions = [\r\n",
    "    ((data['valence'] <4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] <4.5) & (data['arousal'] > 4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal'] > 4.5))]\r\n",
    "choices = ['sadness', 'pleasure', 'anger', 'joy']\r\n",
    "\r\n",
    "data['valence'] = np.select(conditions, choices, default='black')\r\n",
    "data = data.rename(columns={'valence' : 'label'})\r\n",
    "data = data.drop(columns='arousal')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['label']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['label'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "#print(rnd_svm_search.best_params_)\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "params_dict = rnd_rf_search.best_params_\r\n",
    "params_dict.update(rnd_svm_search.best_params_)\r\n",
    "params_dict.update(rnd_knn_search.best_params_)\r\n",
    "with open('experiment6_1_params.csv', 'a') as f: \r\n",
    "    w = csv.DictWriter(f, params_dict.keys())\r\n",
    "    w.writerow(params_dict)\r\n",
    "    \r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_sadness = precision_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_prec_pleasure = precision_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_prec_anger = precision_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_prec_joy = precision_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_prec_sadness = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_prec_pleasure = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_prec_anger = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_prec_joy = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_prec_sadness = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_prec_pleasure = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_prec_anger = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_prec_joy = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_sadness = recall_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_rec_pleasure = recall_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_rec_anger = recall_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_rec_joy = recall_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_rec_sadness = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_rec_pleasure = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_rec_anger = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_rec_joy = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_rec_sadness = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_rec_pleasure = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_rec_anger = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_rec_joy = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_sadness = f1_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_f1_pleasure = f1_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_f1_anger = f1_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_f1_joy = f1_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_f1_sadness = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_f1_pleasure = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_f1_anger = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_f1_joy = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_f1_sadness = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_f1_pleasure = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_f1_anger = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_f1_joy = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "data_CSV = [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_sadness, rf_prec_pleasure, rf_prec_anger, rf_prec_joy, rf_rec_sadness, rf_rec_pleasure, rf_rec_anger, rf_rec_joy, rf_f1_sadness, rf_f1_pleasure, rf_f1_anger, rf_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_sadness, svm_prec_pleasure, svm_prec_anger, svm_prec_joy, svm_rec_sadness, svm_rec_pleasure, svm_rec_anger, svm_rec_joy, svm_f1_sadness, svm_f1_pleasure, svm_f1_anger, svm_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_sadness, knn_prec_pleasure, knn_prec_anger, knn_prec_joy, knn_rec_sadness, knn_rec_pleasure, knn_rec_anger, knn_rec_joy, knn_f1_sadness, knn_f1_pleasure, knn_f1_anger, knn_f1_joy]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment6_1.csv\", mode=\"a\", header=False, index=False)\r\n",
    "\r\n",
    "f = \"data_preprocessed_python\\\\s06.dat\"\r\n",
    "subject_id = 6\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_alpha = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['dominance', 'liking'], axis=1)\r\n",
    "\r\n",
    "conditions = [\r\n",
    "    ((data['valence'] <4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] <4.5) & (data['arousal'] > 4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal'] > 4.5))]\r\n",
    "choices = ['sadness', 'pleasure', 'anger', 'joy']\r\n",
    "\r\n",
    "data['valence'] = np.select(conditions, choices, default='black')\r\n",
    "data = data.rename(columns={'valence' : 'label'})\r\n",
    "data = data.drop(columns='arousal')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['label']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['label'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "#print(rnd_svm_search.best_params_)\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "params_dict = rnd_rf_search.best_params_\r\n",
    "params_dict.update(rnd_svm_search.best_params_)\r\n",
    "params_dict.update(rnd_knn_search.best_params_)\r\n",
    "with open('experiment6_1_params.csv', 'a') as f: \r\n",
    "    w = csv.DictWriter(f, params_dict.keys())\r\n",
    "    w.writerow(params_dict)\r\n",
    "    \r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_sadness = precision_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_prec_pleasure = precision_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_prec_anger = precision_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_prec_joy = precision_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_prec_sadness = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_prec_pleasure = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_prec_anger = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_prec_joy = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_prec_sadness = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_prec_pleasure = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_prec_anger = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_prec_joy = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_sadness = recall_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_rec_pleasure = recall_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_rec_anger = recall_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_rec_joy = recall_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_rec_sadness = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_rec_pleasure = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_rec_anger = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_rec_joy = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_rec_sadness = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_rec_pleasure = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_rec_anger = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_rec_joy = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_sadness = f1_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_f1_pleasure = f1_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_f1_anger = f1_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_f1_joy = f1_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_f1_sadness = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_f1_pleasure = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_f1_anger = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_f1_joy = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_f1_sadness = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_f1_pleasure = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_f1_anger = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_f1_joy = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "data_CSV = [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_sadness, rf_prec_pleasure, rf_prec_anger, rf_prec_joy, rf_rec_sadness, rf_rec_pleasure, rf_rec_anger, rf_rec_joy, rf_f1_sadness, rf_f1_pleasure, rf_f1_anger, rf_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_sadness, svm_prec_pleasure, svm_prec_anger, svm_prec_joy, svm_rec_sadness, svm_rec_pleasure, svm_rec_anger, svm_rec_joy, svm_f1_sadness, svm_f1_pleasure, svm_f1_anger, svm_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_sadness, knn_prec_pleasure, knn_prec_anger, knn_prec_joy, knn_rec_sadness, knn_rec_pleasure, knn_rec_anger, knn_rec_joy, knn_f1_sadness, knn_f1_pleasure, knn_f1_anger, knn_f1_joy]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment6_1.csv\", mode=\"a\", header=False, index=False)\r\n",
    "\r\n",
    "f = \"data_preprocessed_python\\\\s07.dat\"\r\n",
    "subject_id = 7\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_alpha = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['dominance', 'liking'], axis=1)\r\n",
    "\r\n",
    "conditions = [\r\n",
    "    ((data['valence'] <4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] <4.5) & (data['arousal'] > 4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal'] > 4.5))]\r\n",
    "choices = ['sadness', 'pleasure', 'anger', 'joy']\r\n",
    "\r\n",
    "data['valence'] = np.select(conditions, choices, default='black')\r\n",
    "data = data.rename(columns={'valence' : 'label'})\r\n",
    "data = data.drop(columns='arousal')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['label']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['label'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "#print(rnd_svm_search.best_params_)\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "params_dict = rnd_rf_search.best_params_\r\n",
    "params_dict.update(rnd_svm_search.best_params_)\r\n",
    "params_dict.update(rnd_knn_search.best_params_)\r\n",
    "with open('experiment6_1_params.csv', 'a') as f: \r\n",
    "    w = csv.DictWriter(f, params_dict.keys())\r\n",
    "    w.writerow(params_dict)\r\n",
    "    \r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_sadness = precision_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_prec_pleasure = precision_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_prec_anger = precision_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_prec_joy = precision_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_prec_sadness = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_prec_pleasure = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_prec_anger = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_prec_joy = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_prec_sadness = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_prec_pleasure = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_prec_anger = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_prec_joy = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_sadness = recall_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_rec_pleasure = recall_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_rec_anger = recall_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_rec_joy = recall_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_rec_sadness = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_rec_pleasure = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_rec_anger = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_rec_joy = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_rec_sadness = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_rec_pleasure = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_rec_anger = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_rec_joy = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_sadness = f1_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_f1_pleasure = f1_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_f1_anger = f1_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_f1_joy = f1_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_f1_sadness = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_f1_pleasure = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_f1_anger = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_f1_joy = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_f1_sadness = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_f1_pleasure = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_f1_anger = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_f1_joy = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "data_CSV = [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_sadness, rf_prec_pleasure, rf_prec_anger, rf_prec_joy, rf_rec_sadness, rf_rec_pleasure, rf_rec_anger, rf_rec_joy, rf_f1_sadness, rf_f1_pleasure, rf_f1_anger, rf_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_sadness, svm_prec_pleasure, svm_prec_anger, svm_prec_joy, svm_rec_sadness, svm_rec_pleasure, svm_rec_anger, svm_rec_joy, svm_f1_sadness, svm_f1_pleasure, svm_f1_anger, svm_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_sadness, knn_prec_pleasure, knn_prec_anger, knn_prec_joy, knn_rec_sadness, knn_rec_pleasure, knn_rec_anger, knn_rec_joy, knn_f1_sadness, knn_f1_pleasure, knn_f1_anger, knn_f1_joy]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment6_1.csv\", mode=\"a\", header=False, index=False)\r\n",
    "\r\n",
    "f = \"data_preprocessed_python\\\\s08.dat\"\r\n",
    "subject_id = 8\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_alpha = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['dominance', 'liking'], axis=1)\r\n",
    "\r\n",
    "conditions = [\r\n",
    "    ((data['valence'] <4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] <4.5) & (data['arousal'] > 4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal'] > 4.5))]\r\n",
    "choices = ['sadness', 'pleasure', 'anger', 'joy']\r\n",
    "\r\n",
    "data['valence'] = np.select(conditions, choices, default='black')\r\n",
    "data = data.rename(columns={'valence' : 'label'})\r\n",
    "data = data.drop(columns='arousal')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['label']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['label'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "#print(rnd_svm_search.best_params_)\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "params_dict = rnd_rf_search.best_params_\r\n",
    "params_dict.update(rnd_svm_search.best_params_)\r\n",
    "params_dict.update(rnd_knn_search.best_params_)\r\n",
    "with open('experiment6_1_params.csv', 'a') as f: \r\n",
    "    w = csv.DictWriter(f, params_dict.keys())\r\n",
    "    w.writerow(params_dict)\r\n",
    "    \r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_sadness = precision_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_prec_pleasure = precision_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_prec_anger = precision_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_prec_joy = precision_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_prec_sadness = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_prec_pleasure = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_prec_anger = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_prec_joy = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_prec_sadness = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_prec_pleasure = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_prec_anger = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_prec_joy = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_sadness = recall_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_rec_pleasure = recall_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_rec_anger = recall_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_rec_joy = recall_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_rec_sadness = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_rec_pleasure = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_rec_anger = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_rec_joy = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_rec_sadness = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_rec_pleasure = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_rec_anger = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_rec_joy = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_sadness = f1_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_f1_pleasure = f1_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_f1_anger = f1_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_f1_joy = f1_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_f1_sadness = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_f1_pleasure = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_f1_anger = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_f1_joy = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_f1_sadness = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_f1_pleasure = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_f1_anger = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_f1_joy = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "data_CSV = [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_sadness, rf_prec_pleasure, rf_prec_anger, rf_prec_joy, rf_rec_sadness, rf_rec_pleasure, rf_rec_anger, rf_rec_joy, rf_f1_sadness, rf_f1_pleasure, rf_f1_anger, rf_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_sadness, svm_prec_pleasure, svm_prec_anger, svm_prec_joy, svm_rec_sadness, svm_rec_pleasure, svm_rec_anger, svm_rec_joy, svm_f1_sadness, svm_f1_pleasure, svm_f1_anger, svm_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_sadness, knn_prec_pleasure, knn_prec_anger, knn_prec_joy, knn_rec_sadness, knn_rec_pleasure, knn_rec_anger, knn_rec_joy, knn_f1_sadness, knn_f1_pleasure, knn_f1_anger, knn_f1_joy]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment6_1.csv\", mode=\"a\", header=False, index=False)\r\n",
    "\r\n",
    "f = \"data_preprocessed_python\\\\s09.dat\"\r\n",
    "subject_id = 9\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_alpha = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['dominance', 'liking'], axis=1)\r\n",
    "\r\n",
    "conditions = [\r\n",
    "    ((data['valence'] <4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] <4.5) & (data['arousal'] > 4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal'] > 4.5))]\r\n",
    "choices = ['sadness', 'pleasure', 'anger', 'joy']\r\n",
    "\r\n",
    "data['valence'] = np.select(conditions, choices, default='black')\r\n",
    "data = data.rename(columns={'valence' : 'label'})\r\n",
    "data = data.drop(columns='arousal')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['label']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['label'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "#print(rnd_svm_search.best_params_)\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "params_dict = rnd_rf_search.best_params_\r\n",
    "params_dict.update(rnd_svm_search.best_params_)\r\n",
    "params_dict.update(rnd_knn_search.best_params_)\r\n",
    "with open('experiment6_1_params.csv', 'a') as f: \r\n",
    "    w = csv.DictWriter(f, params_dict.keys())\r\n",
    "    w.writerow(params_dict)\r\n",
    "    \r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_sadness = precision_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_prec_pleasure = precision_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_prec_anger = precision_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_prec_joy = precision_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_prec_sadness = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_prec_pleasure = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_prec_anger = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_prec_joy = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_prec_sadness = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_prec_pleasure = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_prec_anger = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_prec_joy = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_sadness = recall_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_rec_pleasure = recall_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_rec_anger = recall_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_rec_joy = recall_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_rec_sadness = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_rec_pleasure = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_rec_anger = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_rec_joy = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_rec_sadness = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_rec_pleasure = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_rec_anger = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_rec_joy = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_sadness = f1_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_f1_pleasure = f1_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_f1_anger = f1_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_f1_joy = f1_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_f1_sadness = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_f1_pleasure = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_f1_anger = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_f1_joy = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_f1_sadness = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_f1_pleasure = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_f1_anger = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_f1_joy = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "data_CSV = [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_sadness, rf_prec_pleasure, rf_prec_anger, rf_prec_joy, rf_rec_sadness, rf_rec_pleasure, rf_rec_anger, rf_rec_joy, rf_f1_sadness, rf_f1_pleasure, rf_f1_anger, rf_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_sadness, svm_prec_pleasure, svm_prec_anger, svm_prec_joy, svm_rec_sadness, svm_rec_pleasure, svm_rec_anger, svm_rec_joy, svm_f1_sadness, svm_f1_pleasure, svm_f1_anger, svm_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_sadness, knn_prec_pleasure, knn_prec_anger, knn_prec_joy, knn_rec_sadness, knn_rec_pleasure, knn_rec_anger, knn_rec_joy, knn_f1_sadness, knn_f1_pleasure, knn_f1_anger, knn_f1_joy]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment6_1.csv\", mode=\"a\", header=False, index=False)\r\n",
    "\r\n",
    "f = \"data_preprocessed_python\\\\s10.dat\"\r\n",
    "subject_id = 10\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_alpha = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['dominance', 'liking'], axis=1)\r\n",
    "\r\n",
    "conditions = [\r\n",
    "    ((data['valence'] <4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] <4.5) & (data['arousal'] > 4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal'] > 4.5))]\r\n",
    "choices = ['sadness', 'pleasure', 'anger', 'joy']\r\n",
    "\r\n",
    "data['valence'] = np.select(conditions, choices, default='black')\r\n",
    "data = data.rename(columns={'valence' : 'label'})\r\n",
    "data = data.drop(columns='arousal')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['label']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['label'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "#print(rnd_svm_search.best_params_)\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "params_dict = rnd_rf_search.best_params_\r\n",
    "params_dict.update(rnd_svm_search.best_params_)\r\n",
    "params_dict.update(rnd_knn_search.best_params_)\r\n",
    "with open('experiment6_1_params.csv', 'a') as f: \r\n",
    "    w = csv.DictWriter(f, params_dict.keys())\r\n",
    "    w.writerow(params_dict)\r\n",
    "    \r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_sadness = precision_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_prec_pleasure = precision_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_prec_anger = precision_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_prec_joy = precision_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_prec_sadness = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_prec_pleasure = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_prec_anger = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_prec_joy = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_prec_sadness = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_prec_pleasure = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_prec_anger = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_prec_joy = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_sadness = recall_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_rec_pleasure = recall_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_rec_anger = recall_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_rec_joy = recall_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_rec_sadness = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_rec_pleasure = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_rec_anger = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_rec_joy = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_rec_sadness = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_rec_pleasure = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_rec_anger = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_rec_joy = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_sadness = f1_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_f1_pleasure = f1_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_f1_anger = f1_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_f1_joy = f1_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_f1_sadness = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_f1_pleasure = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_f1_anger = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_f1_joy = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_f1_sadness = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_f1_pleasure = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_f1_anger = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_f1_joy = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "data_CSV = [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_sadness, rf_prec_pleasure, rf_prec_anger, rf_prec_joy, rf_rec_sadness, rf_rec_pleasure, rf_rec_anger, rf_rec_joy, rf_f1_sadness, rf_f1_pleasure, rf_f1_anger, rf_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_sadness, svm_prec_pleasure, svm_prec_anger, svm_prec_joy, svm_rec_sadness, svm_rec_pleasure, svm_rec_anger, svm_rec_joy, svm_f1_sadness, svm_f1_pleasure, svm_f1_anger, svm_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_sadness, knn_prec_pleasure, knn_prec_anger, knn_prec_joy, knn_rec_sadness, knn_rec_pleasure, knn_rec_anger, knn_rec_joy, knn_f1_sadness, knn_f1_pleasure, knn_f1_anger, knn_f1_joy]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment6_1.csv\", mode=\"a\", header=False, index=False)\r\n",
    "\r\n",
    "f = \"data_preprocessed_python\\\\s11.dat\"\r\n",
    "subject_id = 11\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_alpha = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['dominance', 'liking'], axis=1)\r\n",
    "\r\n",
    "conditions = [\r\n",
    "    ((data['valence'] <4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] <4.5) & (data['arousal'] > 4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal'] > 4.5))]\r\n",
    "choices = ['sadness', 'pleasure', 'anger', 'joy']\r\n",
    "\r\n",
    "data['valence'] = np.select(conditions, choices, default='black')\r\n",
    "data = data.rename(columns={'valence' : 'label'})\r\n",
    "data = data.drop(columns='arousal')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['label']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['label'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "#print(rnd_svm_search.best_params_)\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "params_dict = rnd_rf_search.best_params_\r\n",
    "params_dict.update(rnd_svm_search.best_params_)\r\n",
    "params_dict.update(rnd_knn_search.best_params_)\r\n",
    "with open('experiment6_1_params.csv', 'a') as f: \r\n",
    "    w = csv.DictWriter(f, params_dict.keys())\r\n",
    "    w.writerow(params_dict)\r\n",
    "    \r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_sadness = precision_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_prec_pleasure = precision_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_prec_anger = precision_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_prec_joy = precision_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_prec_sadness = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_prec_pleasure = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_prec_anger = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_prec_joy = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_prec_sadness = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_prec_pleasure = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_prec_anger = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_prec_joy = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_sadness = recall_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_rec_pleasure = recall_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_rec_anger = recall_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_rec_joy = recall_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_rec_sadness = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_rec_pleasure = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_rec_anger = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_rec_joy = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_rec_sadness = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_rec_pleasure = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_rec_anger = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_rec_joy = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_sadness = f1_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_f1_pleasure = f1_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_f1_anger = f1_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_f1_joy = f1_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_f1_sadness = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_f1_pleasure = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_f1_anger = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_f1_joy = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_f1_sadness = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_f1_pleasure = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_f1_anger = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_f1_joy = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "data_CSV = [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_sadness, rf_prec_pleasure, rf_prec_anger, rf_prec_joy, rf_rec_sadness, rf_rec_pleasure, rf_rec_anger, rf_rec_joy, rf_f1_sadness, rf_f1_pleasure, rf_f1_anger, rf_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_sadness, svm_prec_pleasure, svm_prec_anger, svm_prec_joy, svm_rec_sadness, svm_rec_pleasure, svm_rec_anger, svm_rec_joy, svm_f1_sadness, svm_f1_pleasure, svm_f1_anger, svm_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_sadness, knn_prec_pleasure, knn_prec_anger, knn_prec_joy, knn_rec_sadness, knn_rec_pleasure, knn_rec_anger, knn_rec_joy, knn_f1_sadness, knn_f1_pleasure, knn_f1_anger, knn_f1_joy]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment6_1.csv\", mode=\"a\", header=False, index=False)\r\n",
    "\r\n",
    "f = \"data_preprocessed_python\\\\s12.dat\"\r\n",
    "subject_id = 12\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_alpha = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['dominance', 'liking'], axis=1)\r\n",
    "\r\n",
    "conditions = [\r\n",
    "    ((data['valence'] <4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] <4.5) & (data['arousal'] > 4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal'] > 4.5))]\r\n",
    "choices = ['sadness', 'pleasure', 'anger', 'joy']\r\n",
    "\r\n",
    "data['valence'] = np.select(conditions, choices, default='black')\r\n",
    "data = data.rename(columns={'valence' : 'label'})\r\n",
    "data = data.drop(columns='arousal')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['label']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['label'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "#print(rnd_svm_search.best_params_)\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "params_dict = rnd_rf_search.best_params_\r\n",
    "params_dict.update(rnd_svm_search.best_params_)\r\n",
    "params_dict.update(rnd_knn_search.best_params_)\r\n",
    "with open('experiment6_1_params.csv', 'a') as f: \r\n",
    "    w = csv.DictWriter(f, params_dict.keys())\r\n",
    "    w.writerow(params_dict)\r\n",
    "    \r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_sadness = precision_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_prec_pleasure = precision_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_prec_anger = precision_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_prec_joy = precision_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_prec_sadness = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_prec_pleasure = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_prec_anger = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_prec_joy = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_prec_sadness = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_prec_pleasure = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_prec_anger = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_prec_joy = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_sadness = recall_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_rec_pleasure = recall_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_rec_anger = recall_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_rec_joy = recall_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_rec_sadness = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_rec_pleasure = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_rec_anger = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_rec_joy = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_rec_sadness = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_rec_pleasure = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_rec_anger = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_rec_joy = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_sadness = f1_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_f1_pleasure = f1_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_f1_anger = f1_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_f1_joy = f1_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_f1_sadness = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_f1_pleasure = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_f1_anger = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_f1_joy = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_f1_sadness = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_f1_pleasure = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_f1_anger = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_f1_joy = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "data_CSV = [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_sadness, rf_prec_pleasure, rf_prec_anger, rf_prec_joy, rf_rec_sadness, rf_rec_pleasure, rf_rec_anger, rf_rec_joy, rf_f1_sadness, rf_f1_pleasure, rf_f1_anger, rf_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_sadness, svm_prec_pleasure, svm_prec_anger, svm_prec_joy, svm_rec_sadness, svm_rec_pleasure, svm_rec_anger, svm_rec_joy, svm_f1_sadness, svm_f1_pleasure, svm_f1_anger, svm_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_sadness, knn_prec_pleasure, knn_prec_anger, knn_prec_joy, knn_rec_sadness, knn_rec_pleasure, knn_rec_anger, knn_rec_joy, knn_f1_sadness, knn_f1_pleasure, knn_f1_anger, knn_f1_joy]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment6_1.csv\", mode=\"a\", header=False, index=False)\r\n",
    "\r\n",
    "f = \"data_preprocessed_python\\\\s13.dat\"\r\n",
    "subject_id = 13\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_alpha = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['dominance', 'liking'], axis=1)\r\n",
    "\r\n",
    "conditions = [\r\n",
    "    ((data['valence'] <4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] <4.5) & (data['arousal'] > 4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal'] > 4.5))]\r\n",
    "choices = ['sadness', 'pleasure', 'anger', 'joy']\r\n",
    "\r\n",
    "data['valence'] = np.select(conditions, choices, default='black')\r\n",
    "data = data.rename(columns={'valence' : 'label'})\r\n",
    "data = data.drop(columns='arousal')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['label']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['label'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "#print(rnd_svm_search.best_params_)\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "params_dict = rnd_rf_search.best_params_\r\n",
    "params_dict.update(rnd_svm_search.best_params_)\r\n",
    "params_dict.update(rnd_knn_search.best_params_)\r\n",
    "with open('experiment6_1_params.csv', 'a') as f: \r\n",
    "    w = csv.DictWriter(f, params_dict.keys())\r\n",
    "    w.writerow(params_dict)\r\n",
    "    \r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_sadness = precision_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_prec_pleasure = precision_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_prec_anger = precision_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_prec_joy = precision_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_prec_sadness = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_prec_pleasure = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_prec_anger = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_prec_joy = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_prec_sadness = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_prec_pleasure = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_prec_anger = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_prec_joy = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_sadness = recall_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_rec_pleasure = recall_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_rec_anger = recall_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_rec_joy = recall_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_rec_sadness = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_rec_pleasure = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_rec_anger = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_rec_joy = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_rec_sadness = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_rec_pleasure = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_rec_anger = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_rec_joy = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_sadness = f1_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_f1_pleasure = f1_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_f1_anger = f1_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_f1_joy = f1_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_f1_sadness = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_f1_pleasure = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_f1_anger = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_f1_joy = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_f1_sadness = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_f1_pleasure = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_f1_anger = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_f1_joy = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "data_CSV = [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_sadness, rf_prec_pleasure, rf_prec_anger, rf_prec_joy, rf_rec_sadness, rf_rec_pleasure, rf_rec_anger, rf_rec_joy, rf_f1_sadness, rf_f1_pleasure, rf_f1_anger, rf_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_sadness, svm_prec_pleasure, svm_prec_anger, svm_prec_joy, svm_rec_sadness, svm_rec_pleasure, svm_rec_anger, svm_rec_joy, svm_f1_sadness, svm_f1_pleasure, svm_f1_anger, svm_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_sadness, knn_prec_pleasure, knn_prec_anger, knn_prec_joy, knn_rec_sadness, knn_rec_pleasure, knn_rec_anger, knn_rec_joy, knn_f1_sadness, knn_f1_pleasure, knn_f1_anger, knn_f1_joy]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment6_1.csv\", mode=\"a\", header=False, index=False)\r\n",
    "\r\n",
    "f = \"data_preprocessed_python\\\\s14.dat\"\r\n",
    "subject_id = 14\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_alpha = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['dominance', 'liking'], axis=1)\r\n",
    "\r\n",
    "conditions = [\r\n",
    "    ((data['valence'] <4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] <4.5) & (data['arousal'] > 4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal'] > 4.5))]\r\n",
    "choices = ['sadness', 'pleasure', 'anger', 'joy']\r\n",
    "\r\n",
    "data['valence'] = np.select(conditions, choices, default='black')\r\n",
    "data = data.rename(columns={'valence' : 'label'})\r\n",
    "data = data.drop(columns='arousal')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['label']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['label'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "#print(rnd_svm_search.best_params_)\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "params_dict = rnd_rf_search.best_params_\r\n",
    "params_dict.update(rnd_svm_search.best_params_)\r\n",
    "params_dict.update(rnd_knn_search.best_params_)\r\n",
    "with open('experiment6_1_params.csv', 'a') as f: \r\n",
    "    w = csv.DictWriter(f, params_dict.keys())\r\n",
    "    w.writerow(params_dict)\r\n",
    "    \r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_sadness = precision_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_prec_pleasure = precision_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_prec_anger = precision_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_prec_joy = precision_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_prec_sadness = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_prec_pleasure = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_prec_anger = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_prec_joy = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_prec_sadness = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_prec_pleasure = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_prec_anger = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_prec_joy = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_sadness = recall_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_rec_pleasure = recall_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_rec_anger = recall_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_rec_joy = recall_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_rec_sadness = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_rec_pleasure = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_rec_anger = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_rec_joy = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_rec_sadness = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_rec_pleasure = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_rec_anger = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_rec_joy = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_sadness = f1_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_f1_pleasure = f1_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_f1_anger = f1_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_f1_joy = f1_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_f1_sadness = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_f1_pleasure = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_f1_anger = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_f1_joy = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_f1_sadness = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_f1_pleasure = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_f1_anger = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_f1_joy = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "data_CSV = [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_sadness, rf_prec_pleasure, rf_prec_anger, rf_prec_joy, rf_rec_sadness, rf_rec_pleasure, rf_rec_anger, rf_rec_joy, rf_f1_sadness, rf_f1_pleasure, rf_f1_anger, rf_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_sadness, svm_prec_pleasure, svm_prec_anger, svm_prec_joy, svm_rec_sadness, svm_rec_pleasure, svm_rec_anger, svm_rec_joy, svm_f1_sadness, svm_f1_pleasure, svm_f1_anger, svm_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_sadness, knn_prec_pleasure, knn_prec_anger, knn_prec_joy, knn_rec_sadness, knn_rec_pleasure, knn_rec_anger, knn_rec_joy, knn_f1_sadness, knn_f1_pleasure, knn_f1_anger, knn_f1_joy]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment6_1.csv\", mode=\"a\", header=False, index=False)\r\n",
    "\r\n",
    "f = \"data_preprocessed_python\\\\s15.dat\"\r\n",
    "subject_id = 15\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_alpha = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['dominance', 'liking'], axis=1)\r\n",
    "\r\n",
    "conditions = [\r\n",
    "    ((data['valence'] <4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] <4.5) & (data['arousal'] > 4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal'] > 4.5))]\r\n",
    "choices = ['sadness', 'pleasure', 'anger', 'joy']\r\n",
    "\r\n",
    "data['valence'] = np.select(conditions, choices, default='black')\r\n",
    "data = data.rename(columns={'valence' : 'label'})\r\n",
    "data = data.drop(columns='arousal')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['label']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['label'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "#print(rnd_svm_search.best_params_)\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "params_dict = rnd_rf_search.best_params_\r\n",
    "params_dict.update(rnd_svm_search.best_params_)\r\n",
    "params_dict.update(rnd_knn_search.best_params_)\r\n",
    "with open('experiment6_1_params.csv', 'a') as f: \r\n",
    "    w = csv.DictWriter(f, params_dict.keys())\r\n",
    "    w.writerow(params_dict)\r\n",
    "    \r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_sadness = precision_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_prec_pleasure = precision_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_prec_anger = precision_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_prec_joy = precision_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_prec_sadness = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_prec_pleasure = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_prec_anger = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_prec_joy = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_prec_sadness = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_prec_pleasure = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_prec_anger = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_prec_joy = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_sadness = recall_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_rec_pleasure = recall_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_rec_anger = recall_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_rec_joy = recall_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_rec_sadness = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_rec_pleasure = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_rec_anger = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_rec_joy = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_rec_sadness = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_rec_pleasure = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_rec_anger = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_rec_joy = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_sadness = f1_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_f1_pleasure = f1_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_f1_anger = f1_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_f1_joy = f1_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_f1_sadness = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_f1_pleasure = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_f1_anger = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_f1_joy = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_f1_sadness = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_f1_pleasure = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_f1_anger = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_f1_joy = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "data_CSV = [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_sadness, rf_prec_pleasure, rf_prec_anger, rf_prec_joy, rf_rec_sadness, rf_rec_pleasure, rf_rec_anger, rf_rec_joy, rf_f1_sadness, rf_f1_pleasure, rf_f1_anger, rf_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_sadness, svm_prec_pleasure, svm_prec_anger, svm_prec_joy, svm_rec_sadness, svm_rec_pleasure, svm_rec_anger, svm_rec_joy, svm_f1_sadness, svm_f1_pleasure, svm_f1_anger, svm_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_sadness, knn_prec_pleasure, knn_prec_anger, knn_prec_joy, knn_rec_sadness, knn_rec_pleasure, knn_rec_anger, knn_rec_joy, knn_f1_sadness, knn_f1_pleasure, knn_f1_anger, knn_f1_joy]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment6_1.csv\", mode=\"a\", header=False, index=False)\r\n",
    "\r\n",
    "f = \"data_preprocessed_python\\\\s16.dat\"\r\n",
    "subject_id = 16\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_alpha = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['dominance', 'liking'], axis=1)\r\n",
    "\r\n",
    "conditions = [\r\n",
    "    ((data['valence'] <4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] <4.5) & (data['arousal'] > 4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal'] > 4.5))]\r\n",
    "choices = ['sadness', 'pleasure', 'anger', 'joy']\r\n",
    "\r\n",
    "data['valence'] = np.select(conditions, choices, default='black')\r\n",
    "data = data.rename(columns={'valence' : 'label'})\r\n",
    "data = data.drop(columns='arousal')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['label']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['label'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "#print(rnd_svm_search.best_params_)\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "params_dict = rnd_rf_search.best_params_\r\n",
    "params_dict.update(rnd_svm_search.best_params_)\r\n",
    "params_dict.update(rnd_knn_search.best_params_)\r\n",
    "with open('experiment6_1_params.csv', 'a') as f: \r\n",
    "    w = csv.DictWriter(f, params_dict.keys())\r\n",
    "    w.writerow(params_dict)\r\n",
    "    \r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_sadness = precision_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_prec_pleasure = precision_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_prec_anger = precision_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_prec_joy = precision_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_prec_sadness = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_prec_pleasure = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_prec_anger = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_prec_joy = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_prec_sadness = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_prec_pleasure = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_prec_anger = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_prec_joy = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_sadness = recall_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_rec_pleasure = recall_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_rec_anger = recall_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_rec_joy = recall_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_rec_sadness = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_rec_pleasure = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_rec_anger = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_rec_joy = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_rec_sadness = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_rec_pleasure = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_rec_anger = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_rec_joy = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_sadness = f1_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_f1_pleasure = f1_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_f1_anger = f1_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_f1_joy = f1_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_f1_sadness = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_f1_pleasure = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_f1_anger = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_f1_joy = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_f1_sadness = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_f1_pleasure = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_f1_anger = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_f1_joy = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "data_CSV = [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_sadness, rf_prec_pleasure, rf_prec_anger, rf_prec_joy, rf_rec_sadness, rf_rec_pleasure, rf_rec_anger, rf_rec_joy, rf_f1_sadness, rf_f1_pleasure, rf_f1_anger, rf_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_sadness, svm_prec_pleasure, svm_prec_anger, svm_prec_joy, svm_rec_sadness, svm_rec_pleasure, svm_rec_anger, svm_rec_joy, svm_f1_sadness, svm_f1_pleasure, svm_f1_anger, svm_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_sadness, knn_prec_pleasure, knn_prec_anger, knn_prec_joy, knn_rec_sadness, knn_rec_pleasure, knn_rec_anger, knn_rec_joy, knn_f1_sadness, knn_f1_pleasure, knn_f1_anger, knn_f1_joy]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment6_1.csv\", mode=\"a\", header=False, index=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-18-e7d0bc791510>:120: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['valence'] = np.select(conditions, choices, default='black')\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "<ipython-input-18-e7d0bc791510>:358: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['valence'] = np.select(conditions, choices, default='black')\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  74 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "<ipython-input-18-e7d0bc791510>:596: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['valence'] = np.select(conditions, choices, default='black')\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  77 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  76 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done  91 out of 100 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  77 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  77 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done  91 out of 100 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "f = \"data_preprocessed_python\\\\s17.dat\"\r\n",
    "subject_id = 17\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_alpha = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['dominance', 'liking'], axis=1)\r\n",
    "\r\n",
    "conditions = [\r\n",
    "    ((data['valence'] <4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] <4.5) & (data['arousal'] > 4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal'] > 4.5))]\r\n",
    "choices = ['sadness', 'pleasure', 'anger', 'joy']\r\n",
    "\r\n",
    "data['valence'] = np.select(conditions, choices, default='black')\r\n",
    "data = data.rename(columns={'valence' : 'label'})\r\n",
    "data = data.drop(columns='arousal')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['label']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['label'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "#print(rnd_svm_search.best_params_)\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "params_dict = rnd_rf_search.best_params_\r\n",
    "params_dict.update(rnd_svm_search.best_params_)\r\n",
    "params_dict.update(rnd_knn_search.best_params_)\r\n",
    "with open('experiment6_1_params.csv', 'a') as f: \r\n",
    "    w = csv.DictWriter(f, params_dict.keys())\r\n",
    "    w.writerow(params_dict)\r\n",
    "    \r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_sadness = precision_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_prec_pleasure = precision_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_prec_anger = precision_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_prec_joy = precision_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_prec_sadness = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_prec_pleasure = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_prec_anger = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_prec_joy = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_prec_sadness = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_prec_pleasure = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_prec_anger = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_prec_joy = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_sadness = recall_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_rec_pleasure = recall_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_rec_anger = recall_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_rec_joy = recall_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_rec_sadness = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_rec_pleasure = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_rec_anger = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_rec_joy = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_rec_sadness = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_rec_pleasure = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_rec_anger = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_rec_joy = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_sadness = f1_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_f1_pleasure = f1_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_f1_anger = f1_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_f1_joy = f1_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_f1_sadness = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_f1_pleasure = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_f1_anger = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_f1_joy = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_f1_sadness = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_f1_pleasure = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_f1_anger = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_f1_joy = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "data_CSV = [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_sadness, rf_prec_pleasure, rf_prec_anger, rf_prec_joy, rf_rec_sadness, rf_rec_pleasure, rf_rec_anger, rf_rec_joy, rf_f1_sadness, rf_f1_pleasure, rf_f1_anger, rf_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_sadness, svm_prec_pleasure, svm_prec_anger, svm_prec_joy, svm_rec_sadness, svm_rec_pleasure, svm_rec_anger, svm_rec_joy, svm_f1_sadness, svm_f1_pleasure, svm_f1_anger, svm_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_sadness, knn_prec_pleasure, knn_prec_anger, knn_prec_joy, knn_rec_sadness, knn_rec_pleasure, knn_rec_anger, knn_rec_joy, knn_f1_sadness, knn_f1_pleasure, knn_f1_anger, knn_f1_joy]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment6_1.csv\", mode=\"a\", header=False, index=False)\r\n",
    "\r\n",
    "f = \"data_preprocessed_python\\\\s18.dat\"\r\n",
    "subject_id = 18\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_alpha = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['dominance', 'liking'], axis=1)\r\n",
    "\r\n",
    "conditions = [\r\n",
    "    ((data['valence'] <4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] <4.5) & (data['arousal'] > 4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal'] > 4.5))]\r\n",
    "choices = ['sadness', 'pleasure', 'anger', 'joy']\r\n",
    "\r\n",
    "data['valence'] = np.select(conditions, choices, default='black')\r\n",
    "data = data.rename(columns={'valence' : 'label'})\r\n",
    "data = data.drop(columns='arousal')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['label']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['label'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "#print(rnd_svm_search.best_params_)\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "params_dict = rnd_rf_search.best_params_\r\n",
    "params_dict.update(rnd_svm_search.best_params_)\r\n",
    "params_dict.update(rnd_knn_search.best_params_)\r\n",
    "with open('experiment6_1_params.csv', 'a') as f: \r\n",
    "    w = csv.DictWriter(f, params_dict.keys())\r\n",
    "    w.writerow(params_dict)\r\n",
    "    \r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_sadness = precision_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_prec_pleasure = precision_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_prec_anger = precision_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_prec_joy = precision_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_prec_sadness = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_prec_pleasure = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_prec_anger = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_prec_joy = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_prec_sadness = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_prec_pleasure = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_prec_anger = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_prec_joy = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_sadness = recall_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_rec_pleasure = recall_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_rec_anger = recall_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_rec_joy = recall_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_rec_sadness = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_rec_pleasure = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_rec_anger = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_rec_joy = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_rec_sadness = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_rec_pleasure = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_rec_anger = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_rec_joy = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_sadness = f1_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_f1_pleasure = f1_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_f1_anger = f1_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_f1_joy = f1_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_f1_sadness = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_f1_pleasure = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_f1_anger = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_f1_joy = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_f1_sadness = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_f1_pleasure = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_f1_anger = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_f1_joy = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "data_CSV = [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_sadness, rf_prec_pleasure, rf_prec_anger, rf_prec_joy, rf_rec_sadness, rf_rec_pleasure, rf_rec_anger, rf_rec_joy, rf_f1_sadness, rf_f1_pleasure, rf_f1_anger, rf_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_sadness, svm_prec_pleasure, svm_prec_anger, svm_prec_joy, svm_rec_sadness, svm_rec_pleasure, svm_rec_anger, svm_rec_joy, svm_f1_sadness, svm_f1_pleasure, svm_f1_anger, svm_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_sadness, knn_prec_pleasure, knn_prec_anger, knn_prec_joy, knn_rec_sadness, knn_rec_pleasure, knn_rec_anger, knn_rec_joy, knn_f1_sadness, knn_f1_pleasure, knn_f1_anger, knn_f1_joy]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment6_1.csv\", mode=\"a\", header=False, index=False)\r\n",
    "\r\n",
    "f = \"data_preprocessed_python\\\\s19.dat\"\r\n",
    "subject_id = 19\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_alpha = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['dominance', 'liking'], axis=1)\r\n",
    "\r\n",
    "conditions = [\r\n",
    "    ((data['valence'] <4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] <4.5) & (data['arousal'] > 4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal'] > 4.5))]\r\n",
    "choices = ['sadness', 'pleasure', 'anger', 'joy']\r\n",
    "\r\n",
    "data['valence'] = np.select(conditions, choices, default='black')\r\n",
    "data = data.rename(columns={'valence' : 'label'})\r\n",
    "data = data.drop(columns='arousal')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['label']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['label'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "#print(rnd_svm_search.best_params_)\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "params_dict = rnd_rf_search.best_params_\r\n",
    "params_dict.update(rnd_svm_search.best_params_)\r\n",
    "params_dict.update(rnd_knn_search.best_params_)\r\n",
    "with open('experiment6_1_params.csv', 'a') as f: \r\n",
    "    w = csv.DictWriter(f, params_dict.keys())\r\n",
    "    w.writerow(params_dict)\r\n",
    "    \r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_sadness = precision_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_prec_pleasure = precision_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_prec_anger = precision_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_prec_joy = precision_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_prec_sadness = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_prec_pleasure = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_prec_anger = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_prec_joy = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_prec_sadness = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_prec_pleasure = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_prec_anger = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_prec_joy = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_sadness = recall_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_rec_pleasure = recall_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_rec_anger = recall_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_rec_joy = recall_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_rec_sadness = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_rec_pleasure = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_rec_anger = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_rec_joy = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_rec_sadness = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_rec_pleasure = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_rec_anger = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_rec_joy = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_sadness = f1_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_f1_pleasure = f1_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_f1_anger = f1_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_f1_joy = f1_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_f1_sadness = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_f1_pleasure = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_f1_anger = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_f1_joy = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_f1_sadness = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_f1_pleasure = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_f1_anger = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_f1_joy = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "data_CSV = [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_sadness, rf_prec_pleasure, rf_prec_anger, rf_prec_joy, rf_rec_sadness, rf_rec_pleasure, rf_rec_anger, rf_rec_joy, rf_f1_sadness, rf_f1_pleasure, rf_f1_anger, rf_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_sadness, svm_prec_pleasure, svm_prec_anger, svm_prec_joy, svm_rec_sadness, svm_rec_pleasure, svm_rec_anger, svm_rec_joy, svm_f1_sadness, svm_f1_pleasure, svm_f1_anger, svm_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_sadness, knn_prec_pleasure, knn_prec_anger, knn_prec_joy, knn_rec_sadness, knn_rec_pleasure, knn_rec_anger, knn_rec_joy, knn_f1_sadness, knn_f1_pleasure, knn_f1_anger, knn_f1_joy]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment6_1.csv\", mode=\"a\", header=False, index=False)\r\n",
    "\r\n",
    "f = \"data_preprocessed_python\\\\s20.dat\"\r\n",
    "subject_id = 20\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_alpha = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['dominance', 'liking'], axis=1)\r\n",
    "\r\n",
    "conditions = [\r\n",
    "    ((data['valence'] <4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] <4.5) & (data['arousal'] > 4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal'] > 4.5))]\r\n",
    "choices = ['sadness', 'pleasure', 'anger', 'joy']\r\n",
    "\r\n",
    "data['valence'] = np.select(conditions, choices, default='black')\r\n",
    "data = data.rename(columns={'valence' : 'label'})\r\n",
    "data = data.drop(columns='arousal')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['label']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['label'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "#print(rnd_svm_search.best_params_)\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "params_dict = rnd_rf_search.best_params_\r\n",
    "params_dict.update(rnd_svm_search.best_params_)\r\n",
    "params_dict.update(rnd_knn_search.best_params_)\r\n",
    "with open('experiment6_1_params.csv', 'a') as f: \r\n",
    "    w = csv.DictWriter(f, params_dict.keys())\r\n",
    "    w.writerow(params_dict)\r\n",
    "    \r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_sadness = precision_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_prec_pleasure = precision_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_prec_anger = precision_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_prec_joy = precision_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_prec_sadness = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_prec_pleasure = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_prec_anger = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_prec_joy = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_prec_sadness = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_prec_pleasure = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_prec_anger = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_prec_joy = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_sadness = recall_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_rec_pleasure = recall_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_rec_anger = recall_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_rec_joy = recall_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_rec_sadness = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_rec_pleasure = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_rec_anger = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_rec_joy = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_rec_sadness = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_rec_pleasure = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_rec_anger = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_rec_joy = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_sadness = f1_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_f1_pleasure = f1_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_f1_anger = f1_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_f1_joy = f1_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_f1_sadness = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_f1_pleasure = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_f1_anger = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_f1_joy = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_f1_sadness = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_f1_pleasure = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_f1_anger = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_f1_joy = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "data_CSV = [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_sadness, rf_prec_pleasure, rf_prec_anger, rf_prec_joy, rf_rec_sadness, rf_rec_pleasure, rf_rec_anger, rf_rec_joy, rf_f1_sadness, rf_f1_pleasure, rf_f1_anger, rf_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_sadness, svm_prec_pleasure, svm_prec_anger, svm_prec_joy, svm_rec_sadness, svm_rec_pleasure, svm_rec_anger, svm_rec_joy, svm_f1_sadness, svm_f1_pleasure, svm_f1_anger, svm_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_sadness, knn_prec_pleasure, knn_prec_anger, knn_prec_joy, knn_rec_sadness, knn_rec_pleasure, knn_rec_anger, knn_rec_joy, knn_f1_sadness, knn_f1_pleasure, knn_f1_anger, knn_f1_joy]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment6_1.csv\", mode=\"a\", header=False, index=False)\r\n",
    "\r\n",
    "f = \"data_preprocessed_python\\\\s21.dat\"\r\n",
    "subject_id = 21\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_alpha = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['dominance', 'liking'], axis=1)\r\n",
    "\r\n",
    "conditions = [\r\n",
    "    ((data['valence'] <4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] <4.5) & (data['arousal'] > 4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal'] > 4.5))]\r\n",
    "choices = ['sadness', 'pleasure', 'anger', 'joy']\r\n",
    "\r\n",
    "data['valence'] = np.select(conditions, choices, default='black')\r\n",
    "data = data.rename(columns={'valence' : 'label'})\r\n",
    "data = data.drop(columns='arousal')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['label']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['label'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "#print(rnd_svm_search.best_params_)\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "params_dict = rnd_rf_search.best_params_\r\n",
    "params_dict.update(rnd_svm_search.best_params_)\r\n",
    "params_dict.update(rnd_knn_search.best_params_)\r\n",
    "with open('experiment6_1_params.csv', 'a') as f: \r\n",
    "    w = csv.DictWriter(f, params_dict.keys())\r\n",
    "    w.writerow(params_dict)\r\n",
    "    \r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_sadness = precision_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_prec_pleasure = precision_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_prec_anger = precision_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_prec_joy = precision_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_prec_sadness = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_prec_pleasure = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_prec_anger = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_prec_joy = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_prec_sadness = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_prec_pleasure = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_prec_anger = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_prec_joy = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_sadness = recall_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_rec_pleasure = recall_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_rec_anger = recall_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_rec_joy = recall_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_rec_sadness = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_rec_pleasure = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_rec_anger = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_rec_joy = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_rec_sadness = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_rec_pleasure = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_rec_anger = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_rec_joy = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_sadness = f1_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_f1_pleasure = f1_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_f1_anger = f1_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_f1_joy = f1_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_f1_sadness = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_f1_pleasure = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_f1_anger = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_f1_joy = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_f1_sadness = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_f1_pleasure = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_f1_anger = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_f1_joy = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "data_CSV = [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_sadness, rf_prec_pleasure, rf_prec_anger, rf_prec_joy, rf_rec_sadness, rf_rec_pleasure, rf_rec_anger, rf_rec_joy, rf_f1_sadness, rf_f1_pleasure, rf_f1_anger, rf_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_sadness, svm_prec_pleasure, svm_prec_anger, svm_prec_joy, svm_rec_sadness, svm_rec_pleasure, svm_rec_anger, svm_rec_joy, svm_f1_sadness, svm_f1_pleasure, svm_f1_anger, svm_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_sadness, knn_prec_pleasure, knn_prec_anger, knn_prec_joy, knn_rec_sadness, knn_rec_pleasure, knn_rec_anger, knn_rec_joy, knn_f1_sadness, knn_f1_pleasure, knn_f1_anger, knn_f1_joy]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment6_1.csv\", mode=\"a\", header=False, index=False)\r\n",
    "\r\n",
    "f = \"data_preprocessed_python\\\\s22.dat\"\r\n",
    "subject_id = 22\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_alpha = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['dominance', 'liking'], axis=1)\r\n",
    "\r\n",
    "conditions = [\r\n",
    "    ((data['valence'] <4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] <4.5) & (data['arousal'] > 4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal'] > 4.5))]\r\n",
    "choices = ['sadness', 'pleasure', 'anger', 'joy']\r\n",
    "\r\n",
    "data['valence'] = np.select(conditions, choices, default='black')\r\n",
    "data = data.rename(columns={'valence' : 'label'})\r\n",
    "data = data.drop(columns='arousal')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['label']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['label'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "#print(rnd_svm_search.best_params_)\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "params_dict = rnd_rf_search.best_params_\r\n",
    "params_dict.update(rnd_svm_search.best_params_)\r\n",
    "params_dict.update(rnd_knn_search.best_params_)\r\n",
    "with open('experiment6_1_params.csv', 'a') as f: \r\n",
    "    w = csv.DictWriter(f, params_dict.keys())\r\n",
    "    w.writerow(params_dict)\r\n",
    "    \r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_sadness = precision_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_prec_pleasure = precision_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_prec_anger = precision_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_prec_joy = precision_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_prec_sadness = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_prec_pleasure = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_prec_anger = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_prec_joy = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_prec_sadness = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_prec_pleasure = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_prec_anger = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_prec_joy = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_sadness = recall_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_rec_pleasure = recall_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_rec_anger = recall_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_rec_joy = recall_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_rec_sadness = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_rec_pleasure = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_rec_anger = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_rec_joy = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_rec_sadness = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_rec_pleasure = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_rec_anger = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_rec_joy = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_sadness = f1_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_f1_pleasure = f1_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_f1_anger = f1_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_f1_joy = f1_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_f1_sadness = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_f1_pleasure = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_f1_anger = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_f1_joy = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_f1_sadness = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_f1_pleasure = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_f1_anger = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_f1_joy = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "data_CSV = [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_sadness, rf_prec_pleasure, rf_prec_anger, rf_prec_joy, rf_rec_sadness, rf_rec_pleasure, rf_rec_anger, rf_rec_joy, rf_f1_sadness, rf_f1_pleasure, rf_f1_anger, rf_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_sadness, svm_prec_pleasure, svm_prec_anger, svm_prec_joy, svm_rec_sadness, svm_rec_pleasure, svm_rec_anger, svm_rec_joy, svm_f1_sadness, svm_f1_pleasure, svm_f1_anger, svm_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_sadness, knn_prec_pleasure, knn_prec_anger, knn_prec_joy, knn_rec_sadness, knn_rec_pleasure, knn_rec_anger, knn_rec_joy, knn_f1_sadness, knn_f1_pleasure, knn_f1_anger, knn_f1_joy]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment6_1.csv\", mode=\"a\", header=False, index=False)\r\n",
    "\r\n",
    "f = \"data_preprocessed_python\\\\s23.dat\"\r\n",
    "subject_id = 23\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_alpha = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['dominance', 'liking'], axis=1)\r\n",
    "\r\n",
    "conditions = [\r\n",
    "    ((data['valence'] <4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] <4.5) & (data['arousal'] > 4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal'] > 4.5))]\r\n",
    "choices = ['sadness', 'pleasure', 'anger', 'joy']\r\n",
    "\r\n",
    "data['valence'] = np.select(conditions, choices, default='black')\r\n",
    "data = data.rename(columns={'valence' : 'label'})\r\n",
    "data = data.drop(columns='arousal')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['label']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['label'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "#print(rnd_svm_search.best_params_)\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "params_dict = rnd_rf_search.best_params_\r\n",
    "params_dict.update(rnd_svm_search.best_params_)\r\n",
    "params_dict.update(rnd_knn_search.best_params_)\r\n",
    "with open('experiment6_1_params.csv', 'a') as f: \r\n",
    "    w = csv.DictWriter(f, params_dict.keys())\r\n",
    "    w.writerow(params_dict)\r\n",
    "    \r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_sadness = precision_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_prec_pleasure = precision_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_prec_anger = precision_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_prec_joy = precision_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_prec_sadness = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_prec_pleasure = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_prec_anger = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_prec_joy = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_prec_sadness = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_prec_pleasure = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_prec_anger = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_prec_joy = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_sadness = recall_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_rec_pleasure = recall_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_rec_anger = recall_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_rec_joy = recall_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_rec_sadness = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_rec_pleasure = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_rec_anger = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_rec_joy = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_rec_sadness = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_rec_pleasure = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_rec_anger = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_rec_joy = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_sadness = f1_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_f1_pleasure = f1_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_f1_anger = f1_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_f1_joy = f1_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_f1_sadness = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_f1_pleasure = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_f1_anger = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_f1_joy = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_f1_sadness = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_f1_pleasure = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_f1_anger = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_f1_joy = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "data_CSV = [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_sadness, rf_prec_pleasure, rf_prec_anger, rf_prec_joy, rf_rec_sadness, rf_rec_pleasure, rf_rec_anger, rf_rec_joy, rf_f1_sadness, rf_f1_pleasure, rf_f1_anger, rf_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_sadness, svm_prec_pleasure, svm_prec_anger, svm_prec_joy, svm_rec_sadness, svm_rec_pleasure, svm_rec_anger, svm_rec_joy, svm_f1_sadness, svm_f1_pleasure, svm_f1_anger, svm_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_sadness, knn_prec_pleasure, knn_prec_anger, knn_prec_joy, knn_rec_sadness, knn_rec_pleasure, knn_rec_anger, knn_rec_joy, knn_f1_sadness, knn_f1_pleasure, knn_f1_anger, knn_f1_joy]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment6_1.csv\", mode=\"a\", header=False, index=False)\r\n",
    "\r\n",
    "f = \"data_preprocessed_python\\\\s24.dat\"\r\n",
    "subject_id = 24\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_alpha = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['dominance', 'liking'], axis=1)\r\n",
    "\r\n",
    "conditions = [\r\n",
    "    ((data['valence'] <4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] <4.5) & (data['arousal'] > 4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal'] > 4.5))]\r\n",
    "choices = ['sadness', 'pleasure', 'anger', 'joy']\r\n",
    "\r\n",
    "data['valence'] = np.select(conditions, choices, default='black')\r\n",
    "data = data.rename(columns={'valence' : 'label'})\r\n",
    "data = data.drop(columns='arousal')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['label']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['label'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "#print(rnd_svm_search.best_params_)\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "params_dict = rnd_rf_search.best_params_\r\n",
    "params_dict.update(rnd_svm_search.best_params_)\r\n",
    "params_dict.update(rnd_knn_search.best_params_)\r\n",
    "with open('experiment6_1_params.csv', 'a') as f: \r\n",
    "    w = csv.DictWriter(f, params_dict.keys())\r\n",
    "    w.writerow(params_dict)\r\n",
    "    \r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_sadness = precision_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_prec_pleasure = precision_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_prec_anger = precision_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_prec_joy = precision_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_prec_sadness = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_prec_pleasure = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_prec_anger = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_prec_joy = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_prec_sadness = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_prec_pleasure = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_prec_anger = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_prec_joy = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_sadness = recall_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_rec_pleasure = recall_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_rec_anger = recall_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_rec_joy = recall_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_rec_sadness = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_rec_pleasure = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_rec_anger = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_rec_joy = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_rec_sadness = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_rec_pleasure = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_rec_anger = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_rec_joy = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_sadness = f1_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_f1_pleasure = f1_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_f1_anger = f1_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_f1_joy = f1_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_f1_sadness = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_f1_pleasure = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_f1_anger = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_f1_joy = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_f1_sadness = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_f1_pleasure = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_f1_anger = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_f1_joy = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "data_CSV = [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_sadness, rf_prec_pleasure, rf_prec_anger, rf_prec_joy, rf_rec_sadness, rf_rec_pleasure, rf_rec_anger, rf_rec_joy, rf_f1_sadness, rf_f1_pleasure, rf_f1_anger, rf_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_sadness, svm_prec_pleasure, svm_prec_anger, svm_prec_joy, svm_rec_sadness, svm_rec_pleasure, svm_rec_anger, svm_rec_joy, svm_f1_sadness, svm_f1_pleasure, svm_f1_anger, svm_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_sadness, knn_prec_pleasure, knn_prec_anger, knn_prec_joy, knn_rec_sadness, knn_rec_pleasure, knn_rec_anger, knn_rec_joy, knn_f1_sadness, knn_f1_pleasure, knn_f1_anger, knn_f1_joy]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment6_1.csv\", mode=\"a\", header=False, index=False)\r\n",
    "\r\n",
    "f = \"data_preprocessed_python\\\\s25.dat\"\r\n",
    "subject_id = 25\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_alpha = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['dominance', 'liking'], axis=1)\r\n",
    "\r\n",
    "conditions = [\r\n",
    "    ((data['valence'] <4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] <4.5) & (data['arousal'] > 4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal'] > 4.5))]\r\n",
    "choices = ['sadness', 'pleasure', 'anger', 'joy']\r\n",
    "\r\n",
    "data['valence'] = np.select(conditions, choices, default='black')\r\n",
    "data = data.rename(columns={'valence' : 'label'})\r\n",
    "data = data.drop(columns='arousal')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['label']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['label'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "#print(rnd_svm_search.best_params_)\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "params_dict = rnd_rf_search.best_params_\r\n",
    "params_dict.update(rnd_svm_search.best_params_)\r\n",
    "params_dict.update(rnd_knn_search.best_params_)\r\n",
    "with open('experiment6_1_params.csv', 'a') as f: \r\n",
    "    w = csv.DictWriter(f, params_dict.keys())\r\n",
    "    w.writerow(params_dict)\r\n",
    "    \r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_sadness = precision_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_prec_pleasure = precision_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_prec_anger = precision_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_prec_joy = precision_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_prec_sadness = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_prec_pleasure = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_prec_anger = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_prec_joy = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_prec_sadness = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_prec_pleasure = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_prec_anger = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_prec_joy = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_sadness = recall_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_rec_pleasure = recall_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_rec_anger = recall_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_rec_joy = recall_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_rec_sadness = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_rec_pleasure = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_rec_anger = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_rec_joy = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_rec_sadness = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_rec_pleasure = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_rec_anger = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_rec_joy = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_sadness = f1_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_f1_pleasure = f1_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_f1_anger = f1_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_f1_joy = f1_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_f1_sadness = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_f1_pleasure = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_f1_anger = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_f1_joy = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_f1_sadness = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_f1_pleasure = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_f1_anger = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_f1_joy = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "data_CSV = [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_sadness, rf_prec_pleasure, rf_prec_anger, rf_prec_joy, rf_rec_sadness, rf_rec_pleasure, rf_rec_anger, rf_rec_joy, rf_f1_sadness, rf_f1_pleasure, rf_f1_anger, rf_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_sadness, svm_prec_pleasure, svm_prec_anger, svm_prec_joy, svm_rec_sadness, svm_rec_pleasure, svm_rec_anger, svm_rec_joy, svm_f1_sadness, svm_f1_pleasure, svm_f1_anger, svm_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_sadness, knn_prec_pleasure, knn_prec_anger, knn_prec_joy, knn_rec_sadness, knn_rec_pleasure, knn_rec_anger, knn_rec_joy, knn_f1_sadness, knn_f1_pleasure, knn_f1_anger, knn_f1_joy]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment6_1.csv\", mode=\"a\", header=False, index=False)\r\n",
    "\r\n",
    "f = \"data_preprocessed_python\\\\s26.dat\"\r\n",
    "subject_id = 26\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_alpha = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['dominance', 'liking'], axis=1)\r\n",
    "\r\n",
    "conditions = [\r\n",
    "    ((data['valence'] <4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] <4.5) & (data['arousal'] > 4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal'] > 4.5))]\r\n",
    "choices = ['sadness', 'pleasure', 'anger', 'joy']\r\n",
    "\r\n",
    "data['valence'] = np.select(conditions, choices, default='black')\r\n",
    "data = data.rename(columns={'valence' : 'label'})\r\n",
    "data = data.drop(columns='arousal')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['label']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['label'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "#print(rnd_svm_search.best_params_)\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "params_dict = rnd_rf_search.best_params_\r\n",
    "params_dict.update(rnd_svm_search.best_params_)\r\n",
    "params_dict.update(rnd_knn_search.best_params_)\r\n",
    "with open('experiment6_1_params.csv', 'a') as f: \r\n",
    "    w = csv.DictWriter(f, params_dict.keys())\r\n",
    "    w.writerow(params_dict)\r\n",
    "    \r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_sadness = precision_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_prec_pleasure = precision_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_prec_anger = precision_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_prec_joy = precision_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_prec_sadness = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_prec_pleasure = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_prec_anger = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_prec_joy = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_prec_sadness = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_prec_pleasure = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_prec_anger = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_prec_joy = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_sadness = recall_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_rec_pleasure = recall_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_rec_anger = recall_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_rec_joy = recall_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_rec_sadness = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_rec_pleasure = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_rec_anger = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_rec_joy = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_rec_sadness = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_rec_pleasure = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_rec_anger = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_rec_joy = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_sadness = f1_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_f1_pleasure = f1_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_f1_anger = f1_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_f1_joy = f1_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_f1_sadness = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_f1_pleasure = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_f1_anger = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_f1_joy = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_f1_sadness = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_f1_pleasure = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_f1_anger = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_f1_joy = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "data_CSV = [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_sadness, rf_prec_pleasure, rf_prec_anger, rf_prec_joy, rf_rec_sadness, rf_rec_pleasure, rf_rec_anger, rf_rec_joy, rf_f1_sadness, rf_f1_pleasure, rf_f1_anger, rf_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_sadness, svm_prec_pleasure, svm_prec_anger, svm_prec_joy, svm_rec_sadness, svm_rec_pleasure, svm_rec_anger, svm_rec_joy, svm_f1_sadness, svm_f1_pleasure, svm_f1_anger, svm_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_sadness, knn_prec_pleasure, knn_prec_anger, knn_prec_joy, knn_rec_sadness, knn_rec_pleasure, knn_rec_anger, knn_rec_joy, knn_f1_sadness, knn_f1_pleasure, knn_f1_anger, knn_f1_joy]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment6_1.csv\", mode=\"a\", header=False, index=False)\r\n",
    "\r\n",
    "f = \"data_preprocessed_python\\\\s27.dat\"\r\n",
    "subject_id = 27\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_alpha = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['dominance', 'liking'], axis=1)\r\n",
    "\r\n",
    "conditions = [\r\n",
    "    ((data['valence'] <4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] <4.5) & (data['arousal'] > 4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal'] > 4.5))]\r\n",
    "choices = ['sadness', 'pleasure', 'anger', 'joy']\r\n",
    "\r\n",
    "data['valence'] = np.select(conditions, choices, default='black')\r\n",
    "data = data.rename(columns={'valence' : 'label'})\r\n",
    "data = data.drop(columns='arousal')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['label']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['label'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "#print(rnd_svm_search.best_params_)\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "params_dict = rnd_rf_search.best_params_\r\n",
    "params_dict.update(rnd_svm_search.best_params_)\r\n",
    "params_dict.update(rnd_knn_search.best_params_)\r\n",
    "with open('experiment6_1_params.csv', 'a') as f: \r\n",
    "    w = csv.DictWriter(f, params_dict.keys())\r\n",
    "    w.writerow(params_dict)\r\n",
    "    \r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_sadness = precision_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_prec_pleasure = precision_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_prec_anger = precision_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_prec_joy = precision_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_prec_sadness = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_prec_pleasure = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_prec_anger = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_prec_joy = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_prec_sadness = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_prec_pleasure = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_prec_anger = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_prec_joy = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_sadness = recall_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_rec_pleasure = recall_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_rec_anger = recall_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_rec_joy = recall_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_rec_sadness = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_rec_pleasure = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_rec_anger = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_rec_joy = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_rec_sadness = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_rec_pleasure = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_rec_anger = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_rec_joy = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_sadness = f1_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_f1_pleasure = f1_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_f1_anger = f1_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_f1_joy = f1_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_f1_sadness = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_f1_pleasure = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_f1_anger = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_f1_joy = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_f1_sadness = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_f1_pleasure = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_f1_anger = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_f1_joy = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "data_CSV = [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_sadness, rf_prec_pleasure, rf_prec_anger, rf_prec_joy, rf_rec_sadness, rf_rec_pleasure, rf_rec_anger, rf_rec_joy, rf_f1_sadness, rf_f1_pleasure, rf_f1_anger, rf_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_sadness, svm_prec_pleasure, svm_prec_anger, svm_prec_joy, svm_rec_sadness, svm_rec_pleasure, svm_rec_anger, svm_rec_joy, svm_f1_sadness, svm_f1_pleasure, svm_f1_anger, svm_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_sadness, knn_prec_pleasure, knn_prec_anger, knn_prec_joy, knn_rec_sadness, knn_rec_pleasure, knn_rec_anger, knn_rec_joy, knn_f1_sadness, knn_f1_pleasure, knn_f1_anger, knn_f1_joy]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment6_1.csv\", mode=\"a\", header=False, index=False)\r\n",
    "\r\n",
    "f = \"data_preprocessed_python\\\\s28.dat\"\r\n",
    "subject_id = 28\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_alpha = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['dominance', 'liking'], axis=1)\r\n",
    "\r\n",
    "conditions = [\r\n",
    "    ((data['valence'] <4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] <4.5) & (data['arousal'] > 4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal'] > 4.5))]\r\n",
    "choices = ['sadness', 'pleasure', 'anger', 'joy']\r\n",
    "\r\n",
    "data['valence'] = np.select(conditions, choices, default='black')\r\n",
    "data = data.rename(columns={'valence' : 'label'})\r\n",
    "data = data.drop(columns='arousal')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['label']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['label'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "#print(rnd_svm_search.best_params_)\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "params_dict = rnd_rf_search.best_params_\r\n",
    "params_dict.update(rnd_svm_search.best_params_)\r\n",
    "params_dict.update(rnd_knn_search.best_params_)\r\n",
    "with open('experiment6_1_params.csv', 'a') as f: \r\n",
    "    w = csv.DictWriter(f, params_dict.keys())\r\n",
    "    w.writerow(params_dict)\r\n",
    "    \r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_sadness = precision_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_prec_pleasure = precision_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_prec_anger = precision_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_prec_joy = precision_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_prec_sadness = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_prec_pleasure = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_prec_anger = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_prec_joy = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_prec_sadness = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_prec_pleasure = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_prec_anger = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_prec_joy = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_sadness = recall_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_rec_pleasure = recall_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_rec_anger = recall_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_rec_joy = recall_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_rec_sadness = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_rec_pleasure = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_rec_anger = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_rec_joy = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_rec_sadness = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_rec_pleasure = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_rec_anger = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_rec_joy = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_sadness = f1_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_f1_pleasure = f1_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_f1_anger = f1_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_f1_joy = f1_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_f1_sadness = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_f1_pleasure = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_f1_anger = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_f1_joy = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_f1_sadness = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_f1_pleasure = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_f1_anger = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_f1_joy = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "data_CSV = [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_sadness, rf_prec_pleasure, rf_prec_anger, rf_prec_joy, rf_rec_sadness, rf_rec_pleasure, rf_rec_anger, rf_rec_joy, rf_f1_sadness, rf_f1_pleasure, rf_f1_anger, rf_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_sadness, svm_prec_pleasure, svm_prec_anger, svm_prec_joy, svm_rec_sadness, svm_rec_pleasure, svm_rec_anger, svm_rec_joy, svm_f1_sadness, svm_f1_pleasure, svm_f1_anger, svm_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_sadness, knn_prec_pleasure, knn_prec_anger, knn_prec_joy, knn_rec_sadness, knn_rec_pleasure, knn_rec_anger, knn_rec_joy, knn_f1_sadness, knn_f1_pleasure, knn_f1_anger, knn_f1_joy]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment6_1.csv\", mode=\"a\", header=False, index=False)\r\n",
    "\r\n",
    "f = \"data_preprocessed_python\\\\s29.dat\"\r\n",
    "subject_id = 29\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_alpha = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['dominance', 'liking'], axis=1)\r\n",
    "\r\n",
    "conditions = [\r\n",
    "    ((data['valence'] <4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] <4.5) & (data['arousal'] > 4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal'] > 4.5))]\r\n",
    "choices = ['sadness', 'pleasure', 'anger', 'joy']\r\n",
    "\r\n",
    "data['valence'] = np.select(conditions, choices, default='black')\r\n",
    "data = data.rename(columns={'valence' : 'label'})\r\n",
    "data = data.drop(columns='arousal')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['label']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['label'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "#print(rnd_svm_search.best_params_)\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "params_dict = rnd_rf_search.best_params_\r\n",
    "params_dict.update(rnd_svm_search.best_params_)\r\n",
    "params_dict.update(rnd_knn_search.best_params_)\r\n",
    "with open('experiment6_1_params.csv', 'a') as f: \r\n",
    "    w = csv.DictWriter(f, params_dict.keys())\r\n",
    "    w.writerow(params_dict)\r\n",
    "    \r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_sadness = precision_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_prec_pleasure = precision_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_prec_anger = precision_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_prec_joy = precision_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_prec_sadness = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_prec_pleasure = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_prec_anger = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_prec_joy = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_prec_sadness = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_prec_pleasure = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_prec_anger = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_prec_joy = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_sadness = recall_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_rec_pleasure = recall_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_rec_anger = recall_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_rec_joy = recall_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_rec_sadness = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_rec_pleasure = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_rec_anger = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_rec_joy = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_rec_sadness = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_rec_pleasure = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_rec_anger = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_rec_joy = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_sadness = f1_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_f1_pleasure = f1_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_f1_anger = f1_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_f1_joy = f1_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_f1_sadness = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_f1_pleasure = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_f1_anger = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_f1_joy = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_f1_sadness = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_f1_pleasure = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_f1_anger = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_f1_joy = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "data_CSV = [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_sadness, rf_prec_pleasure, rf_prec_anger, rf_prec_joy, rf_rec_sadness, rf_rec_pleasure, rf_rec_anger, rf_rec_joy, rf_f1_sadness, rf_f1_pleasure, rf_f1_anger, rf_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_sadness, svm_prec_pleasure, svm_prec_anger, svm_prec_joy, svm_rec_sadness, svm_rec_pleasure, svm_rec_anger, svm_rec_joy, svm_f1_sadness, svm_f1_pleasure, svm_f1_anger, svm_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_sadness, knn_prec_pleasure, knn_prec_anger, knn_prec_joy, knn_rec_sadness, knn_rec_pleasure, knn_rec_anger, knn_rec_joy, knn_f1_sadness, knn_f1_pleasure, knn_f1_anger, knn_f1_joy]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment6_1.csv\", mode=\"a\", header=False, index=False)\r\n",
    "\r\n",
    "f = \"data_preprocessed_python\\\\s30.dat\"\r\n",
    "subject_id = 30\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_alpha = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['dominance', 'liking'], axis=1)\r\n",
    "\r\n",
    "conditions = [\r\n",
    "    ((data['valence'] <4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] <4.5) & (data['arousal'] > 4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal'] > 4.5))]\r\n",
    "choices = ['sadness', 'pleasure', 'anger', 'joy']\r\n",
    "\r\n",
    "data['valence'] = np.select(conditions, choices, default='black')\r\n",
    "data = data.rename(columns={'valence' : 'label'})\r\n",
    "data = data.drop(columns='arousal')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['label']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['label'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "#print(rnd_svm_search.best_params_)\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "params_dict = rnd_rf_search.best_params_\r\n",
    "params_dict.update(rnd_svm_search.best_params_)\r\n",
    "params_dict.update(rnd_knn_search.best_params_)\r\n",
    "with open('experiment6_1_params.csv', 'a') as f: \r\n",
    "    w = csv.DictWriter(f, params_dict.keys())\r\n",
    "    w.writerow(params_dict)\r\n",
    "    \r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_sadness = precision_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_prec_pleasure = precision_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_prec_anger = precision_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_prec_joy = precision_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_prec_sadness = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_prec_pleasure = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_prec_anger = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_prec_joy = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_prec_sadness = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_prec_pleasure = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_prec_anger = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_prec_joy = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_sadness = recall_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_rec_pleasure = recall_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_rec_anger = recall_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_rec_joy = recall_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_rec_sadness = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_rec_pleasure = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_rec_anger = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_rec_joy = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_rec_sadness = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_rec_pleasure = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_rec_anger = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_rec_joy = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_sadness = f1_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_f1_pleasure = f1_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_f1_anger = f1_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_f1_joy = f1_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_f1_sadness = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_f1_pleasure = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_f1_anger = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_f1_joy = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_f1_sadness = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_f1_pleasure = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_f1_anger = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_f1_joy = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "data_CSV = [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_sadness, rf_prec_pleasure, rf_prec_anger, rf_prec_joy, rf_rec_sadness, rf_rec_pleasure, rf_rec_anger, rf_rec_joy, rf_f1_sadness, rf_f1_pleasure, rf_f1_anger, rf_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_sadness, svm_prec_pleasure, svm_prec_anger, svm_prec_joy, svm_rec_sadness, svm_rec_pleasure, svm_rec_anger, svm_rec_joy, svm_f1_sadness, svm_f1_pleasure, svm_f1_anger, svm_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_sadness, knn_prec_pleasure, knn_prec_anger, knn_prec_joy, knn_rec_sadness, knn_rec_pleasure, knn_rec_anger, knn_rec_joy, knn_f1_sadness, knn_f1_pleasure, knn_f1_anger, knn_f1_joy]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment6_1.csv\", mode=\"a\", header=False, index=False)\r\n",
    "\r\n",
    "f = \"data_preprocessed_python\\\\s31.dat\"\r\n",
    "subject_id = 31\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_alpha = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['dominance', 'liking'], axis=1)\r\n",
    "\r\n",
    "conditions = [\r\n",
    "    ((data['valence'] <4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] <4.5) & (data['arousal'] > 4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal'] > 4.5))]\r\n",
    "choices = ['sadness', 'pleasure', 'anger', 'joy']\r\n",
    "\r\n",
    "data['valence'] = np.select(conditions, choices, default='black')\r\n",
    "data = data.rename(columns={'valence' : 'label'})\r\n",
    "data = data.drop(columns='arousal')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['label']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['label'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "#print(rnd_svm_search.best_params_)\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "params_dict = rnd_rf_search.best_params_\r\n",
    "params_dict.update(rnd_svm_search.best_params_)\r\n",
    "params_dict.update(rnd_knn_search.best_params_)\r\n",
    "with open('experiment6_1_params.csv', 'a') as f: \r\n",
    "    w = csv.DictWriter(f, params_dict.keys())\r\n",
    "    w.writerow(params_dict)\r\n",
    "    \r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_sadness = precision_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_prec_pleasure = precision_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_prec_anger = precision_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_prec_joy = precision_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_prec_sadness = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_prec_pleasure = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_prec_anger = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_prec_joy = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_prec_sadness = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_prec_pleasure = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_prec_anger = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_prec_joy = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_sadness = recall_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_rec_pleasure = recall_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_rec_anger = recall_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_rec_joy = recall_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_rec_sadness = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_rec_pleasure = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_rec_anger = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_rec_joy = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_rec_sadness = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_rec_pleasure = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_rec_anger = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_rec_joy = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_sadness = f1_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_f1_pleasure = f1_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_f1_anger = f1_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_f1_joy = f1_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_f1_sadness = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_f1_pleasure = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_f1_anger = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_f1_joy = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_f1_sadness = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_f1_pleasure = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_f1_anger = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_f1_joy = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "data_CSV = [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_sadness, rf_prec_pleasure, rf_prec_anger, rf_prec_joy, rf_rec_sadness, rf_rec_pleasure, rf_rec_anger, rf_rec_joy, rf_f1_sadness, rf_f1_pleasure, rf_f1_anger, rf_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_sadness, svm_prec_pleasure, svm_prec_anger, svm_prec_joy, svm_rec_sadness, svm_rec_pleasure, svm_rec_anger, svm_rec_joy, svm_f1_sadness, svm_f1_pleasure, svm_f1_anger, svm_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_sadness, knn_prec_pleasure, knn_prec_anger, knn_prec_joy, knn_rec_sadness, knn_rec_pleasure, knn_rec_anger, knn_rec_joy, knn_f1_sadness, knn_f1_pleasure, knn_f1_anger, knn_f1_joy]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment6_1.csv\", mode=\"a\", header=False, index=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-38-05678e9fb950>:120: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['valence'] = np.select(conditions, choices, default='black')\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  45 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=5)]: Done  91 out of 100 | elapsed:    2.5s remaining:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "<ipython-input-38-05678e9fb950>:350: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['valence'] = np.select(conditions, choices, default='black')\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "<ipython-input-38-05678e9fb950>:580: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['valence'] = np.select(conditions, choices, default='black')\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "<ipython-input-38-05678e9fb950>:810: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['valence'] = np.select(conditions, choices, default='black')\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "<ipython-input-38-05678e9fb950>:1040: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['valence'] = np.select(conditions, choices, default='black')\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "<ipython-input-38-05678e9fb950>:1270: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['valence'] = np.select(conditions, choices, default='black')\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "<ipython-input-38-05678e9fb950>:1500: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['valence'] = np.select(conditions, choices, default='black')\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  81 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "<ipython-input-38-05678e9fb950>:1730: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['valence'] = np.select(conditions, choices, default='black')\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "<ipython-input-38-05678e9fb950>:1960: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['valence'] = np.select(conditions, choices, default='black')\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  77 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "<ipython-input-38-05678e9fb950>:2190: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['valence'] = np.select(conditions, choices, default='black')\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "<ipython-input-38-05678e9fb950>:2420: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['valence'] = np.select(conditions, choices, default='black')\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  77 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done  91 out of 100 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "<ipython-input-38-05678e9fb950>:2650: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['valence'] = np.select(conditions, choices, default='black')\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "<ipython-input-38-05678e9fb950>:2880: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['valence'] = np.select(conditions, choices, default='black')\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "<ipython-input-38-05678e9fb950>:3110: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['valence'] = np.select(conditions, choices, default='black')\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "<ipython-input-38-05678e9fb950>:3340: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['valence'] = np.select(conditions, choices, default='black')\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "C:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "0",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-05678e9fb950>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m   3469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3470\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mchannel\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'FP1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'AF3'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'F7'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'F3'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'FC1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'FC5'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'T7'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'C3'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'CP1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'CP5'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'P7'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'P3'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Pz'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'PO3'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'O1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Oz'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'O2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'PO4'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'P4'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'P8'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'CP6'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'CP2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'C4'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'T8'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'FC6'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'FC2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'F4'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'F8'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'AF4'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Fp2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Fz'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Cz'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3471\u001b[1;33m         \u001b[0mZxx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdo_stft\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3472\u001b[0m         \u001b[0meeg_band_fft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_bands\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZxx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3473\u001b[0m      \u001b[1;31m#   print(video)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-02974aca68e6>\u001b[0m in \u001b[0;36mdo_stft\u001b[1;34m(video, channel)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdo_stft\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# Array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_videos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvideo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchannel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# Array of the sample frequency, Array of the segment times, STFT of x\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "f = \"data_preprocessed_python\\\\s32.dat\"\r\n",
    "subject_id = 32\r\n",
    "dictRaw = pd.read_pickle(f)\r\n",
    "\r\n",
    "labels = dictRaw.get('labels')\r\n",
    "\r\n",
    "dfLabels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "\r\n",
    "data = dictRaw.get('data')\r\n",
    "\r\n",
    "df_videos = {}\r\n",
    "    #Recorrido de cada una de las columnas de la tabla 3d, y guardado en una una lista de dataframes para cada uno de los videos.\r\n",
    "for i in range(40):\r\n",
    "    df_videos[i] = pd.DataFrame(data[i])\r\n",
    "    df_videos[i] = df_videos[i].transpose()\r\n",
    "    df_videos[i].columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz','hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature']\r\n",
    "    df_videos[i].drop(['hEOG','vEOG','zEMG','tEMG','GSR','Respiration','PLethy','Temperature'],axis = 'columns', inplace=True)\r\n",
    "\r\n",
    "for video in range(0, 40):\r\n",
    "    delta_bands = []\r\n",
    "    theta_bands = []\r\n",
    "    beta_bands = []\r\n",
    "    alpha_bands = []\r\n",
    "    gamma_bands = []\r\n",
    "\r\n",
    "    for channel in ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']:\r\n",
    "        Zxx = do_stft(video, channel)\r\n",
    "        eeg_band_fft = make_bands(Zxx)\r\n",
    "     #   print(video)\r\n",
    "      #  print(\"\\n\")\r\n",
    "       # print(channel)\r\n",
    "        freq = channel_freq(eeg_band_fft)\r\n",
    "\r\n",
    "        delta_bands.append(freq[0])\r\n",
    "        theta_bands.append(freq[1])\r\n",
    "        alpha_bands.append(freq[2])\r\n",
    "        beta_bands.append(freq[3])\r\n",
    "        gamma_bands.append(freq[4])\r\n",
    "\r\n",
    "# Banda delta\r\n",
    "df = pd.DataFrame(delta_bands)\r\n",
    "delta_df = df.transpose()\r\n",
    "delta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "delta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in delta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_delta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda theta\r\n",
    "df = pd.DataFrame(theta_bands)\r\n",
    "theta_df = df.transpose()\r\n",
    "theta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "theta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in theta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_theta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "#Bnada alpha\r\n",
    "df = pd.DataFrame(alpha_bands)\r\n",
    "alpha_df = df.transpose()\r\n",
    "alpha_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "alpha_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in alpha_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_alpha = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda beta\r\n",
    "df = pd.DataFrame(beta_bands)\r\n",
    "beta_df = df.transpose()\r\n",
    "beta_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "beta_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in beta_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_beta = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "# Banda gamma\r\n",
    "df = pd.DataFrame(gamma_bands)\r\n",
    "gamma_df = df.transpose()\r\n",
    "gamma_df.columns = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','Fp2','Fz','Cz']\r\n",
    "gamma_df\r\n",
    "\r\n",
    "rows = list()\r\n",
    "for _,row in gamma_df.iterrows():\r\n",
    "    rows += [row]*40\r\n",
    "aux_gamma = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "df_labels = pd.DataFrame(data=labels, columns=[\"valence\", \"arousal\", \"dominance\", \"liking\"])\r\n",
    "rows = list()\r\n",
    "for _,row in dfLabels.iterrows():\r\n",
    "    rows += [row]*64\r\n",
    "aux_labels = pd.DataFrame(rows).reset_index(drop=True)\r\n",
    "\r\n",
    "data_delta = aux_delta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_theta = aux_theta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_alpha = aux_alpha.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_beta = aux_beta.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "data_gamma = aux_gamma.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "frames = [aux_delta, aux_theta, aux_alpha, aux_beta, aux_gamma]\r\n",
    "aux_all_bands = pd.concat(frames, axis=1)\r\n",
    "all_bands_df = aux_all_bands.merge(aux_labels, left_index=True, right_index=True)\r\n",
    "\r\n",
    "data = all_bands_df.drop(['dominance', 'liking'], axis=1)\r\n",
    "\r\n",
    "conditions = [\r\n",
    "    ((data['valence'] <4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] <4.5) & (data['arousal'] > 4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal']<4.5)),\r\n",
    "    ((data['valence'] >4.5) & (data['arousal'] > 4.5))]\r\n",
    "choices = ['sadness', 'pleasure', 'anger', 'joy']\r\n",
    "\r\n",
    "data['valence'] = np.select(conditions, choices, default='black')\r\n",
    "data = data.rename(columns={'valence' : 'label'})\r\n",
    "data = data.drop(columns='arousal')\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "copia = data.copy()\r\n",
    "\r\n",
    "y = copia[['label']]\r\n",
    "\r\n",
    "x = copia.drop(columns=['label'])\r\n",
    "\r\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y, test_size = 0.2, random_state = 1)\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "\r\n",
    "clf = RandomForestClassifier(random_state=0, verbose=1, n_jobs=5)\r\n",
    "rnd_rf_search = GridSearchCV(clf, param_grid=param_distribs_rf, scoring='accuracy') \r\n",
    "rnd_rf_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_rf_search.best_estimator_\r\n",
    "predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "    \r\n",
    "from sklearn import svm\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "svm_clf = svm.SVC()\r\n",
    "rnd_svm_search = GridSearchCV(svm_clf, param_grid=param_distribs_svm, scoring='accuracy')\r\n",
    "rnd_svm_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_svm_search.best_estimator_\r\n",
    "#print(rnd_svm_search.best_params_)\r\n",
    "svm_predict = model.predict(xTest)\r\n",
    "\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "\r\n",
    "knn = KNeighborsClassifier()\r\n",
    "rnd_knn_search = GridSearchCV(knn, param_grid=param_distribs_knn, scoring='accuracy')\r\n",
    "rnd_knn_search.fit(xTrain, np.ravel(yTrain))\r\n",
    "model = rnd_knn_search.best_estimator_\r\n",
    "knn_predict = model.predict(xTest)\r\n",
    "\r\n",
    "params_dict = rnd_rf_search.best_params_\r\n",
    "params_dict.update(rnd_svm_search.best_params_)\r\n",
    "params_dict.update(rnd_knn_search.best_params_)\r\n",
    "with open('experiment6_1_params.csv', 'a') as f: \r\n",
    "    w = csv.DictWriter(f, params_dict.keys())\r\n",
    "    w.writerow(params_dict)\r\n",
    "    \r\n",
    "rf_acc = accuracy_score(yTest, predict)\r\n",
    "svm_acc = accuracy_score(yTest, svm_predict)\r\n",
    "knn_acc = accuracy_score(yTest, knn_predict)\r\n",
    "\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "rf_prec_sadness = precision_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_prec_pleasure = precision_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_prec_anger = precision_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_prec_joy = precision_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_prec_sadness = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_prec_pleasure = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_prec_anger = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_prec_joy = precision_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_prec_sadness = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_prec_pleasure = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_prec_anger = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_prec_joy = precision_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "rf_rec_sadness = recall_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_rec_pleasure = recall_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_rec_anger = recall_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_rec_joy = recall_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_rec_sadness = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_rec_pleasure = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_rec_anger = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_rec_joy = recall_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_rec_sadness = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_rec_pleasure = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_rec_anger = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_rec_joy = recall_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "rf_f1_sadness = f1_score(yTest, predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "rf_f1_pleasure = f1_score(yTest, predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "rf_f1_anger = f1_score(yTest, predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "rf_f1_joy = f1_score(yTest, predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "\r\n",
    "svm_f1_sadness = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "svm_f1_pleasure = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "svm_f1_anger = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "svm_f1_joy = f1_score(yTest, svm_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "knn_f1_sadness = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"sadness\"])\r\n",
    "knn_f1_pleasure = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"pleasure\"])\r\n",
    "knn_f1_anger = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"anger\"])\r\n",
    "knn_f1_joy = f1_score(yTest, knn_predict, average=\"micro\", labels=[\"joy\"])\r\n",
    "\r\n",
    "data_CSV = [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"Random Forest\", rf_acc, rf_prec_sadness, rf_prec_pleasure, rf_prec_anger, rf_prec_joy, rf_rec_sadness, rf_rec_pleasure, rf_rec_anger, rf_rec_joy, rf_f1_sadness, rf_f1_pleasure, rf_f1_anger, rf_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"SVM\", svm_acc, svm_prec_sadness, svm_prec_pleasure, svm_prec_anger, svm_prec_joy, svm_rec_sadness, svm_rec_pleasure, svm_rec_anger, svm_rec_joy, svm_f1_sadness, svm_f1_pleasure, svm_f1_anger, svm_f1_joy], [subject_id, 6, \"STFT\", \"All bands\", \"-\", \"kNN\", knn_acc, knn_prec_sadness, knn_prec_pleasure, knn_prec_anger, knn_prec_joy, knn_rec_sadness, knn_rec_pleasure, knn_rec_anger, knn_rec_joy, knn_f1_sadness, knn_f1_pleasure, knn_f1_anger, knn_f1_joy]\r\n",
    "all_data = data_CSV\r\n",
    "\r\n",
    "csv_df = pd.DataFrame(all_data, columns=col_names)\r\n",
    "\r\n",
    "csv_df.to_csv(\"experiment6_1.csv\", mode=\"a\", header=False, index=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-39-c814bf7c53f8>:120: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['valence'] = np.select(conditions, choices, default='black')\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}